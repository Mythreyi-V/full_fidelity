{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2207,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "yYxMeRJPLlzF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, mean_squared_error, r2_score\n",
    "import sklearn\n",
    "from sklearn.utils.validation import check_symmetric\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "import hdbscan\n",
    "from scipy.cluster import hierarchy\n",
    "import scipy.spatial.distance as ssd\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import learning\n",
    "import pyAgrum\n",
    "#from acv_explainers import ACXplainer\n",
    "\n",
    "#from anchor import anchor_tabular\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import filterfalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        feat_pos[i]+=n\n",
    "        n=n-1\n",
    "    \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_features(cls):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    coef = [abs(val) for val in og_coef]\n",
    "        \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_features(cls, instance):\n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "    \n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "\n",
    "    likelihoods = []\n",
    "    \n",
    "    for i in range(len(means)):\n",
    "        lk = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = scipy.stats.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = lk-alt_lk\n",
    "        likelihoods.append(lkhood)\n",
    "        \n",
    "    return np.abs(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_rankings(cls, instance, cls_method, X_train, feat_list):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        feat_pos = get_reg_features(cls)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        feat_pos = get_nb_features(cls, instance)\n",
    "        \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TuQANu5PLlzK"
   },
   "outputs": [],
   "source": [
    "def permute_instance(instance, i, perm_iter = 100, min_i = [0], max_i=[1], mean_i=[0], mode=\"permutation\"):\n",
    "            \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        if mode==\"baseline_max\":\n",
    "            n_val = [max_i[j]]*perm_iter\n",
    "        elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "            n_val = [mean_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_min\":\n",
    "            n_val = [min_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_0\":\n",
    "            n_val = [0]*perm_iter\n",
    "        else:\n",
    "            n_val = np.random.uniform(min_i[j], max_i[j], perm_iter)\n",
    "\n",
    "\n",
    "        permutations[i[j]] = n_val\n",
    "    \n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_values(instance, i, perm_iter = 100, min_i = [0], max_i=[1], mean_i=[0], unique_values=[[0,1]], mode=\"permutation\"):\n",
    "\n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "\n",
    "    for j in range(len(i)):\n",
    "        if mode==\"baseline_max\":\n",
    "            n_val = [max_i[j]]*perm_iter\n",
    "        elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "            n_val = [mean_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_min\":\n",
    "            n_val = [min_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_0\":\n",
    "            n_val = [0]*perm_iter\n",
    "        else:\n",
    "            n_val = np.random.choice(unique_values[j], perm_iter)\n",
    "\n",
    "        permutations[i[j]] = n_val\n",
    "        \n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_multiple(instance, i, columns, col_dict, perm_iter=100, min_i = [0], max_i=[1], mean_i=[0],unique_values=[[0,1]], mode=\"permutation\"):\n",
    "    \n",
    "    cats = []\n",
    "    nums = []\n",
    "    \n",
    "    if col_dict[\"discrete\"]!=None:\n",
    "        cats = [int(col) for col in i if columns[col] in col_dict[\"discrete\"]]\n",
    "    if col_dict[\"continuous\"]!=None:\n",
    "        nums = [int(col) for col in i if columns[col] in col_dict[\"continuous\"]]\n",
    "    \n",
    "    cat_permutations = False\n",
    "    num_permutations = False\n",
    "    \n",
    "    if len(cats)>0:\n",
    "        mins = min_i[columns[cats]].values\n",
    "        maxes = max_i[columns[cats]].values\n",
    "        means = mean_i[columns[cats]].values\n",
    "        uniques = unique_values[columns[cats]].values\n",
    "        cat_permutations = cycle_values(instance, cats, perm_iter, mins, maxes, \n",
    "                                  means, uniques, mode) \n",
    "    if len(nums)>0:\n",
    "        mins = min_i[columns[nums]].values\n",
    "        maxes = max_i[columns[nums]].values\n",
    "        means = mean_i[columns[nums]].values\n",
    "        num_permutations = permute_instance(instance, nums, perm_iter, mins, maxes, \n",
    "                                  means, mode)\n",
    "        \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    if type(cat_permutations)!=bool:\n",
    "        cat_permutations = cat_permutations.transpose()\n",
    "        for j in cats:\n",
    "            permutations[j] = cat_permutations[j]\n",
    "            \n",
    "    if type(num_permutations)!=bool:\n",
    "        num_permutations = num_permutations.transpose()\n",
    "        for j in nums:\n",
    "            permutations[j] = num_permutations[j]\n",
    "#     if  len(cats)>0:\n",
    "#         print(\"categorical:\", permutations[cats])\n",
    "#     if len(nums)>0:\n",
    "#         print(\"numeric\", permutations[nums])\n",
    "#     print(\"all\", permutations[i])        \n",
    "    permutations = permutations.transpose()\n",
    "    \n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = scipy.stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = np.sqrt(numerator/denominator)\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "3VkV42oeLlzL"
   },
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset = \"nursery\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "classification = True\n",
    "# xai_method = \"SHAP\"\n",
    "\n",
    "modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "random_state = 22\n",
    "exp_iter = 10\n",
    "perm_iter = 1000\n",
    "\n",
    "save_to = \"%s/%s/\" % (PATH, dataset)\n",
    "dataset_folder = \"%s/datasets/\" % (save_to)\n",
    "final_folder = \"%s/%s/\" % (save_to, cls_method)\n",
    "\n",
    "#Get datasets\n",
    "X_train = pd.read_csv(dataset_folder+dataset+\"_Xtrain.csv\", index_col=False, sep = \";\")\n",
    "y_train = pd.read_csv(dataset_folder+dataset+\"_Ytrain.csv\", index_col=False, sep = \";\")\n",
    "test_x = pd.read_csv(final_folder+\"test_sample.csv\", index_col=False, sep = \";\")\n",
    "results = pd.read_csv(os.path.join(final_folder,\"results.csv\"), index_col=False, sep = \";\")\n",
    "actual = results[\"Actual\"].values\n",
    "\n",
    "with open(dataset_folder+\"col_dict.json\", \"r\") as f:\n",
    "    col_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "feat_list = [each.replace(' ','_') for each in X_train.columns]\n",
    "\n",
    "cls = joblib.load(save_to+cls_method+\"/cls.joblib\")\n",
    "scaler = joblib.load(save_to+\"/scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If there are discrete columns, get the original dataset, not the one-hot encoded version\n",
    "# if col_dict[\"discrete\"]!=None:\n",
    "#     if dataset == \"compas\":\n",
    "#         data = pd.read_csv(dataset_folder+\"compas-scores-two-years.csv\")\n",
    "\n",
    "#         keep_cols = ['age', 'age_cat', 'sex', 'race',  'priors_count', 'days_b_screening_arrest', 'c_jail_in', 'c_jail_out',\n",
    "#                        'c_charge_degree', 'is_recid', 'is_violent_recid', 'two_year_recid', 'decile_score', 'score_text']\n",
    "\n",
    "#         df = data[keep_cols]\n",
    "\n",
    "#         target_col = 'class'\n",
    "\n",
    "#         df['days_b_screening_arrest'] = np.abs(df['days_b_screening_arrest'])\n",
    "\n",
    "#         df['c_jail_out'] = pd.to_datetime(df['c_jail_out'])\n",
    "#         df['c_jail_in'] = pd.to_datetime(df['c_jail_in'])\n",
    "#         df['length_of_stay'] = (df['c_jail_out'] - df['c_jail_in']).dt.days\n",
    "#         df['length_of_stay'] = np.abs(df['length_of_stay'])\n",
    "\n",
    "#         df['length_of_stay'].fillna(df['length_of_stay'].value_counts().index[0], inplace=True)\n",
    "#         df['days_b_screening_arrest'].fillna(df['days_b_screening_arrest'].value_counts().index[0], inplace=True)\n",
    "\n",
    "#         df['length_of_stay'] = df['length_of_stay'].astype(int)\n",
    "#         df['days_b_screening_arrest'] = df['days_b_screening_arrest'].astype(int)\n",
    "\n",
    "#         def get_class(x):\n",
    "#             if x < 7:\n",
    "#                 return 'Medium-Low'\n",
    "#             else:\n",
    "#                 return 'High'\n",
    "\n",
    "#         df['class'] = df['decile_score'].apply(get_class)\n",
    "\n",
    "#         del df['c_jail_in']\n",
    "#         del df['c_jail_out']\n",
    "#         del df['decile_score']\n",
    "#         del df['score_text']\n",
    "\n",
    "#         target_name = \"class\"\n",
    "\n",
    "#         feature_names = [col for col in df.columns if col != target_col]\n",
    "\n",
    "#         #df = remove_missing_values(df)\n",
    "\n",
    "#         possible_outcomes = list(df[target_col].unique())\n",
    "#         #print(\"Possible outcomes:\", possible_outcomes)\n",
    "\n",
    "#         #numerical_cols, categorical_cols, columns_type = get_columns_type(df)\n",
    "\n",
    "\n",
    "#         #for column in data.columns:\n",
    "#         #    data[column].replace(' ?', None, inplace=True)\n",
    "#         pro_data = df.dropna()\n",
    "\n",
    "#         other_cat_cols = ['is_recid', 'is_violent_recid', 'two_year_recid']\n",
    "        \n",
    "#     elif dataset == \"income\":\n",
    "#         pro_data = pd.read_csv(dataset_folder+\"adult.csv\", index_col = False)\n",
    "#         target_col = 'income-class'\n",
    "#         for column in pro_data.columns:\n",
    "#             pro_data[column].replace(' ?', None, inplace=True)\n",
    "#         pro_data = pro_data.drop('fnlwgt', axis=1)\n",
    "#         other_cat_cols = []\n",
    "        \n",
    "#     elif dataset == \"mushroom\":\n",
    "#         df = pd.read_csv(dataset_folder+\"mushroom_unpro.csv\")\n",
    "#         target_col = 'target'\n",
    "#         pro_data = df.dropna()\n",
    "#         other_cat_cols = []\n",
    "        \n",
    "#     elif dataset == \"nursery\":\n",
    "#         df = pd.read_csv(dataset_folder+\"nursery_unpro.csv\")\n",
    "#         other_cat_cols = []\n",
    "#         target_col = 'class'\n",
    "#         feature_names = [col for col in df.columns if col != target_col]\n",
    "#         possible_outcomes = list(df[target_col].unique())\n",
    "# #         print(\"Possible outcomes:\", possible_outcomes)\n",
    "# #         print(df[target_col].value_counts())\n",
    "\n",
    "#         def binarise_targets(x):\n",
    "#             if x == \"not_recom\":\n",
    "#                 return \"0_declined\"\n",
    "#             else:\n",
    "#                 return \"1_approved\"\n",
    "\n",
    "#         df[target_col] = df[target_col].apply(binarise_targets)\n",
    "\n",
    "#         pro_data = df.dropna()\n",
    "\n",
    "#     else:\n",
    "#         print(\"cannot find dataset\")\n",
    "        \n",
    "#     non_num_cols = [column for column in pro_data.drop([target_col], axis = 1).columns if pro_data[column].dtypes == \"O\"]\n",
    "#     num_cols = [column for column in pro_data.drop([target_col], axis = 1).columns if column not in non_num_cols]\n",
    "#     # if other_cat_cols != None:\n",
    "#     #     num_cols.extend(other_cat_cols)\n",
    "\n",
    "#     num_data = pro_data[num_cols]\n",
    "#     cat_data = pro_data[non_num_cols]\n",
    "    \n",
    "#     if other_cat_cols != None:\n",
    "#         num_cols = list(filterfalse(other_cat_cols.__contains__, num_cols))\n",
    "#         non_num_cols = non_num_cols + other_cat_cols\n",
    "\n",
    "#     onehot = OneHotEncoder(sparse=True, handle_unknown='ignore').fit(cat_data)\n",
    "\n",
    "#     orig_cols = np.arange(len(num_data.columns)).reshape(-1, 1).tolist()\n",
    "\n",
    "#     for col in cat_data.columns:\n",
    "#         new_cols = [i for i in range(len(X_train.columns)) if col in X_train.columns[i]]\n",
    "#         orig_cols.append(new_cols)\n",
    "\n",
    "#     orig_col_names = []\n",
    "#     orig_col_names.extend(num_data.columns)\n",
    "#     orig_col_names.extend(cat_data.columns)\n",
    "# else:\n",
    "#     num_data = X_train.copy()\n",
    "#     orig_cols = np.arange(X_train.shape[1]).reshape(-1, 1)\n",
    "#     orig_col_names= X_train.columns.tolist()\n",
    "#     cat_data = 0\n",
    "#     pro_data = X_train.copy()\n",
    "    \n",
    "#     num_cols = X_train.columns.tolist()\n",
    "#     non_num_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "aiQNPNswLlzM"
   },
   "outputs": [],
   "source": [
    "min_X = np.min(X_train)\n",
    "max_X = np.max(X_train)\n",
    "mean_X = np.mean(X_train, axis=0)\n",
    "unique_values = pd.Series({col: X_train[col].unique() for col in X_train.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if type(cat_data)!=int:\n",
    "#     min_num = np.min(X_train[num_data.columns])\n",
    "#     max_num = np.max(X_train[num_data.columns])\n",
    "#     mean_num = np.mean(X_train[num_data.columns], axis=0)\n",
    "#     num_orig = pd.Series({col: num_data[col].unique() for col in num_data.columns})\n",
    "    \n",
    "#     min_cat = pd.Series({col: 0 for col in cat_data.columns})\n",
    "#     max_cat = pd.Series({col: 1 for col in cat_data.columns})\n",
    "#     mean_cat = pd.Series({col: 0.5 for col in cat_data.columns})\n",
    "#     cat_orig = pd.Series({col: cat_data[col].unique() for col in cat_data.columns})\n",
    "\n",
    "#     min_orig = min_num.append(min_cat)\n",
    "#     max_orig = max_num.append(max_cat)\n",
    "#     mean_orig = mean_num.append(mean_cat)\n",
    "#     unique_orig = num_orig.append(cat_orig)\n",
    "    \n",
    "# else:\n",
    "#     min_orig = min_X.copy()\n",
    "#     max_orig = max_X.copy()\n",
    "#     mean_orig = mean_X.copy()\n",
    "#     unique_orig = unique_values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if non_num_cols!=[]:\n",
    "#     ordinal = OrdinalEncoder(handle_unknown = \"use_encoded_value\", unknown_value = -1)\n",
    "#     cat_enc = pd.DataFrame(ordinal.fit_transform(cat_data), columns = cat_data.columns)\n",
    "#     full_enc = pd.concat([num_data, cat_enc], axis=1)\n",
    "# else:\n",
    "#     full_enc = pro_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# corr_mat = np.zeros((full_enc.shape[1],full_enc.shape[1]))\n",
    "# for i in range(full_enc.shape[1]):\n",
    "#     for j in range(full_enc.shape[1]):\n",
    "#         if num_cols!=[]:\n",
    "#             if full_enc.columns[i] in num_cols and full_enc.columns[j] in num_cols:\n",
    "#                 corr_mat[i][j] = abs(scipy.stats.pearsonr(X_train.iloc[:, i], X_train.iloc[:, j]).statistic)\n",
    "#             elif full_enc.columns[i] in non_num_cols and full_enc.columns[j] in non_num_cols:\n",
    "#                 corr_mat[i][j] = cramers_v(X_train.iloc[:, i], X_train.iloc[:, j])\n",
    "#             elif full_enc.columns[i] in num_cols and full_enc.columns[j] in non_num_cols:\n",
    "#                 corr_mat[i][j] = correlation_ratio(X_train.iloc[:, j], X_train.iloc[:, i])\n",
    "#             elif full_enc.columns[i] in non_num_cols and full_enc.columns[j] in num_cols:\n",
    "#                 corr_mat[i][j] = correlation_ratio(X_train.iloc[:, i], X_train.iloc[:, j])\n",
    "                \n",
    "# sns.heatmap(corr_mat, yticklabels=orig_col_names, xticklabels = orig_col_names)\n",
    "# # plt.xticks(orig_col_names)\n",
    "# # plt.yticks(orig_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = full_enc.corr().fillna(0)\n",
    "# np.fill_diagonal(corr.values, 1)\n",
    "# # a = np.where(np.triu(corr, k=1)==0, np.nan,corr)\n",
    "# # corr = np.where(np.isnan(a), a.T, a)\n",
    "# distances = 1-corr.abs().values\n",
    "# sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sns.heatmap(corr.abs())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# mat = np.zeros((full_enc.shape[1],full_enc.shape[1]))\n",
    "# for i in range(full_enc.shape[1]):\n",
    "#     for j in range(full_enc.shape[1]):\n",
    "#         if num_cols!=[]:\n",
    "#             if full_enc.columns[i] in num_cols and full_enc.columns[j] in num_cols:\n",
    "#                 mat[i][j] = sklearn.metrics.pairwise_distances([X_train.iloc[:, i], X_train.iloc[:, j]], metric='cosine')[0,1]\n",
    "#                 #print(full_enc.columns[i], full_enc.columns[j], mat[i][j])\n",
    "#             elif full_enc.columns[i] in non_num_cols and full_enc.columns[j] in non_num_cols:\n",
    "#                 mat[i][j] = sklearn.metrics.pairwise_distances([X_train.iloc[:, i], X_train.iloc[:, j]], metric=\"hamming\")[0,1]\n",
    "#                 #print(full_enc.columns[i], full_enc.columns[j], mat[i][j])\n",
    "#             elif full_enc.columns[i] in num_cols and full_enc.columns[j] in non_num_cols:\n",
    "#                 mat[i][j] = correlation_ratio(X_train.iloc[:, j], X_train.iloc[:, i])\n",
    "#                 #print(full_enc.columns[i], full_enc.columns[j], mat[i][j])\n",
    "#             elif full_enc.columns[i] in non_num_cols and full_enc.columns[j] in num_cols:\n",
    "#                 mat[i][j] = 1-correlation_ratio(X_train.iloc[:, i], X_train.iloc[:, j])\n",
    "#                 #print(full_enc.columns[i], full_enc.columns[j], mat[i][j])\n",
    "                \n",
    "# sns.heatmap(mat, yticklabels=orig_col_names, xticklabels = orig_col_names)\n",
    "# # plt.xticks(orig_col_names)\n",
    "# # plt.yticks(orig_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric=\"precomputed\", cluster_selection_method=\"leaf\").fit(mat)\n",
    "# clusters = clusterer.labels_\n",
    "# clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# for each in np.unique(clusters):\n",
    "#     print(each)\n",
    "#     ind = np.where(clusters==each)\n",
    "#     rel_cols = full_enc.columns[ind]\n",
    "#     #sns.heatmap(full_enc[rel_cols].corr().abs(),vmin=0, vmax=1)\n",
    "#     sns.heatmap([corr[ind] for corr in corr_mat[ind]], yticklabels=rel_cols, xticklabels = rel_cols, vmin=0,vmax=1)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "# # for each in np.unique(clusters):\n",
    "# #     ind = np.where(clusters==each)\n",
    "# #     rel_cols = X_train.columns[ind]\n",
    "# #     sns.heatmap(X_train[rel_cols].corr().abs(),vmin=0, vmax=1)\n",
    "# #     plt.show()\n",
    "\n",
    "# corr_cols = []\n",
    "\n",
    "# for each in np.unique(clusters):\n",
    "#     ind = np.where(clusters==each)[0]\n",
    "#     if each == -1:\n",
    "#         corr_cols.extend(np.array(ind).reshape(-1,1))\n",
    "#     else:\n",
    "#         corr_cols.append(ind)\n",
    "    \n",
    "# #corr_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "nq_FxFrVLlzM",
    "outputId": "5aa1fc25-ef58-4357-fc0a-758bcdb2fc62",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee042efc726945ceaecd525b37c3a256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE 0.8954415954415957\n",
      "RMSE 0.8977051597316887\n",
      "R-Squared -0.8974179227357721\n",
      "baseline_min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5426ff5600b843a89064a19ef46b88d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE 0.19502195069673892\n",
      "RMSE 0.19502195069673892\n",
      "R-Squared -0.18134770957820515\n",
      "baseline_mean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f436a0a07e48fbacb4a14650272ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE 0.8286633078387746\n",
      "RMSE 0.8286633078387746\n",
      "R-Squared -0.8293782320839579\n",
      "baseline_max\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bc408e79164aecbf482d94afaf294c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE 0.43158920133343087\n",
      "RMSE 0.43158920133343087\n",
      "R-Squared -0.36392604892556024\n",
      "baseline_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dec6e5e18b4b7cb5d9dc07bb84d591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE 0.19502195069673892\n",
      "RMSE 0.19502195069673892\n",
      "R-Squared -0.18134770957820515\n"
     ]
    }
   ],
   "source": [
    "#permute individual features\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    ktb_list = []\n",
    "    true_v_mape = []\n",
    "    true_v_rmse = []\n",
    "    true_v_r2 = []\n",
    "\n",
    "    for i in tqdm_notebook(range(len(test_x.values))):\n",
    "        instance = test_x.values[i]\n",
    "\n",
    "        tr = get_true_rankings(cls, instance, cls_method, X_train, feat_list)\n",
    "\n",
    "        if classification:\n",
    "            pred = cls.predict(instance.reshape(1, -1))\n",
    "            proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "            p1_list = list(proba)*perm_iter\n",
    "\n",
    "        perm_mape = np.zeros(len(instance))\n",
    "        perm_rmse = np.zeros(len(instance))\n",
    "        perm_r2 = np.zeros(len(instance))\n",
    "\n",
    "        for j in range(len(instance)):\n",
    "            if col_dict[\"continuous\"] != None:\n",
    "                if X_train.columns[j] in col_dict[\"continuous\"]:\n",
    "                    permutations = permute_instance(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], mode)\n",
    "                else:\n",
    "                    permutations = cycle_values(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], [unique_values[X_train.columns[j]]], mode)\n",
    "            else:\n",
    "                permutations = cycle_values(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], [unique_values[X_train.columns[j]]], mode)\n",
    "\n",
    "            if classification:\n",
    "                p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "                perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "                perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "                perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "        #print(\"Final MAPE for all features:\", perm_mape)\n",
    "        mape_corr = scipy.stats.kendalltau(tr, perm_mape, variant=\"b\")[0]\n",
    "        rmse_corr = scipy.stats.kendalltau(tr, perm_rmse, variant=\"b\")[0]\n",
    "        r2_corr = scipy.stats.kendalltau(tr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "        true_v_mape.append(mape_corr)\n",
    "        true_v_rmse.append(rmse_corr)\n",
    "        true_v_r2.append(r2_corr)\n",
    "        \n",
    "    print(\"MAPE\", np.nanmean(true_v_mape))\n",
    "    print(\"RMSE\", np.nanmean(true_v_rmse))\n",
    "    print(\"R-Squared\", np.nanmean(true_v_r2))\n",
    "\n",
    "    results[\"MAPE Correctness\"] = true_v_mape\n",
    "    results[\"RMSE Correctness\"] = true_v_rmse\n",
    "    results[\"R2 Correctness\"] = true_v_r2\n",
    "    results.to_csv(os.path.join(save_to, cls_method, mode+\"_results.csv\"), index = False, sep = \";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #permute original features\n",
    "# for mode in modes:\n",
    "#         print(mode)\n",
    "#     #if col_dict[\"discrete\"]!=None:\n",
    "#         ktb_list = []\n",
    "#         true_v_mape = []\n",
    "#         true_v_rmse = []\n",
    "#         true_v_r2 = []\n",
    "        \n",
    "#         if col_dict[\"discrete\"]!=None:\n",
    "#             orig_cat = onehot.inverse_transform(test_x.values[:, len(num_data.columns):])\n",
    "#             orig_num = test_x.values[:, :len(num_data.columns)]\n",
    "#             orig_test = np.hstack((orig_num, orig_cat))\n",
    "#         else:\n",
    "#             orig_test = test_x.values.copy()\n",
    "        \n",
    "#         for i in tqdm_notebook(range(len(test_x.values))):\n",
    "#             instance = test_x.values[i]\n",
    "#             og_instance = orig_test[i]\n",
    "\n",
    "#             tr = get_true_rankings(cls, instance, cls_method, X_train, feat_list)\n",
    "#             cr = []\n",
    "#             for cluster in orig_cols:\n",
    "#                 ranks = np.array(tr)[cluster]\n",
    "#                 cr.append(np.sum(abs(ranks)))\n",
    "\n",
    "#             pred = cls.predict(instance.reshape(1, -1))\n",
    "#             proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "#             p1_list = list(proba)*perm_iter\n",
    "\n",
    "#             perm_mape = np.zeros(len(orig_cols))\n",
    "#             perm_rmse = np.zeros(len(orig_cols))\n",
    "#             perm_r2 = np.zeros(len(orig_cols))\n",
    "\n",
    "#             for j in range(len(orig_cols)):\n",
    "#                 cols = orig_cols[j]\n",
    "#                 if col_dict[\"continuous\"] != None:\n",
    "#                     if orig_col_names[j] in col_dict[\"continuous\"]:\n",
    "#                         permutations = permute_instance(og_instance, [j], perm_iter, [min_X[cols]], [max_X[cols]], [mean_X[cols]], mode)\n",
    "#                     else:\n",
    "#                         permutations = cycle_values(og_instance, [j], perm_iter, [0], [1], [0.5], [unique_orig[orig_col_names[j]]], mode)\n",
    "#                 else:\n",
    "#                     permutations = cycle_values(og_instance, [j], perm_iter, [0], [1], [0.5], [unique_orig[orig_col_names[j]]], mode)\n",
    "                    \n",
    "#                 if col_dict[\"discrete\"]!=None:\n",
    "#                     num_perm = permutations[:, :len(num_data.columns)]\n",
    "#                     cat_perm = permutations[:, len(num_data.columns):]\n",
    "\n",
    "#                     cat_perm = onehot.transform(cat_perm).toarray()\n",
    "\n",
    "#                     permutations = np.hstack((num_perm, cat_perm))\n",
    " \n",
    "#                 p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "#                 perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "#                 perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "#                 perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "#             #print(\"Final MAPE for all features:\", perm_mape)\n",
    "#             mape_corr = scipy.stats.kendalltau(cr, perm_mape, variant=\"b\")[0]\n",
    "#             rmse_corr = scipy.stats.kendalltau(cr, perm_rmse, variant=\"b\")[0]\n",
    "#             r2_corr = scipy.stats.kendalltau(cr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "#             true_v_mape.append(mape_corr)\n",
    "#             true_v_rmse.append(rmse_corr)\n",
    "#             true_v_r2.append(r2_corr)\n",
    "            \n",
    "#         print(\"MAPE\", np.nanmean(true_v_mape))\n",
    "#         print(\"RMSE\", np.nanmean(true_v_rmse))\n",
    "#         print(\"R-Squared\", np.nanmean(true_v_r2))\n",
    "\n",
    "#         #results[xai_method+\"_KT-B\"] = ktb_list\n",
    "#         results[\"MAPE Correctness\"] = true_v_mape\n",
    "#         results[\"RMSE Correctness\"] = true_v_rmse\n",
    "#         results[\"R2 Correctness\"] = true_v_r2\n",
    "#         results.to_csv(os.path.join(save_to, cls_method, mode+\"__orig_results.csv\"), index = False, sep = \";\")\n",
    "        \n",
    "# #     else:\n",
    "# #         results[\"MAPE Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results[\"RMSE Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results[\"R2 Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results.to_csv(os.path.join(save_to, cls_method, mode+\"__orig_results.csv\"), index = False, sep = \";\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #permute correlated features\n",
    "# for mode in modes:\n",
    "#         print(mode)\n",
    "#     #if col_dict[\"discrete\"]!=None:\n",
    "#         ktb_list = []\n",
    "#         true_v_mape = []\n",
    "#         true_v_rmse = []\n",
    "#         true_v_r2 = []\n",
    "        \n",
    "#         if col_dict[\"discrete\"]!=None:\n",
    "#             orig_cat = onehot.inverse_transform(test_x.values[:, len(num_data.columns):])\n",
    "#             orig_num = test_x.values[:, :len(num_data.columns)]\n",
    "#             orig_test = np.hstack((orig_num, orig_cat))\n",
    "#         else:\n",
    "#             orig_test = test_x.values.copy()\n",
    "        \n",
    "#         for i in tqdm_notebook(range(len(test_x.values))):\n",
    "#             instance = test_x.values[i]\n",
    "#             og_instance = orig_test[i]\n",
    "\n",
    "#             cols_mapped = []\n",
    "#             for each in corr_cols:\n",
    "#                 cluster = []\n",
    "#                 for col in each:\n",
    "#                     cluster.extend(orig_cols[col])\n",
    "#                 cols_mapped.append(cluster)\n",
    "#             #print(cols_mapped)\n",
    "\n",
    "#             instance = test_x.values[0]\n",
    "#             tr = get_true_rankings(cls, instance, cls_method, X_train, feat_list)\n",
    "#             cr = []\n",
    "#             for cluster in cols_mapped:\n",
    "#                 ranks = np.array(tr)[cluster]\n",
    "#                 cr.append(np.sum(abs(ranks)))\n",
    "\n",
    "#             pred = cls.predict(instance.reshape(1, -1))\n",
    "#             proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "#             p1_list = list(proba)*perm_iter\n",
    "\n",
    "#             perm_mape = np.zeros(len(corr_cols))\n",
    "#             perm_rmse = np.zeros(len(corr_cols))\n",
    "#             perm_r2 = np.zeros(len(corr_cols))\n",
    "\n",
    "#             for j in range(len(corr_cols)):\n",
    "#                 cols = corr_cols[j]\n",
    "#                 orig_col_dict = {\"continuous\": num_cols, \"discrete\": non_num_cols}\n",
    "                \n",
    "#                 mins = min_orig[np.array(orig_col_names)[corr_cols[j]]]\n",
    "#                 means = mean_orig[np.array(orig_col_names)[corr_cols[j]]]\n",
    "#                 maxes = max_orig[np.array(orig_col_names)[corr_cols[j]]]\n",
    "#                 uniques = unique_orig[np.array(orig_col_names)[corr_cols[j]]]\n",
    "                                \n",
    "#                 permutations = permute_multiple(og_instance, cols, np.array(orig_col_names), orig_col_dict, perm_iter, \n",
    "#                                                 mins, maxes, means, uniques, mode)                   \n",
    "                \n",
    "#                 if col_dict[\"discrete\"]!=None:\n",
    "#                     num_perm = permutations[:, :orig_num.shape[1]]\n",
    "#                     cat_perm = permutations[:, orig_num.shape[1]:]\n",
    "\n",
    "#                     cat_perm = onehot.transform(cat_perm).toarray()\n",
    "\n",
    "#                     permutations = np.hstack((num_perm, cat_perm))\n",
    " \n",
    "#                 p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "#                 perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "#                 perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "#                 perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "#             #print(\"Final MAPE for all features:\", perm_mape)\n",
    "#             mape_corr = scipy.stats.kendalltau(cr, perm_mape, variant=\"b\")[0]\n",
    "#             rmse_corr = scipy.stats.kendalltau(cr, perm_rmse, variant=\"b\")[0]\n",
    "#             r2_corr = scipy.stats.kendalltau(cr, perm_r2, variant=\"b\")[0]\n",
    "            \n",
    "#             #print(mape_corr,rmse_corr,r2_corr)\n",
    "\n",
    "#             true_v_mape.append(mape_corr)\n",
    "#             true_v_rmse.append(rmse_corr)\n",
    "#             true_v_r2.append(r2_corr)\n",
    "            \n",
    "#         print(\"MAPE\", np.nanmean(true_v_mape))\n",
    "#         print(\"RMSE\", np.nanmean(true_v_rmse))\n",
    "#         print(\"R-Squared\", np.nanmean(true_v_r2))\n",
    "\n",
    "#         #results[xai_method+\"_KT-B\"] = ktb_list\n",
    "#         results[\"MAPE Correctness\"] = true_v_mape\n",
    "#         results[\"RMSE Correctness\"] = true_v_rmse\n",
    "#         results[\"R2 Correctness\"] = true_v_r2\n",
    "#         results.to_csv(os.path.join(save_to, cls_method, mode+\"__corr_results.csv\"), index = False, sep = \";\")\n",
    "        \n",
    "# #     else:\n",
    "# #         results[\"MAPE Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results[\"RMSE Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results[\"R2 Correctness\"] = [np.nan]*results.shape[0]\n",
    "# #         results.to_csv(os.path.join(save_to, cls_method, mode+\"__corr_results.csv\"), index = False, sep = \";\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mode in modes:\n",
    "#     if len(corr_cols) > 1:\n",
    "#         ktb_list = []\n",
    "#         true_v_mape = []\n",
    "#         true_v_rmse = []\n",
    "#         true_v_r2 = []\n",
    "\n",
    "#         for i in tqdm_notebook(range(len(test_x))):\n",
    "#             instance = test_x.values[i]\n",
    "\n",
    "#             tr = get_true_rankings(cls, instance, cls_method, X_train, feat_list)\n",
    "#             cr = []\n",
    "#             for cluster in corr_cols:\n",
    "#                 ranks = np.array(tr)[cluster]\n",
    "#                 cr.append(np.sum(ranks))\n",
    "\n",
    "#             pred = cls.predict(instance.reshape(1, -1))\n",
    "#             proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "#             p1_list = list(proba)*perm_iter\n",
    "\n",
    "#             perm_mape = np.zeros(len(corr_cols))\n",
    "#             perm_rmse = np.zeros(len(corr_cols))\n",
    "#             perm_r2 = np.zeros(len(corr_cols))\n",
    "\n",
    "#             for j in range(len(corr_cols)):\n",
    "#                 mins = min_X[X_train.columns[corr_cols[j]]]\n",
    "#                 means = mean_X[X_train.columns[corr_cols[j]]]\n",
    "#                 maxes = max_X[X_train.columns[corr_cols[j]]]\n",
    "#                 uniques = unique_values[X_train.columns[corr_cols[j]]]\n",
    "                \n",
    "#                 permutations = permute_multiple(instance, corr_cols[j], X_train.columns, col_dict, perm_iter, \n",
    "#                                                 mins, maxes, means, uniques, mode)\n",
    " \n",
    "#                 p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "#                 perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "#                 perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "#                 perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "#             #print(\"Final MAPE for all features:\", perm_mape)\n",
    "#             mape_corr = scipy.stats.kendalltau(cr, perm_mape, variant=\"b\")[0]\n",
    "#             rmse_corr = scipy.stats.kendalltau(cr, perm_rmse, variant=\"b\")[0]\n",
    "#             r2_corr = scipy.stats.kendalltau(cr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "#             true_v_mape.append(mape_corr)\n",
    "#             true_v_rmse.append(rmse_corr)\n",
    "#             true_v_r2.append(r2_corr)\n",
    "            \n",
    "#         print(\"MAPE\", np.nanmean(true_v_mape))\n",
    "#         print(\"RMSE\", np.nanmean(true_v_rmse))\n",
    "#         print(\"R-Squared\", np.nanmean(true_v_r2))\n",
    "\n",
    "#         #results[xai_method+\"_KT-B\"] = ktb_list\n",
    "#         results[\"MAPE Correctness\"] = true_v_mape\n",
    "#         results[\"RMSE Correctness\"] = true_v_rmse\n",
    "#         results[\"R2 Correctness\"] = true_v_r2\n",
    "#         results.to_csv(os.path.join(save_to, cls_method, mode+\"__corr_results.csv\"), index = False, sep = \";\")\n",
    "    \n",
    "#     else:\n",
    "#         results[\"MAPE Correctness\"] = [np.nan]*results.shape[0]\n",
    "#         results[\"RMSE Correctness\"] = [np.nan]*results.shape[0]\n",
    "#         results[\"R2 Correctness\"] = [np.nan]*results.shape[0]\n",
    "#         results.to_csv(os.path.join(save_to, cls_method, mode+\"__corr_results.csv\"), index = False, sep = \";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual                               0\n",
       "Prediction                           0\n",
       "Prediction Probability               0\n",
       "SHAP Subset Stability                0\n",
       "SHAP Weight Stability                0\n",
       "SHAP Adjusted Weight Stability       0\n",
       "LIME Subset Stability                0\n",
       "LIME Weight Stability                0\n",
       "LIME Adjusted Weight Stability       0\n",
       "ACV Subset Stability                 0\n",
       "ACV Weight Stability                 0\n",
       "ACV Adjusted Weight Stability        0\n",
       "LINDA Subset Stability             100\n",
       "LINDA Weight Stability               0\n",
       "LINDA Adjusted Weight Stability      0\n",
       "SHAP Precision                       0\n",
       "SHAP Recall                          0\n",
       "SHAP Correlation                     0\n",
       "MAPE Correctness                     0\n",
       "RMSE Correctness                     0\n",
       "R2 Correctness                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perm_mape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fidelity_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
