{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4733,
     "status": "ok",
     "timestamp": 1652073020845,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Efp_y7Q2LpyR",
    "outputId": "9783452f-45e7-4e82-afd4-6a562593daf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# import os\n",
    "\n",
    "# os.chdir('drive/MyDrive/three-phase-fidelity-wip')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # # # !pip install -r requirements.txt\n",
    "# !pip install ipython==7.31.1\n",
    "# !pip install importlib-metadata==4.10.0\n",
    "# !pip install acv-exp\n",
    "# !pip install hyperopt==0.27\n",
    "# # # ## !pip freeze > requirements.txt\n",
    "\n",
    "# !pip install lime\n",
    "# !pip install shap\n",
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2207,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "yYxMeRJPLlzF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, mean_squared_error, r2_score\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import learning\n",
    "import pyAgrum\n",
    "#from acv_explainers import ACXplainer\n",
    "\n",
    "#from anchor import anchor_tabular\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "hv1zKoQOLlzI"
   },
   "outputs": [],
   "source": [
    "# def get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "    \n",
    "#     shap_exp = []\n",
    "    \n",
    "#     pred = cls.predict(instance.reshape(1, -1))\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if type(explainer) == shap.explainers._tree.Tree:\n",
    "#             exp = explainer(instance, check_additivity = False).values\n",
    "#         else:\n",
    "#             exp = explainer(instance.reshape(1, -1)).values\n",
    "                \n",
    "#         if exp.shape == (1, len(feat_list), 2):\n",
    "#             exp = exp[0]\n",
    "            \n",
    "#         #print(exp.shape)\n",
    "        \n",
    "#         if exp.shape == (len(feat_list), 2):\n",
    "#             exp = np.array([feat[pred] for feat in exp]).reshape(len(feat_list))\n",
    "#         elif exp.shape == (1, len(feat_list)) or exp.shape == (len(feat_list), 1):\n",
    "#             exp = exp.reshape(len(feat_list))\n",
    "            \n",
    "#         shap_exp.append(exp)\n",
    "        \n",
    "        \n",
    "#     if np.array(shap_exp).shape != (exp_iter, len(feat_list)):\n",
    "#         raise Exception(\"Explanation shape is not correct. It is\", np.array(shap_exp).shape, \"instead of the expected\", (exp_iter, len(feat_list)))\n",
    "            \n",
    "#     avg_val = np.average(shap_exp, axis = 0)\n",
    "#     abs_val = [abs(val) for val in avg_val]\n",
    "    \n",
    "#     return avg_val, abs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "EwoHjZeWLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "#     lime_exp = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if classification==True:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict_proba, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "#         else:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "            \n",
    "#     weights = [[] for each in feat_list]\n",
    "#     for exp in lime_exp:\n",
    "#         feat = exp[0]\n",
    "#         if '<' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('<')\n",
    "#         elif '>' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('>')\n",
    "#         else:\n",
    "#             parts = feat.split(\"=\")\n",
    "        \n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         n = feat_list.index(feat_name)\n",
    "#         weights[n].append(exp[1])\n",
    "    \n",
    "#     weights = np.transpose(weights)\n",
    "#     avg_weight = np.average(np.array(weights), axis = 0)\n",
    "#     abs_weight = [abs(weight) for weight in avg_weight]\n",
    "    \n",
    "#     bins = pd.cut(abs_weight, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     sorted_weight = np.copy(abs_weight)\n",
    "#     sorted_weight.sort()\n",
    "    \n",
    "#     lime_features = [i for i in range(len(feat_list)) if abs_weight[i] >= q1_min]\n",
    "    \n",
    "#     return avg_weight, abs_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list):\n",
    "#     label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "#     feat_pos = []\n",
    "#     lkhoods = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         [bn, inference, infoBN] = learning.generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "#                                                                        None, scaler, cls, save_to+\"/\"+cls_method+\"/\", dataset, show_in_notebook = False)\n",
    "        \n",
    "#         ie = pyAgrum.LazyPropagation(bn)\n",
    "#         result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#         result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#         row = instance['original_vector']\n",
    "#         #print(row)\n",
    "\n",
    "#         likelihood = [0]*len(feat_list)\n",
    "\n",
    "#         for j in range(len(feat_list)):\n",
    "#             var_labels = bn.variable(feat_list[j]).labels()\n",
    "#             str_bins = list(var_labels)\n",
    "#             bins = []\n",
    "\n",
    "#             for disc_bin in str_bins:\n",
    "#                 disc_bin = disc_bin.strip('\"(]')\n",
    "#                 cat = [float(val) for val in disc_bin.split(',')]\n",
    "#                 bins.append(cat)\n",
    "\n",
    "#             for k in range(len(bins)):\n",
    "#                 if k == 0 and row[j] <= bins[k][0]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif k == len(bins)-1 and row[j] >= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif row[j] > bins[k][0] and row[j] <= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "\n",
    "#             ie = pyAgrum.LazyPropagation(bn)\n",
    "#             ie.setEvidence({feat_list[j]: feat_bin})\n",
    "#             ie.makeInference()\n",
    "            \n",
    "#             result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#             new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#             #print(result_proba, new_proba)\n",
    "#             proba_change = result_proba-new_proba\n",
    "#             likelihood[j] = abs(proba_change)\n",
    "\n",
    "#         lkhoods.append(likelihood)\n",
    "        \n",
    "#     bins = pd.cut(np.mean(lkhoods, axis=0), 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "\n",
    "#     #If fixing all features produces the same result for the class,\n",
    "#     #return all features\n",
    "#     if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "#         feat_pos.extend(range(len(feat_list)))\n",
    "#         #print(lkhoods)\n",
    "#     else:\n",
    "#         feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "#     feat_pos = set(feat_pos)\n",
    "#     #print(feat_pos)\n",
    "    \n",
    "#     return np.mean(lkhoods, axis=0), np.mean(lkhoods, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "r6DGbiuVLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list):\n",
    "#     instance = instance.reshape(1, -1)\n",
    "#     y = cls.predict(instance)\n",
    "\n",
    "#     feat_pos = []\n",
    "#     feat_imp = []\n",
    "\n",
    "#     for i in range(exp_iter):\n",
    "#         sdp_importance, sdp_index, size, sdp = explainer.importance_sdp_rf(instance, y, X_train, y_train)\n",
    "#         feat_pos.extend(sdp_index[0, :size[0]])\n",
    "        \n",
    "#         sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train)\n",
    "#         #print(np.array(sufficient_expl).shape)\n",
    "#         lximp = explainer.compute_local_sdp(len(feat_list), sufficient_expl[0])\n",
    "#         feat_imp.append(lximp)\n",
    "\n",
    "\n",
    "#     feats = Counter(feat_pos)\n",
    "#     imp = feats.items()\n",
    "\n",
    "#     occ = np.zeros(len(feat_list))\n",
    "\n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     avg_imp = np.mean(feat_imp, axis=0)\n",
    "#     abs_imp = [abs(imp) for imp in avg_imp]\n",
    "\n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "#     #print(\"Feature importance score:\", avg_imp)\n",
    "#     return occ, occ\n",
    "#     #return avg_imp, abs_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_anchor_features(explainer, instance, cls, train_data, classification, exp_iter):\n",
    "#     anchor_exp = []\n",
    "#     for i in range(exp_iter):\n",
    "#         if classification == True:\n",
    "#             anchor_exp.extend(explainer.explain_instance(instance, cls.predict).names())\n",
    "#     print(anchor_exp)\n",
    "\n",
    "#     locs = []\n",
    "#     for exp in anchor_exp:\n",
    "#         feat = exp.replace(\"= \",'')\n",
    "#         #print(feat)\n",
    "#         if '<' in feat:\n",
    "#             parts = feat.split('<')\n",
    "#             #print(\"less than\", parts)\n",
    "#         elif '>' in feat:\n",
    "#             parts = feat.split('>')\n",
    "#             #print(\"more than\", parts)\n",
    "\n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         locs.append(feat_list.index(feat_name))\n",
    "    \n",
    "#     feats = Counter(locs)\n",
    "#     imp = feats.items()\n",
    "    \n",
    "#     #print(imp)\n",
    "    \n",
    "#     occ = np.zeros(len(feat_list))\n",
    "    \n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "    \n",
    "#     return occ, occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "gXQO43zuLlzK"
   },
   "outputs": [],
   "source": [
    "# def get_explanation_features(explainer, instance, cls, scaler, dataset, classification, exp_iter, xai_method, feat_list, X_train, y_train):\n",
    "#     if xai_method == \"SHAP\":\n",
    "#         feat_pos = get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LIME\":\n",
    "#         feat_pos = get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LINDA\":\n",
    "#         feat_pos = get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list)\n",
    "\n",
    "#     elif xai_method == \"ACV\":\n",
    "#         feat_pos = get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"Anchor\":\n",
    "#         feat_pos = get_anchor_features(explainer, instance, cls, X_train, classification, exp_iter)\n",
    "    \n",
    "#     return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        feat_pos[i]+=n\n",
    "#        feat_pos[i]+=1\n",
    "        n=n-1\n",
    "    #feat_pos = set(feats)\n",
    "    \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_features(cls):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    coef = [abs(val) for val in og_coef]\n",
    "    \n",
    "#     bins = pd.cut(coef, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(coef)) if coef[i] > q1_min]\n",
    "    \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_features(cls, instance):\n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "    \n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "\n",
    "    likelihoods = []\n",
    "    \n",
    "    for i in range(len(means)):\n",
    "        lk = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = scipy.stats.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = lk-alt_lk\n",
    "#        lkhood = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        likelihoods.append(lkhood)\n",
    "    \n",
    "#     bins = pd.cut(likelihoods, 4, retbins = True, duplicates = \"drop\")[1]\n",
    "#     lim = bins[-2]\n",
    "    \n",
    "# #     bins = pd.cut(likelihoods, 10, retbins = True, duplicates = \"drop\")[1]\n",
    "# #     lim_1 = bins[-2]\n",
    "# #     lim_2 = bins[1]\n",
    "    \n",
    "# #     sortedls = sorted(likelihoods, reverse=True)\n",
    "# #     pos = math.ceil(len(likelihoods)/4)\n",
    "# #     lim = likelihoods[pos]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(likelihoods)) if likelihoods[i] >= lim]# or likelihoods[i] <= lim_2]\n",
    "    \n",
    "    return np.abs(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_rankings(cls, instance, cls_method, X_train, feat_list):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        feat_pos = get_reg_features(cls)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        feat_pos = get_nb_features(cls, instance)\n",
    "        \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TuQANu5PLlzK"
   },
   "outputs": [],
   "source": [
    "def permute_instance(instance, i, perm_iter = 100, min_i = 0, max_i=1, mean_i=0, mode=\"permutation\"):\n",
    "    if mode==\"baseline_max\":\n",
    "        n_val = [max_i]*perm_iter\n",
    "    elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "        n_val = [mean_i]*perm_iter\n",
    "    elif mode==\"baseline_min\":\n",
    "        n_val = [min_i]*perm_iter\n",
    "    elif mode==\"baseline_0\":\n",
    "        n_val = [0]*perm_iter\n",
    "    else:\n",
    "        n_val = np.random.uniform(min_i, max_i, perm_iter)\n",
    "    \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    permutations[i] = n_val\n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_values(instance, i, perm_iter = 100, min_i = 0, max_i=1, mean_i=0, unique_values=[0,1], mode=\"permutation\"):\n",
    "    if mode==\"baseline_max\":\n",
    "        n_val = [max_i]*perm_iter\n",
    "    elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "        n_val = [mean_i]*perm_iter\n",
    "    elif mode==\"baseline_min\":\n",
    "        n_val = [min_i]*perm_iter\n",
    "    elif mode==\"baseline_0\":\n",
    "        n_val = [0]*perm_iter\n",
    "    else:\n",
    "        n_val = np.random.choice(unique_values, perm_iter)\n",
    "    \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    permutations[i] = n_val\n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "3VkV42oeLlzL"
   },
   "outputs": [],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset = \"nursery\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "classification = True\n",
    "# xai_method = \"SHAP\"\n",
    "\n",
    "modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "random_state = 22\n",
    "exp_iter = 10\n",
    "perm_iter = 1000\n",
    "\n",
    "save_to = \"%s/%s/\" % (PATH, dataset)\n",
    "dataset_folder = \"%s/datasets/\" % (save_to)\n",
    "final_folder = \"%s/%s/\" % (save_to, cls_method)\n",
    "\n",
    "#Get datasets\n",
    "X_train = pd.read_csv(dataset_folder+dataset+\"_Xtrain.csv\", index_col=False, sep = \";\")\n",
    "y_train = pd.read_csv(dataset_folder+dataset+\"_Ytrain.csv\", index_col=False, sep = \";\")\n",
    "test_x = pd.read_csv(final_folder+\"test_sample.csv\", index_col=False, sep = \";\").values\n",
    "results = pd.read_csv(os.path.join(final_folder,\"results.csv\"), index_col=False, sep = \";\")\n",
    "actual = results[\"Actual\"].values\n",
    "\n",
    "with open(dataset_folder+\"col_dict.json\", \"r\") as f:\n",
    "    col_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "feat_list = [each.replace(' ','_') for each in X_train.columns]\n",
    "\n",
    "cls = joblib.load(save_to+cls_method+\"/cls.joblib\")\n",
    "scaler = joblib.load(save_to+\"/scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "X7OQDhF6LlzL"
   },
   "outputs": [],
   "source": [
    "# if xai_method == \"SHAP\":\n",
    "#   #  explainer = shap.explainers._permutation.Permutation(cls.predict_proba, X_train)\n",
    "#     if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "#         explainer = shap.Explainer(cls)\n",
    "#     elif cls_method == \"nb\":\n",
    "#         if classification:\n",
    "#             masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#             explainer = shap.Explainer(cls.predict_proba, masker)\n",
    "#         else:\n",
    "#             print(\"NB is classification only\")\n",
    "#     elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "#         masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#         explainer = shap.Explainer(cls, masker)\n",
    "#     else:\n",
    "#         explainer = shap.Explainer(cls, X_train)\n",
    "#     print(type(explainer))\n",
    "    \n",
    "# elif xai_method == \"LIME\":\n",
    "#     if col_dict['discrete'] != None:\n",
    "#         cat_cols = [each.replace(' ','_') for each in col_dict['discrete']]\n",
    "#         col_inds = [feat_list.index(each) for each in cat_cols]\n",
    "#     else:\n",
    "#         col_inds = []\n",
    "    \n",
    "#     if classification==True:\n",
    "#         class_names=['Negative','Positive']# negative is 0, positive is 1, 0 is left, 1 is right\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                             class_names=class_names, categorical_features = col_inds,\n",
    "#                                                             discretize_continuous=False)\n",
    "#     else:\n",
    "#         class_names = ['Final Value']\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                            class_names=class_names, discretize_continuous=True, \n",
    "#                                                            categorical_features = col_inds, mode = \"regression\")\n",
    "        \n",
    "# elif xai_method == \"LINDA\":\n",
    "#     test_dict = learning.generate_local_predictions( test_x, results[\"Actual\"].values, cls, scaler, None )\n",
    "# #    feat_list = feat_list+[\"Result\"]\n",
    "\n",
    "#     explainer = None\n",
    "\n",
    "# elif xai_method == \"ACV\":\n",
    "#     explainer = joblib.load(save_to+cls_method+\"/acv_explainer.joblib\")\n",
    "    \n",
    "# elif xai_method == \"Anchor\":\n",
    "#     if classification:\n",
    "#         class_names=['Negative','Positive']\n",
    "#         explainer = anchor_tabular.AnchorTabularExplainer(class_names, feat_list, X_train.values)\n",
    "#     else:\n",
    "#         raise Exception(\"Anchor only works for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cls_method == \"nb\":\n",
    "    \n",
    "#     probas = cls.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_isotonic = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"isotonic\")\n",
    "#     clf_isotonic.fit(X_train, y_train)\n",
    "#     isotonic_probas = clf_isotonic.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_sigmoid = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"sigmoid\")\n",
    "#     clf_sigmoid.fit(X_train, y_train)\n",
    "#     sigmoid_probas = clf_sigmoid.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     cls_score = brier_score_loss(results[\"Actual\"], probas)\n",
    "#     iso_score = brier_score_loss(results[\"Actual\"], isotonic_probas)\n",
    "#     sig_score = brier_score_loss(results[\"Actual\"], sigmoid_probas)\n",
    "    \n",
    "#     if iso_score < sig_score and iso_score < cls_score:\n",
    "#         print(\"Winner is iso\")\n",
    "#         cls = clf_isotonic.calibrated_classifiers_[0].base_estimator\n",
    "#     elif sig_score < iso_score and sig_score < cls_score:\n",
    "#         print(\"Winner is sigmoid\")\n",
    "#         cls = clf_sigmoid.calibrated_classifiers_[0].base_estimator\n",
    "#     else:\n",
    "#         cls = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "aiQNPNswLlzM"
   },
   "outputs": [],
   "source": [
    "min_X = np.min(X_train)\n",
    "max_X = np.max(X_train)\n",
    "mean_X = np.mean(X_train, axis=0)\n",
    "unique_values = pd.Series({col: X_train[col].unique() for col in X_train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parents_great_pret      [0.0, 1.0]\n",
       "parents_pretentious     [0.0, 1.0]\n",
       "parents_usual           [1.0, 0.0]\n",
       "has_nurs_critical       [0.0, 1.0]\n",
       "has_nurs_improper       [0.0, 1.0]\n",
       "has_nurs_less_proper    [0.0, 1.0]\n",
       "has_nurs_proper         [0.0, 1.0]\n",
       "has_nurs_very_crit      [1.0, 0.0]\n",
       "form_complete           [0.0, 1.0]\n",
       "form_completed          [1.0, 0.0]\n",
       "form_foster             [0.0, 1.0]\n",
       "form_incomplete         [0.0, 1.0]\n",
       "children_1              [0.0, 1.0]\n",
       "children_2              [0.0, 1.0]\n",
       "children_3              [0.0, 1.0]\n",
       "children_more           [1.0, 0.0]\n",
       "housing_convenient      [1.0, 0.0]\n",
       "housing_critical        [0.0, 1.0]\n",
       "housing_less_conv       [0.0, 1.0]\n",
       "finance_convenient      [0.0, 1.0]\n",
       "finance_inconv          [1.0, 0.0]\n",
       "social_nonprob          [0.0, 1.0]\n",
       "social_problematic      [1.0, 0.0]\n",
       "social_slightly_prob    [0.0, 1.0]\n",
       "health_not_recom        [0.0, 1.0]\n",
       "health_priority         [0.0, 1.0]\n",
       "health_recommended      [1.0, 0.0]\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'continuous': None,\n",
       " 'discrete': ['parents_great_pret',\n",
       "  'parents_pretentious',\n",
       "  'parents_usual',\n",
       "  'has_nurs_critical',\n",
       "  'has_nurs_improper',\n",
       "  'has_nurs_less_proper',\n",
       "  'has_nurs_proper',\n",
       "  'has_nurs_very_crit',\n",
       "  'form_complete',\n",
       "  'form_completed',\n",
       "  'form_foster',\n",
       "  'form_incomplete',\n",
       "  'children_1',\n",
       "  'children_2',\n",
       "  'children_3',\n",
       "  'children_more',\n",
       "  'housing_convenient',\n",
       "  'housing_critical',\n",
       "  'housing_less_conv',\n",
       "  'finance_convenient',\n",
       "  'finance_inconv',\n",
       "  'social_nonprob',\n",
       "  'social_problematic',\n",
       "  'social_slightly_prob',\n",
       "  'health_not_recom',\n",
       "  'health_priority',\n",
       "  'health_recommended']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if X_train.columns[-3] in col_dict[\"continuous\"]:\n",
    "#     print(True)\n",
    "# else:\n",
    "#     print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "nq_FxFrVLlzM",
    "outputId": "5aa1fc25-ef58-4357-fc0a-758bcdb2fc62",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a9882dde0542f2b11cb394c8d3f7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99071e1f092469da2d57def8315c1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4286b6cdee45a5b282528863faada7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a86b8111534179bf9995732dc64e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a1cb0a40a34d1882590350f6927c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mode in modes:\n",
    "    ktb_list = []\n",
    "    true_v_mape = []\n",
    "    true_v_rmse = []\n",
    "    true_v_r2 = []\n",
    "\n",
    "    for i in tqdm_notebook(range(len(test_x))):\n",
    "        instance = test_x[i]\n",
    "\n",
    "        tr = get_true_rankings(cls, instance, cls_method, X_train, feat_list)\n",
    "\n",
    "        if classification:\n",
    "            pred = cls.predict(instance.reshape(1, -1))\n",
    "            proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "    #        p1_list = [pred]*perm_iter\n",
    "            p1_list = list(proba)*perm_iter\n",
    "#         else:\n",
    "#             pred = cls.predict(instance.reshape(1, -1)).reshape(1)\n",
    "#             p1_list = [pred]*perm_iter\n",
    "\n",
    "    #     pred = cls.predict(instance.reshape(1, -1)).reshape(1)\n",
    "    #     p1_list = [pred]*perm_iter\n",
    "\n",
    "        perm_mape = np.zeros(len(instance))\n",
    "        perm_rmse = np.zeros(len(instance))\n",
    "        perm_r2 = np.zeros(len(instance))\n",
    "\n",
    "        for j in range(len(instance)):\n",
    "           # print(\"Permuting\", feat_list[j])\n",
    "\n",
    "            if col_dict[\"continuous\"] != None:\n",
    "                if X_train.columns[j] in col_dict[\"continuous\"]:\n",
    "                    permutations = permute_instance(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], mode)\n",
    "                else:\n",
    "                    permutations = cycle_values(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], unique_values[X_train.columns[j]], mode)\n",
    "            else:\n",
    "                permutations = cycle_values(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], unique_values[X_train.columns[j]], mode)\n",
    "\n",
    "            if classification:\n",
    "    #             p2_list = cls.predict_proba(permutations)[:, pred].reshape(perm_iter)\n",
    "    #             perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "                p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "           #     perm_acc[j] = 1-accuracy_score(p1_list, p2_list)\n",
    "                perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "                perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "                perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "    #             perm_mape[j] = len([np for np in new_preds if np!=pred])\n",
    "    #             print(pred)\n",
    "    #             print(new_preds)\n",
    "    #             print(perm_mape[j])\n",
    "#            else:\n",
    "#                 p2_list = cls.predict(permutations).reshape(perm_iter)\n",
    "#                 perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "            #p2_list = cls.predict(permutations).reshape(perm_iter)\n",
    "            #perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "        #print(\"Final MAPE for all features:\", perm_mape)\n",
    "        mape_corr = scipy.stats.kendalltau(tr, perm_mape, variant=\"b\")[0]\n",
    "        rmse_corr = scipy.stats.kendalltau(tr, perm_rmse, variant=\"b\")[0]\n",
    "        r2_corr = scipy.stats.kendalltau(tr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "        #if xai_method==\"LINDA\":\n",
    "        #    instance = test_dict[i]\n",
    "\n",
    "        #avg_explanation, abs_explanation = get_explanation_features(explainer, instance, cls, scaler, dataset, classification, exp_iter, xai_method, \n",
    "        #                                                        feat_list, X_train.values, y_train)\n",
    "        #print(\"Average explanation:\", abs_explanation)\n",
    "\n",
    "        #if np.all(abs_explanation==abs_explanation[0]):\n",
    "        #    ktb=0\n",
    "        #else:\n",
    "        #    ktb = scipy.stats.kendalltau(perm_mape, abs_explanation, variant=\"b\")[0]\n",
    "\n",
    "      #  ktb_list.append(ktb)\n",
    "        true_v_mape.append(mape_corr)\n",
    "        true_v_rmse.append(rmse_corr)\n",
    "        true_v_r2.append(r2_corr)\n",
    "\n",
    "    #results[xai_method+\"_KT-B\"] = ktb_list\n",
    "    results[\"MAPE Correctness\"] = true_v_mape\n",
    "    results[\"RMSE Correctness\"] = true_v_rmse\n",
    "    results[\"R2 Correctness\"] = true_v_r2\n",
    "    results.to_csv(os.path.join(save_to, cls_method, mode+\"_results.csv\"), index = False, sep = \";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fidelity_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
