{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4733,
     "status": "ok",
     "timestamp": 1652073020845,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Efp_y7Q2LpyR",
    "outputId": "9783452f-45e7-4e82-afd4-6a562593daf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# import os\n",
    "\n",
    "# os.chdir('drive/MyDrive/three-phase-fidelity-wip')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # # # !pip install -r requirements.txt\n",
    "# !pip install ipython==7.31.1\n",
    "# !pip install importlib-metadata==4.10.0\n",
    "# !pip install acv-exp\n",
    "# !pip install hyperopt==0.27\n",
    "# # # ## !pip freeze > requirements.txt\n",
    "\n",
    "# !pip install lime\n",
    "# !pip install shap\n",
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2207,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "yYxMeRJPLlzF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, mean_squared_error, r2_score\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import learning\n",
    "import pyAgrum\n",
    "#from acv_explainers import ACXplainer\n",
    "\n",
    "#from anchor import anchor_tabular\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "hv1zKoQOLlzI"
   },
   "outputs": [],
   "source": [
    "# def get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "    \n",
    "#     shap_exp = []\n",
    "    \n",
    "#     pred = cls.predict(instance.reshape(1, -1))\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if type(explainer) == shap.explainers._tree.Tree:\n",
    "#             exp = explainer(instance, check_additivity = False).values\n",
    "#         else:\n",
    "#             exp = explainer(instance.reshape(1, -1)).values\n",
    "                \n",
    "#         if exp.shape == (1, len(feat_list), 2):\n",
    "#             exp = exp[0]\n",
    "            \n",
    "#         #print(exp.shape)\n",
    "        \n",
    "#         if exp.shape == (len(feat_list), 2):\n",
    "#             exp = np.array([feat[pred] for feat in exp]).reshape(len(feat_list))\n",
    "#         elif exp.shape == (1, len(feat_list)) or exp.shape == (len(feat_list), 1):\n",
    "#             exp = exp.reshape(len(feat_list))\n",
    "            \n",
    "#         shap_exp.append(exp)\n",
    "        \n",
    "        \n",
    "#     if np.array(shap_exp).shape != (exp_iter, len(feat_list)):\n",
    "#         raise Exception(\"Explanation shape is not correct. It is\", np.array(shap_exp).shape, \"instead of the expected\", (exp_iter, len(feat_list)))\n",
    "            \n",
    "#     avg_val = np.average(shap_exp, axis = 0)\n",
    "#     abs_val = [abs(val) for val in avg_val]\n",
    "    \n",
    "#     return avg_val, abs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "EwoHjZeWLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "#     lime_exp = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if classification==True:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict_proba, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "#         else:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "            \n",
    "#     weights = [[] for each in feat_list]\n",
    "#     for exp in lime_exp:\n",
    "#         feat = exp[0]\n",
    "#         if '<' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('<')\n",
    "#         elif '>' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('>')\n",
    "#         else:\n",
    "#             parts = feat.split(\"=\")\n",
    "        \n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         n = feat_list.index(feat_name)\n",
    "#         weights[n].append(exp[1])\n",
    "    \n",
    "#     weights = np.transpose(weights)\n",
    "#     avg_weight = np.average(np.array(weights), axis = 0)\n",
    "#     abs_weight = [abs(weight) for weight in avg_weight]\n",
    "    \n",
    "#     bins = pd.cut(abs_weight, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     sorted_weight = np.copy(abs_weight)\n",
    "#     sorted_weight.sort()\n",
    "    \n",
    "#     lime_features = [i for i in range(len(feat_list)) if abs_weight[i] >= q1_min]\n",
    "    \n",
    "#     return avg_weight, abs_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list):\n",
    "#     label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "#     feat_pos = []\n",
    "#     lkhoods = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         [bn, inference, infoBN] = learning.generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "#                                                                        None, scaler, cls, save_to+\"/\"+cls_method+\"/\", dataset, show_in_notebook = False)\n",
    "        \n",
    "#         ie = pyAgrum.LazyPropagation(bn)\n",
    "#         result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#         result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#         row = instance['original_vector']\n",
    "#         #print(row)\n",
    "\n",
    "#         likelihood = [0]*len(feat_list)\n",
    "\n",
    "#         for j in range(len(feat_list)):\n",
    "#             var_labels = bn.variable(feat_list[j]).labels()\n",
    "#             str_bins = list(var_labels)\n",
    "#             bins = []\n",
    "\n",
    "#             for disc_bin in str_bins:\n",
    "#                 disc_bin = disc_bin.strip('\"(]')\n",
    "#                 cat = [float(val) for val in disc_bin.split(',')]\n",
    "#                 bins.append(cat)\n",
    "\n",
    "#             for k in range(len(bins)):\n",
    "#                 if k == 0 and row[j] <= bins[k][0]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif k == len(bins)-1 and row[j] >= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif row[j] > bins[k][0] and row[j] <= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "\n",
    "#             ie = pyAgrum.LazyPropagation(bn)\n",
    "#             ie.setEvidence({feat_list[j]: feat_bin})\n",
    "#             ie.makeInference()\n",
    "            \n",
    "#             result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#             new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#             #print(result_proba, new_proba)\n",
    "#             proba_change = result_proba-new_proba\n",
    "#             likelihood[j] = abs(proba_change)\n",
    "\n",
    "#         lkhoods.append(likelihood)\n",
    "        \n",
    "#     bins = pd.cut(np.mean(lkhoods, axis=0), 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "\n",
    "#     #If fixing all features produces the same result for the class,\n",
    "#     #return all features\n",
    "#     if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "#         feat_pos.extend(range(len(feat_list)))\n",
    "#         #print(lkhoods)\n",
    "#     else:\n",
    "#         feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "#     feat_pos = set(feat_pos)\n",
    "#     #print(feat_pos)\n",
    "    \n",
    "#     return np.mean(lkhoods, axis=0), np.mean(lkhoods, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "r6DGbiuVLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list):\n",
    "#     instance = instance.reshape(1, -1)\n",
    "#     y = cls.predict(instance)\n",
    "\n",
    "#     feat_pos = []\n",
    "#     feat_imp = []\n",
    "\n",
    "#     for i in range(exp_iter):\n",
    "#         sdp_importance, sdp_index, size, sdp = explainer.importance_sdp_rf(instance, y, X_train, y_train)\n",
    "#         feat_pos.extend(sdp_index[0, :size[0]])\n",
    "        \n",
    "#         sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train)\n",
    "#         #print(np.array(sufficient_expl).shape)\n",
    "#         lximp = explainer.compute_local_sdp(len(feat_list), sufficient_expl[0])\n",
    "#         feat_imp.append(lximp)\n",
    "\n",
    "\n",
    "#     feats = Counter(feat_pos)\n",
    "#     imp = feats.items()\n",
    "\n",
    "#     occ = np.zeros(len(feat_list))\n",
    "\n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     avg_imp = np.mean(feat_imp, axis=0)\n",
    "#     abs_imp = [abs(imp) for imp in avg_imp]\n",
    "\n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "#     #print(\"Feature importance score:\", avg_imp)\n",
    "#     return occ, occ\n",
    "#     #return avg_imp, abs_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_anchor_features(explainer, instance, cls, train_data, classification, exp_iter):\n",
    "#     anchor_exp = []\n",
    "#     for i in range(exp_iter):\n",
    "#         if classification == True:\n",
    "#             anchor_exp.extend(explainer.explain_instance(instance, cls.predict).names())\n",
    "#     print(anchor_exp)\n",
    "\n",
    "#     locs = []\n",
    "#     for exp in anchor_exp:\n",
    "#         feat = exp.replace(\"= \",'')\n",
    "#         #print(feat)\n",
    "#         if '<' in feat:\n",
    "#             parts = feat.split('<')\n",
    "#             #print(\"less than\", parts)\n",
    "#         elif '>' in feat:\n",
    "#             parts = feat.split('>')\n",
    "#             #print(\"more than\", parts)\n",
    "\n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         locs.append(feat_list.index(feat_name))\n",
    "    \n",
    "#     feats = Counter(locs)\n",
    "#     imp = feats.items()\n",
    "    \n",
    "#     #print(imp)\n",
    "    \n",
    "#     occ = np.zeros(len(feat_list))\n",
    "    \n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "    \n",
    "#     return occ, occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "gXQO43zuLlzK"
   },
   "outputs": [],
   "source": [
    "# def get_explanation_features(explainer, instance, cls, scaler, dataset, classification, exp_iter, xai_method, feat_list, X_train, y_train):\n",
    "#     if xai_method == \"SHAP\":\n",
    "#         feat_pos = get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LIME\":\n",
    "#         feat_pos = get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LINDA\":\n",
    "#         feat_pos = get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list)\n",
    "\n",
    "#     elif xai_method == \"ACV\":\n",
    "#         feat_pos = get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"Anchor\":\n",
    "#         feat_pos = get_anchor_features(explainer, instance, cls, X_train, classification, exp_iter)\n",
    "    \n",
    "#     return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        feat_pos[i]+=n\n",
    "#        feat_pos[i]+=1\n",
    "        n=n-1\n",
    "    #feat_pos = set(feats)\n",
    "    \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_features(cls):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    coef = [abs(val) for val in og_coef]\n",
    "    \n",
    "#     bins = pd.cut(coef, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(coef)) if coef[i] > q1_min]\n",
    "    \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_features(cls, instance):\n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "    \n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "\n",
    "    likelihoods = []\n",
    "    \n",
    "    for i in range(len(means)):\n",
    "        lk = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = scipy.stats.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = lk-alt_lk\n",
    "#        lkhood = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        likelihoods.append(lkhood)\n",
    "    \n",
    "#     bins = pd.cut(likelihoods, 4, retbins = True, duplicates = \"drop\")[1]\n",
    "#     lim = bins[-2]\n",
    "    \n",
    "# #     bins = pd.cut(likelihoods, 10, retbins = True, duplicates = \"drop\")[1]\n",
    "# #     lim_1 = bins[-2]\n",
    "# #     lim_2 = bins[1]\n",
    "    \n",
    "# #     sortedls = sorted(likelihoods, reverse=True)\n",
    "# #     pos = math.ceil(len(likelihoods)/4)\n",
    "# #     lim = likelihoods[pos]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(likelihoods)) if likelihoods[i] >= lim]# or likelihoods[i] <= lim_2]\n",
    "    \n",
    "    return np.abs(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_rankings(cls, instance, cls_method, X_train, feat_list):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        feat_pos = get_reg_features(cls)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        feat_pos = get_nb_features(cls, instance)\n",
    "        \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TuQANu5PLlzK"
   },
   "outputs": [],
   "source": [
    "def permute_instance(instance, i, perm_iter = 100, min_i = 0, max_i=1, mean_i=0, mode=\"permutation\"):\n",
    "    if mode==\"baseline_max\":\n",
    "        n_val = [max_i]*perm_iter\n",
    "    elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "        n_val = [mean_i]*perm_iter\n",
    "    elif mode==\"baseline_min\":\n",
    "        n_val = [min_i]*perm_iter\n",
    "    elif mode==\"baseline_0\":\n",
    "        n_val = [0]*perm_iter\n",
    "    else:\n",
    "        n_val = np.random.uniform(min_i, max_i, perm_iter)\n",
    "    \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    permutations[i] = n_val\n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_values(instance, i, perm_iter = 100, min_i = 0, max_i=1, mean_i=0, unique_values=[0,1], mode=\"permutation\"):\n",
    "    if mode==\"baseline_max\":\n",
    "        n_val = [max_i]*perm_iter\n",
    "    elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "        n_val = [mean_i]*perm_iter\n",
    "    elif mode==\"baseline_min\":\n",
    "        n_val = [min_i]*perm_iter\n",
    "    elif mode==\"baseline_0\":\n",
    "        n_val = [0]*perm_iter\n",
    "    else:\n",
    "        n_val = np.random.choice(unique_values, perm_iter)\n",
    "    \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    permutations[i] = n_val\n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"prefix\"\n",
    "cls_encoding = \"index\"\n",
    "cls_method = \"nb\"\n",
    "\n",
    "classification=True\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "perm_iter = 1000\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\": [\"production\"] \n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "3VkV42oeLlzL"
   },
   "outputs": [],
   "source": [
    "# # path to project folder\n",
    "# # please change to your own\n",
    "# PATH = os.getcwd()\n",
    "\n",
    "# dataset = \"nursery\"\n",
    "# cls_method = \"nb\"\n",
    "\n",
    "# classification = True\n",
    "# # xai_method = \"SHAP\"\n",
    "\n",
    "# modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "# random_state = 22\n",
    "# exp_iter = 10\n",
    "# perm_iter = 1000\n",
    "\n",
    "# save_to = \"%s/%s/\" % (PATH, dataset)\n",
    "# dataset_folder = \"%s/datasets/\" % (save_to)\n",
    "# final_folder = \"%s/%s/\" % (save_to, cls_method)\n",
    "\n",
    "# #Get datasets\n",
    "# X_train = pd.read_csv(dataset_folder+dataset+\"_Xtrain.csv\", index_col=False, sep = \";\")\n",
    "# y_train = pd.read_csv(dataset_folder+dataset+\"_Ytrain.csv\", index_col=False, sep = \";\")\n",
    "# test_x = pd.read_csv(final_folder+\"test_sample.csv\", index_col=False, sep = \";\").values\n",
    "# results = pd.read_csv(os.path.join(final_folder,\"results.csv\"), index_col=False, sep = \";\")\n",
    "# actual = results[\"Actual\"].values\n",
    "\n",
    "# with open(dataset_folder+\"col_dict.json\", \"r\") as f:\n",
    "#     col_dict = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# feat_list = [each.replace(' ','_') for each in X_train.columns]\n",
    "\n",
    "# cls = joblib.load(save_to+cls_method+\"/cls.joblib\")\n",
    "# scaler = joblib.load(save_to+\"/scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "X7OQDhF6LlzL"
   },
   "outputs": [],
   "source": [
    "# if xai_method == \"SHAP\":\n",
    "#   #  explainer = shap.explainers._permutation.Permutation(cls.predict_proba, X_train)\n",
    "#     if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "#         explainer = shap.Explainer(cls)\n",
    "#     elif cls_method == \"nb\":\n",
    "#         if classification:\n",
    "#             masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#             explainer = shap.Explainer(cls.predict_proba, masker)\n",
    "#         else:\n",
    "#             print(\"NB is classification only\")\n",
    "#     elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "#         masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#         explainer = shap.Explainer(cls, masker)\n",
    "#     else:\n",
    "#         explainer = shap.Explainer(cls, X_train)\n",
    "#     print(type(explainer))\n",
    "    \n",
    "# elif xai_method == \"LIME\":\n",
    "#     if col_dict['discrete'] != None:\n",
    "#         cat_cols = [each.replace(' ','_') for each in col_dict['discrete']]\n",
    "#         col_inds = [feat_list.index(each) for each in cat_cols]\n",
    "#     else:\n",
    "#         col_inds = []\n",
    "    \n",
    "#     if classification==True:\n",
    "#         class_names=['Negative','Positive']# negative is 0, positive is 1, 0 is left, 1 is right\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                             class_names=class_names, categorical_features = col_inds,\n",
    "#                                                             discretize_continuous=False)\n",
    "#     else:\n",
    "#         class_names = ['Final Value']\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                            class_names=class_names, discretize_continuous=True, \n",
    "#                                                            categorical_features = col_inds, mode = \"regression\")\n",
    "        \n",
    "# elif xai_method == \"LINDA\":\n",
    "#     test_dict = learning.generate_local_predictions( test_x, results[\"Actual\"].values, cls, scaler, None )\n",
    "# #    feat_list = feat_list+[\"Result\"]\n",
    "\n",
    "#     explainer = None\n",
    "\n",
    "# elif xai_method == \"ACV\":\n",
    "#     explainer = joblib.load(save_to+cls_method+\"/acv_explainer.joblib\")\n",
    "    \n",
    "# elif xai_method == \"Anchor\":\n",
    "#     if classification:\n",
    "#         class_names=['Negative','Positive']\n",
    "#         explainer = anchor_tabular.AnchorTabularExplainer(class_names, feat_list, X_train.values)\n",
    "#     else:\n",
    "#         raise Exception(\"Anchor only works for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cls_method == \"nb\":\n",
    "    \n",
    "#     probas = cls.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_isotonic = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"isotonic\")\n",
    "#     clf_isotonic.fit(X_train, y_train)\n",
    "#     isotonic_probas = clf_isotonic.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_sigmoid = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"sigmoid\")\n",
    "#     clf_sigmoid.fit(X_train, y_train)\n",
    "#     sigmoid_probas = clf_sigmoid.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     cls_score = brier_score_loss(results[\"Actual\"], probas)\n",
    "#     iso_score = brier_score_loss(results[\"Actual\"], isotonic_probas)\n",
    "#     sig_score = brier_score_loss(results[\"Actual\"], sigmoid_probas)\n",
    "    \n",
    "#     if iso_score < sig_score and iso_score < cls_score:\n",
    "#         print(\"Winner is iso\")\n",
    "#         cls = clf_isotonic.calibrated_classifiers_[0].base_estimator\n",
    "#     elif sig_score < iso_score and sig_score < cls_score:\n",
    "#         print(\"Winner is sigmoid\")\n",
    "#         cls = clf_sigmoid.calibrated_classifiers_[0].base_estimator\n",
    "#     else:\n",
    "#         cls = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "aiQNPNswLlzM"
   },
   "outputs": [],
   "source": [
    "# min_X = np.min(X_train)\n",
    "# max_X = np.max(X_train)\n",
    "# mean_X = np.mean(X_train, axis=0)\n",
    "# unique_values = pd.Series({col: X_train[col].unique() for col in X_train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if X_train.columns[-3] in col_dict[\"continuous\"]:\n",
    "#     print(True)\n",
    "# else:\n",
    "#     print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "nq_FxFrVLlzM",
    "outputId": "5aa1fc25-ef58-4357-fc0a-758bcdb2fc62",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e4d06e53d040698d44b9aa99e1d55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "permutation Mean correlation: -0.8138038460194186\n",
      "baseline_min Mean correlation: -0.6924256511258209\n",
      "baseline_mean Mean correlation: -0.7693594015749742\n",
      "baseline_max Mean correlation: -0.8026927349083075\n",
      "baseline_0 Mean correlation: -0.6924256511258209\n",
      "Bucket 2\n",
      "permutation Mean correlation: -0.6634578127217019\n",
      "baseline_min Mean correlation: -0.5745391324717293\n",
      "baseline_mean Mean correlation: -0.6731843575418994\n",
      "baseline_max Mean correlation: -0.659112346863032\n",
      "baseline_0 Mean correlation: -0.5745391324717293\n",
      "Bucket 3\n",
      "permutation Mean correlation: -0.6784444054636882\n",
      "baseline_min Mean correlation: -0.4143929226952225\n",
      "baseline_mean Mean correlation: -0.5299989644044821\n",
      "baseline_max Mean correlation: -0.5594801025448266\n",
      "baseline_0 Mean correlation: -0.4143929226952225\n",
      "Bucket 4\n",
      "permutation Mean correlation: -0.540529203507564\n",
      "baseline_min Mean correlation: 0.1414291785013539\n",
      "baseline_mean Mean correlation: -0.3856603310893573\n",
      "baseline_max Mean correlation: -0.4492703997775157\n",
      "baseline_0 Mean correlation: 0.1414291785013539\n",
      "Bucket 5\n",
      "permutation Mean correlation: -0.47890362521076796\n",
      "baseline_min Mean correlation: 0.24631546394619905\n",
      "baseline_mean Mean correlation: -0.22337384899795243\n",
      "baseline_max Mean correlation: -0.4442731533381997\n",
      "baseline_0 Mean correlation: 0.24631546394619905\n",
      "Bucket 6\n",
      "permutation Mean correlation: -0.47543782874812046\n",
      "baseline_min Mean correlation: 0.1946476223168822\n",
      "baseline_mean Mean correlation: -0.052093812615249625\n",
      "baseline_max Mean correlation: -0.4844846150190016\n",
      "baseline_0 Mean correlation: 0.1946476223168822\n",
      "Bucket 7\n",
      "permutation Mean correlation: -0.561675342591211\n",
      "baseline_min Mean correlation: 0.01883405720969219\n",
      "baseline_mean Mean correlation: -0.41188594647255283\n",
      "baseline_max Mean correlation: -0.4434626505056728\n",
      "baseline_0 Mean correlation: 0.01883405720969219\n",
      "Bucket 8\n",
      "permutation Mean correlation: -0.3459958017569297\n",
      "baseline_min Mean correlation: 0.133884554668887\n",
      "baseline_mean Mean correlation: -0.18312200978384846\n",
      "baseline_max Mean correlation: -0.2733128540055289\n",
      "baseline_0 Mean correlation: 0.133884554668887\n",
      "Bucket 9\n",
      "permutation Mean correlation: -0.6192663213610206\n",
      "baseline_min Mean correlation: -0.027455765854249633\n",
      "baseline_mean Mean correlation: -0.47515908671830975\n",
      "baseline_max Mean correlation: -0.475119652138778\n",
      "baseline_0 Mean correlation: -0.027455765854249633\n",
      "Bucket 10\n",
      "permutation Mean correlation: -0.3925189504777852\n",
      "baseline_min Mean correlation: 0.024525141827203382\n",
      "baseline_mean Mean correlation: -0.37583770994789273\n",
      "baseline_max Mean correlation: -0.2808248430136179\n",
      "baseline_0 Mean correlation: 0.024525141827203382\n",
      "Bucket 11\n",
      "permutation Mean correlation: -0.8153054193986987\n",
      "baseline_min Mean correlation: -0.04575681097042595\n",
      "baseline_mean Mean correlation: -0.5751157092117652\n",
      "baseline_max Mean correlation: -0.6834890941731246\n",
      "baseline_0 Mean correlation: -0.04575681097042595\n",
      "Bucket 12\n",
      "permutation Mean correlation: -0.6317056948008841\n",
      "baseline_min Mean correlation: -0.027128056807865446\n",
      "baseline_mean Mean correlation: -0.4710793944048203\n",
      "baseline_max Mean correlation: -0.48796939748624407\n",
      "baseline_0 Mean correlation: -0.027128056807865446\n",
      "Bucket 13\n",
      "permutation Mean correlation: -0.6947986103544388\n",
      "baseline_min Mean correlation: -0.08297784672947857\n",
      "baseline_mean Mean correlation: -0.5810278900292842\n",
      "baseline_max Mean correlation: -0.6133639168513513\n",
      "baseline_0 Mean correlation: -0.08297784672947857\n",
      "Bucket 14\n",
      "permutation Mean correlation: -0.5373964800767073\n",
      "baseline_min Mean correlation: -0.08639213503116205\n",
      "baseline_mean Mean correlation: -0.45093432791997073\n",
      "baseline_max Mean correlation: -0.37274228563875106\n",
      "baseline_0 Mean correlation: -0.08639213503116205\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for dataset_name in datasets:\n",
    "        \n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            \n",
    "            #Identify feature names\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "            cat_cols = trainingdata.columns[cats]\n",
    "            \n",
    "            #scale data if necessary\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "                trainingdata = pd.DataFrame(trainingdata, columns=feat_list)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "\n",
    "            #scale data if necessary\n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "            \n",
    "\n",
    "            #trainingdata = pd.DataFrame(trainingdata, columns=feat_list)\n",
    "            min_X = np.min(trainingdata)\n",
    "            max_X = np.max(trainingdata)\n",
    "            mean_X = np.mean(trainingdata, axis=0)\n",
    "            unique_values = pd.Series({col: trainingdata[col].unique() for col in trainingdata.columns})\n",
    "\n",
    "            for mode in modes:\n",
    "                ktb_list = []\n",
    "                true_v_mape = []\n",
    "                true_v_rmse = []\n",
    "                true_v_r2 = []\n",
    "\n",
    "                #for i in tqdm_notebook(range(len(sample_instances))):\n",
    "                for i in range(len(sample_instances)):\n",
    "                    instance = sample_instances[i]\n",
    "\n",
    "                    tr = get_true_rankings(cls, instance, cls_method, trainingdata, feat_list)\n",
    "\n",
    "                    if classification:\n",
    "                        pred = cls.predict(instance.reshape(1, -1))\n",
    "                        proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "                #        p1_list = [pred]*perm_iter\n",
    "                        p1_list = list(proba)*perm_iter\n",
    "            #         else:\n",
    "            #             pred = cls.predict(instance.reshape(1, -1)).reshape(1)\n",
    "            #             p1_list = [pred]*perm_iter\n",
    "\n",
    "                #     pred = cls.predict(instance.reshape(1, -1)).reshape(1)\n",
    "                #     p1_list = [pred]*perm_iter\n",
    "\n",
    "                    perm_mape = np.zeros(len(instance))\n",
    "                    perm_rmse = np.zeros(len(instance))\n",
    "                    perm_r2 = np.zeros(len(instance))\n",
    "\n",
    "                    for j in range(len(instance)):\n",
    "                       # print(\"Permuting\", feat_list[j])\n",
    "\n",
    "                        if cats != None:\n",
    "                            if trainingdata.columns[j] in cat_cols:\n",
    "                                permutations = cycle_values(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], unique_values[trainingdata.columns[j]], mode)\n",
    "                            else:\n",
    "                                permutations = permute_instance(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], mode)\n",
    "                        else:\n",
    "                            permutations = permute_instance(instance, j, perm_iter, min_X[j], max_X[j], mean_X[j], mode)\n",
    "\n",
    "                        if classification:\n",
    "                #             p2_list = cls.predict_proba(permutations)[:, pred].reshape(perm_iter)\n",
    "                #             perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "                            p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "                       #     perm_acc[j] = 1-accuracy_score(p1_list, p2_list)\n",
    "                            perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "                            perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "                            perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "                #             perm_mape[j] = len([np for np in new_preds if np!=pred])\n",
    "                #             print(pred)\n",
    "                #             print(new_preds)\n",
    "                #             print(perm_mape[j])\n",
    "            #            else:\n",
    "            #                 p2_list = cls.predict(permutations).reshape(perm_iter)\n",
    "            #                 perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "                        #p2_list = cls.predict(permutations).reshape(perm_iter)\n",
    "                        #perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "\n",
    "                    #print(\"Final MAPE for all features:\", perm_mape)\n",
    "                    mape_corr = scipy.stats.kendalltau(tr, perm_mape, variant=\"b\")[0]\n",
    "                    rmse_corr = scipy.stats.kendalltau(tr, perm_rmse, variant=\"b\")[0]\n",
    "                    r2_corr = scipy.stats.kendalltau(tr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "                    #if xai_method==\"LINDA\":\n",
    "                    #    instance = test_dict[i]\n",
    "\n",
    "                    #avg_explanation, abs_explanation = get_explanation_features(explainer, instance, cls, scaler, dataset, classification, exp_iter, xai_method, \n",
    "                    #                                                        feat_list, X_train.values, y_train)\n",
    "                    #print(\"Average explanation:\", abs_explanation)\n",
    "\n",
    "                    #if np.all(abs_explanation==abs_explanation[0]):\n",
    "                    #    ktb=0\n",
    "                    #else:\n",
    "                    #    ktb = scipy.stats.kendalltau(perm_mape, abs_explanation, variant=\"b\")[0]\n",
    "\n",
    "                  #  ktb_list.append(ktb)\n",
    "                    true_v_mape.append(mape_corr)\n",
    "                    true_v_rmse.append(rmse_corr)\n",
    "                    true_v_r2.append(r2_corr)\n",
    "                    \n",
    "#                     print(tr)\n",
    "#                     print(perm_r2)\n",
    "#                     print(r2_corr)\n",
    "\n",
    "                #results[xai_method+\"_KT-B\"] = ktb_list\n",
    "                results[\"MAPE Correctness\"] = true_v_mape\n",
    "                results[\"RMSE Correctness\"] = true_v_rmse\n",
    "                results[\"R2 Correctness\"] = true_v_r2\n",
    "                results[\"Mode\"] = [mode]*results.shape[0]\n",
    "                print(mode, \"Mean correlation:\", np.mean(results[\"R2 Correctness\"]))\n",
    "                results.to_csv(os.path.join(save_to,\"samples\", mode+\"_results_bucket_%s.csv\" % (bucketID)), \n",
    "                               index = False, sep = \";\")\n",
    "                all_results.append(results)\n",
    "                \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/all_perm_results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index__Resource_13_other'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdata.columns[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index__Activity_0_A_SUBMITTED-COMPLETE',\n",
       "       'index__Activity_1_A_PARTLYSUBMITTED-COMPLETE',\n",
       "       'index__Activity_2_A_PREACCEPTED-COMPLETE',\n",
       "       'index__Activity_2_W_Afhandelen_leads-SCHEDULE',\n",
       "       'index__Activity_2_W_Beoordelen_fraude-SCHEDULE',\n",
       "       'index__Activity_3_W_Afhandelen_leads-START',\n",
       "       'index__Activity_3_W_Beoordelen_fraude-START',\n",
       "       'index__Activity_3_W_Completeren_aanvraag-SCHEDULE',\n",
       "       'index__Activity_4_A_PREACCEPTED-COMPLETE',\n",
       "       'index__Activity_4_W_Afhandelen_leads-COMPLETE',\n",
       "       ...\n",
       "       'index__Resource_13_11180.0', 'index__Resource_13_11181.0',\n",
       "       'index__Resource_13_11189.0', 'index__Resource_13_112.0',\n",
       "       'index__Resource_13_11200.0', 'index__Resource_13_11201.0',\n",
       "       'index__Resource_13_11202.0', 'index__Resource_13_11203.0',\n",
       "       'index__Resource_13_11259.0', 'index__Resource_13_other'],\n",
       "      dtype='object', length=657)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingdata.columns[cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fidelity_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
