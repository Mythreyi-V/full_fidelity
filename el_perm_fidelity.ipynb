{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4733,
     "status": "ok",
     "timestamp": 1652073020845,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Efp_y7Q2LpyR",
    "outputId": "9783452f-45e7-4e82-afd4-6a562593daf9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# import os\n",
    "\n",
    "# os.chdir('drive/MyDrive/three-phase-fidelity-wip')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# # # # !pip install -r requirements.txt\n",
    "# !pip install ipython==7.31.1\n",
    "# !pip install importlib-metadata==4.10.0\n",
    "# !pip install acv-exp\n",
    "# !pip install hyperopt==0.27\n",
    "# # # ## !pip freeze > requirements.txt\n",
    "\n",
    "# !pip install lime\n",
    "# !pip install shap\n",
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2207,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "yYxMeRJPLlzF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 16:49:19.105483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-09 16:49:19.105533: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, mean_squared_error, r2_score\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy\n",
    "\n",
    "import shap\n",
    "import lime\n",
    "import learning\n",
    "import pyAgrum\n",
    "#from acv_explainers import ACXplainer\n",
    "\n",
    "#from anchor import anchor_tabular\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023049,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "hv1zKoQOLlzI"
   },
   "outputs": [],
   "source": [
    "# def get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "    \n",
    "#     shap_exp = []\n",
    "    \n",
    "#     pred = cls.predict(instance.reshape(1, -1))\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if type(explainer) == shap.explainers._tree.Tree:\n",
    "#             exp = explainer(instance, check_additivity = False).values\n",
    "#         else:\n",
    "#             exp = explainer(instance.reshape(1, -1)).values\n",
    "                \n",
    "#         if exp.shape == (1, len(feat_list), 2):\n",
    "#             exp = exp[0]\n",
    "            \n",
    "#         #print(exp.shape)\n",
    "        \n",
    "#         if exp.shape == (len(feat_list), 2):\n",
    "#             exp = np.array([feat[pred] for feat in exp]).reshape(len(feat_list))\n",
    "#         elif exp.shape == (1, len(feat_list)) or exp.shape == (len(feat_list), 1):\n",
    "#             exp = exp.reshape(len(feat_list))\n",
    "            \n",
    "#         shap_exp.append(exp)\n",
    "        \n",
    "        \n",
    "#     if np.array(shap_exp).shape != (exp_iter, len(feat_list)):\n",
    "#         raise Exception(\"Explanation shape is not correct. It is\", np.array(shap_exp).shape, \"instead of the expected\", (exp_iter, len(feat_list)))\n",
    "            \n",
    "#     avg_val = np.average(shap_exp, axis = 0)\n",
    "#     abs_val = [abs(val) for val in avg_val]\n",
    "    \n",
    "#     return avg_val, abs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "EwoHjZeWLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list):\n",
    "#     lime_exp = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         if classification==True:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict_proba, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "#         else:\n",
    "#             lime_exp.extend(explainer.explain_instance(instance, cls.predict, \n",
    "#                                                 num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "            \n",
    "#     weights = [[] for each in feat_list]\n",
    "#     for exp in lime_exp:\n",
    "#         feat = exp[0]\n",
    "#         if '<' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('<')\n",
    "#         elif '>' in feat:\n",
    "#             feat = exp[0].replace(\"= \",'')\n",
    "#             parts = feat.split('>')\n",
    "#         else:\n",
    "#             parts = feat.split(\"=\")\n",
    "        \n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         n = feat_list.index(feat_name)\n",
    "#         weights[n].append(exp[1])\n",
    "    \n",
    "#     weights = np.transpose(weights)\n",
    "#     avg_weight = np.average(np.array(weights), axis = 0)\n",
    "#     abs_weight = [abs(weight) for weight in avg_weight]\n",
    "    \n",
    "#     bins = pd.cut(abs_weight, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     sorted_weight = np.copy(abs_weight)\n",
    "#     sorted_weight.sort()\n",
    "    \n",
    "#     lime_features = [i for i in range(len(feat_list)) if abs_weight[i] >= q1_min]\n",
    "    \n",
    "#     return avg_weight, abs_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list):\n",
    "#     label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "#     feat_pos = []\n",
    "#     lkhoods = []\n",
    "    \n",
    "#     for i in range(exp_iter):\n",
    "#         [bn, inference, infoBN] = learning.generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "#                                                                        None, scaler, cls, save_to+\"/\"+cls_method+\"/\", dataset, show_in_notebook = False)\n",
    "        \n",
    "#         ie = pyAgrum.LazyPropagation(bn)\n",
    "#         result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#         result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#         row = instance['original_vector']\n",
    "#         #print(row)\n",
    "\n",
    "#         likelihood = [0]*len(feat_list)\n",
    "\n",
    "#         for j in range(len(feat_list)):\n",
    "#             var_labels = bn.variable(feat_list[j]).labels()\n",
    "#             str_bins = list(var_labels)\n",
    "#             bins = []\n",
    "\n",
    "#             for disc_bin in str_bins:\n",
    "#                 disc_bin = disc_bin.strip('\"(]')\n",
    "#                 cat = [float(val) for val in disc_bin.split(',')]\n",
    "#                 bins.append(cat)\n",
    "\n",
    "#             for k in range(len(bins)):\n",
    "#                 if k == 0 and row[j] <= bins[k][0]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif k == len(bins)-1 and row[j] >= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "#                 elif row[j] > bins[k][0] and row[j] <= bins[k][1]:\n",
    "#                     feat_bin = str_bins[k]\n",
    "\n",
    "#             ie = pyAgrum.LazyPropagation(bn)\n",
    "#             ie.setEvidence({feat_list[j]: feat_bin})\n",
    "#             ie.makeInference()\n",
    "            \n",
    "#             result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "#             new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "#             #print(result_proba, new_proba)\n",
    "#             proba_change = result_proba-new_proba\n",
    "#             likelihood[j] = abs(proba_change)\n",
    "\n",
    "#         lkhoods.append(likelihood)\n",
    "        \n",
    "#     bins = pd.cut(np.mean(lkhoods, axis=0), 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "\n",
    "#     #If fixing all features produces the same result for the class,\n",
    "#     #return all features\n",
    "#     if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "#         feat_pos.extend(range(len(feat_list)))\n",
    "#         #print(lkhoods)\n",
    "#     else:\n",
    "#         feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "#     feat_pos = set(feat_pos)\n",
    "#     #print(feat_pos)\n",
    "    \n",
    "#     return np.mean(lkhoods, axis=0), np.mean(lkhoods, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "r6DGbiuVLlzJ"
   },
   "outputs": [],
   "source": [
    "# def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list):\n",
    "#     instance = instance.reshape(1, -1)\n",
    "#     y = cls.predict(instance)\n",
    "\n",
    "#     feat_pos = []\n",
    "#     feat_imp = []\n",
    "\n",
    "#     for i in range(exp_iter):\n",
    "#         sdp_importance, sdp_index, size, sdp = explainer.importance_sdp_rf(instance, y, X_train, y_train)\n",
    "#         feat_pos.extend(sdp_index[0, :size[0]])\n",
    "        \n",
    "#         sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train)\n",
    "#         #print(np.array(sufficient_expl).shape)\n",
    "#         lximp = explainer.compute_local_sdp(len(feat_list), sufficient_expl[0])\n",
    "#         feat_imp.append(lximp)\n",
    "\n",
    "\n",
    "#     feats = Counter(feat_pos)\n",
    "#     imp = feats.items()\n",
    "\n",
    "#     occ = np.zeros(len(feat_list))\n",
    "\n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     avg_imp = np.mean(feat_imp, axis=0)\n",
    "#     abs_imp = [abs(imp) for imp in avg_imp]\n",
    "\n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "#     #print(\"Feature importance score:\", avg_imp)\n",
    "#     return occ, occ\n",
    "#     #return avg_imp, abs_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_anchor_features(explainer, instance, cls, train_data, classification, exp_iter):\n",
    "#     anchor_exp = []\n",
    "#     for i in range(exp_iter):\n",
    "#         if classification == True:\n",
    "#             anchor_exp.extend(explainer.explain_instance(instance, cls.predict).names())\n",
    "#     print(anchor_exp)\n",
    "\n",
    "#     locs = []\n",
    "#     for exp in anchor_exp:\n",
    "#         feat = exp.replace(\"= \",'')\n",
    "#         #print(feat)\n",
    "#         if '<' in feat:\n",
    "#             parts = feat.split('<')\n",
    "#             #print(\"less than\", parts)\n",
    "#         elif '>' in feat:\n",
    "#             parts = feat.split('>')\n",
    "#             #print(\"more than\", parts)\n",
    "\n",
    "#         for part in parts:\n",
    "#             if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "#                 feat_name = part.replace(' ','')\n",
    "#         locs.append(feat_list.index(feat_name))\n",
    "    \n",
    "#     feats = Counter(locs)\n",
    "#     imp = feats.items()\n",
    "    \n",
    "#     #print(imp)\n",
    "    \n",
    "#     occ = np.zeros(len(feat_list))\n",
    "    \n",
    "#     for each in imp:\n",
    "#         occ[each[0]] = each[1]\n",
    "        \n",
    "#     print(\"Frequency of occurrence:\", occ)\n",
    "    \n",
    "#     return occ, occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "gXQO43zuLlzK"
   },
   "outputs": [],
   "source": [
    "# def get_explanation_features(explainer, instance, cls, scaler, dataset, classification, exp_iter, xai_method, feat_list, X_train, y_train):\n",
    "#     if xai_method == \"SHAP\":\n",
    "#         feat_pos = get_shap_vals(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LIME\":\n",
    "#         feat_pos = get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"LINDA\":\n",
    "#         feat_pos = get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list)\n",
    "\n",
    "#     elif xai_method == \"ACV\":\n",
    "#         feat_pos = get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter, feat_list)\n",
    "        \n",
    "#     elif xai_method == \"Anchor\":\n",
    "#         feat_pos = get_anchor_features(explainer, instance, cls, X_train, classification, exp_iter)\n",
    "    \n",
    "#     return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        feat_pos[i]+=n\n",
    "#        feat_pos[i]+=1\n",
    "        n=n-1\n",
    "    #feat_pos = set(feats)\n",
    "    \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_features(cls):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    coef = [abs(val) for val in og_coef]\n",
    "    \n",
    "#     bins = pd.cut(coef, 4, retbins = True, duplicates = \"drop\")\n",
    "#     q1_min = bins[1][-2]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(coef)) if coef[i] > q1_min]\n",
    "    \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_features(cls, instance):\n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "    \n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "\n",
    "    likelihoods = []\n",
    "    \n",
    "    for i in range(len(means)):\n",
    "        lk = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = scipy.stats.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = lk-alt_lk\n",
    "#        lkhood = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        likelihoods.append(lkhood)\n",
    "    \n",
    "#     bins = pd.cut(likelihoods, 4, retbins = True, duplicates = \"drop\")[1]\n",
    "#     lim = bins[-2]\n",
    "    \n",
    "# #     bins = pd.cut(likelihoods, 10, retbins = True, duplicates = \"drop\")[1]\n",
    "# #     lim_1 = bins[-2]\n",
    "# #     lim_2 = bins[1]\n",
    "    \n",
    "# #     sortedls = sorted(likelihoods, reverse=True)\n",
    "# #     pos = math.ceil(len(likelihoods)/4)\n",
    "# #     lim = likelihoods[pos]\n",
    "    \n",
    "#     feat_pos = [i for i in range(len(likelihoods)) if likelihoods[i] >= lim]# or likelihoods[i] <= lim_2]\n",
    "    \n",
    "    return np.abs(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_rankings(cls, instance, cls_method, X_train, feat_list):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        feat_pos = get_reg_features(cls)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        feat_pos = get_nb_features(cls, instance)\n",
    "        \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TuQANu5PLlzK"
   },
   "outputs": [],
   "source": [
    "def permute_instance(instance, i, perm_iter = 100, min_i = [0], max_i=[1], mean_i=[0], mode=\"permutation\"):\n",
    "            \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    \n",
    "    for j in range(len(i)):\n",
    "        if mode==\"baseline_max\":\n",
    "            n_val = [max_i[j]]*perm_iter\n",
    "        elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "            n_val = [mean_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_min\":\n",
    "            n_val = [min_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_0\":\n",
    "            n_val = [0]*perm_iter\n",
    "        else:\n",
    "            n_val = np.random.uniform(min_i[j], max_i[j], perm_iter)\n",
    "\n",
    "\n",
    "        permutations[i[j]] = n_val\n",
    "    \n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_values(instance, i, perm_iter = 100, min_i = [0], max_i=[1], mean_i=[0], unique_values=[[0,1]], mode=\"permutation\"):\n",
    "\n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "\n",
    "    for j in range(len(i)):\n",
    "        if mode==\"baseline_max\":\n",
    "            n_val = [max_i[j]]*perm_iter\n",
    "        elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "            n_val = [mean_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_min\":\n",
    "            n_val = [min_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_0\":\n",
    "            n_val = [0]*perm_iter\n",
    "        else:\n",
    "            n_val = np.random.choice(unique_values[j], perm_iter)\n",
    "\n",
    "        permutations[i[j]] = n_val\n",
    "        \n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_multiple(instance, i, columns, col_dict, perm_iter=100, min_i = [0], max_i=[1], mean_i=[0],unique_values=[[0,1]], mode=\"permutation\"):\n",
    "    \n",
    "    cats = []\n",
    "    nums = []\n",
    "    \n",
    "    if col_dict[\"discrete\"]!=None:\n",
    "        cats = [int(col) for col in i if columns[col] in col_dict[\"discrete\"]]\n",
    "    if col_dict[\"continuous\"]!=None:\n",
    "        nums = [int(col) for col in i if columns[col] in col_dict[\"continuous\"]]\n",
    "    \n",
    "    cat_permutations = False\n",
    "    num_permutations = False\n",
    "    \n",
    "    if len(cats)>0:\n",
    "        mins = min_i[columns[cats]].values\n",
    "        maxes = max_i[columns[cats]].values\n",
    "        means = mean_i[columns[cats]].values\n",
    "        uniques = unique_values[columns[cats]].values\n",
    "        cat_permutations = cycle_values(instance, cats, perm_iter, mins, maxes, \n",
    "                                  means, uniques, mode) \n",
    "    if len(nums)>0:\n",
    "        mins = min_i[columns[nums]].values\n",
    "        maxes = max_i[columns[nums]].values\n",
    "        means = mean_i[columns[nums]].values\n",
    "        num_permutations = permute_instance(instance, nums, perm_iter, mins, maxes, \n",
    "                                  means, mode)\n",
    "        \n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "    if type(cat_permutations)!=bool:\n",
    "        cat_permutations = cat_permutations.transpose()\n",
    "        for j in cats:\n",
    "            permutations[j] = cat_permutations[j]\n",
    "            \n",
    "    if type(num_permutations)!=bool:\n",
    "        num_permutations = num_permutations.transpose()\n",
    "        for j in nums:\n",
    "            permutations[j] = num_permutations[j]\n",
    "#     if  len(cats)>0:\n",
    "#         print(\"categorical:\", permutations[cats])\n",
    "#     if len(nums)>0:\n",
    "#         print(\"numeric\", permutations[nums])\n",
    "#     print(\"all\", permutations[i])        \n",
    "    permutations = permutations.transpose()\n",
    "    \n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = scipy.stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_ratio(categories, measurements):\n",
    "    fcat, _ = pd.factorize(categories)\n",
    "    cat_num = np.max(fcat)+1\n",
    "    y_avg_array = np.zeros(cat_num)\n",
    "    n_array = np.zeros(cat_num)\n",
    "    for i in range(0,cat_num):\n",
    "        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n",
    "        n_array[i] = len(cat_measures)\n",
    "        y_avg_array[i] = np.average(cat_measures)\n",
    "    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n",
    "    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n",
    "    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n",
    "    if numerator == 0:\n",
    "        eta = 0.0\n",
    "    else:\n",
    "        eta = np.sqrt(numerator/denominator)\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepsis_cases_1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to project folder\n",
    "# please change to your own\n",
    "PATH = os.getcwd()\n",
    "\n",
    "dataset_ref = \"sepsis_cases\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"decision_tree\"\n",
    "\n",
    "classification=True\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "perm_iter = 1000\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"],\n",
    "    \"bpic2011\": [\"bpic2011_f1\"],\n",
    "    \"hospital\": [\"hospital_billing_2\"],\n",
    "    \"traffic\": [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/train_prefixes.csv\" % (dataset_ref)))\n",
    "dt_test_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/test_prefixes.csv\" % (dataset_ref)))\n",
    "dt_val_prefixes = pd.read_csv(os.path.join(PATH, \"%s/datasets/val_prefixes.csv\" % (dataset_ref)))\n",
    "\n",
    "dt_train_prefixes = pd.concat([dt_train_prefixes, dt_val_prefixes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(datasets[0])\n",
    "data = dataset_manager.read_dataset()\n",
    "\n",
    "non_num_cols = dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols\n",
    "num_cols = dataset_manager.dynamic_num_cols+dataset_manager.static_num_cols\n",
    "\n",
    "num_data = dt_train_prefixes[num_cols]\n",
    "cat_data = dt_train_prefixes[non_num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023050,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "3VkV42oeLlzL"
   },
   "outputs": [],
   "source": [
    "# # path to project folder\n",
    "# # please change to your own\n",
    "# PATH = os.getcwd()\n",
    "\n",
    "# dataset = \"nursery\"\n",
    "# cls_method = \"nb\"\n",
    "\n",
    "# classification = True\n",
    "# # xai_method = \"SHAP\"\n",
    "\n",
    "# modes = [\"permutation\", \"baseline_min\", \"baseline_mean\", \"baseline_max\", \"baseline_0\"]\n",
    "\n",
    "# random_state = 22\n",
    "# exp_iter = 10\n",
    "# perm_iter = 1000\n",
    "\n",
    "# save_to = \"%s/%s/\" % (PATH, dataset)\n",
    "# dataset_folder = \"%s/datasets/\" % (save_to)\n",
    "# final_folder = \"%s/%s/\" % (save_to, cls_method)\n",
    "\n",
    "# #Get datasets\n",
    "# X_train = pd.read_csv(dataset_folder+dataset+\"_Xtrain.csv\", index_col=False, sep = \";\")\n",
    "# y_train = pd.read_csv(dataset_folder+dataset+\"_Ytrain.csv\", index_col=False, sep = \";\")\n",
    "# test_x = pd.read_csv(final_folder+\"test_sample.csv\", index_col=False, sep = \";\").values\n",
    "# results = pd.read_csv(os.path.join(final_folder,\"results.csv\"), index_col=False, sep = \";\")\n",
    "# actual = results[\"Actual\"].values\n",
    "\n",
    "# with open(dataset_folder+\"col_dict.json\", \"r\") as f:\n",
    "#     col_dict = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# feat_list = [each.replace(' ','_') for each in X_train.columns]\n",
    "\n",
    "# cls = joblib.load(save_to+cls_method+\"/cls.joblib\")\n",
    "# scaler = joblib.load(save_to+\"/scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "X7OQDhF6LlzL"
   },
   "outputs": [],
   "source": [
    "# if xai_method == \"SHAP\":\n",
    "#   #  explainer = shap.explainers._permutation.Permutation(cls.predict_proba, X_train)\n",
    "#     if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "#         explainer = shap.Explainer(cls)\n",
    "#     elif cls_method == \"nb\":\n",
    "#         if classification:\n",
    "#             masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#             explainer = shap.Explainer(cls.predict_proba, masker)\n",
    "#         else:\n",
    "#             print(\"NB is classification only\")\n",
    "#     elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "#         masker = shap.maskers._tabular.Independent(X_train.values, len(X_train))\n",
    "#         explainer = shap.Explainer(cls, masker)\n",
    "#     else:\n",
    "#         explainer = shap.Explainer(cls, X_train)\n",
    "#     print(type(explainer))\n",
    "    \n",
    "# elif xai_method == \"LIME\":\n",
    "#     if col_dict['discrete'] != None:\n",
    "#         cat_cols = [each.replace(' ','_') for each in col_dict['discrete']]\n",
    "#         col_inds = [feat_list.index(each) for each in cat_cols]\n",
    "#     else:\n",
    "#         col_inds = []\n",
    "    \n",
    "#     if classification==True:\n",
    "#         class_names=['Negative','Positive']# negative is 0, positive is 1, 0 is left, 1 is right\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                             class_names=class_names, categorical_features = col_inds,\n",
    "#                                                             discretize_continuous=False)\n",
    "#     else:\n",
    "#         class_names = ['Final Value']\n",
    "#         explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feat_list, \n",
    "#                                                            class_names=class_names, discretize_continuous=True, \n",
    "#                                                            categorical_features = col_inds, mode = \"regression\")\n",
    "        \n",
    "# elif xai_method == \"LINDA\":\n",
    "#     test_dict = learning.generate_local_predictions( test_x, results[\"Actual\"].values, cls, scaler, None )\n",
    "# #    feat_list = feat_list+[\"Result\"]\n",
    "\n",
    "#     explainer = None\n",
    "\n",
    "# elif xai_method == \"ACV\":\n",
    "#     explainer = joblib.load(save_to+cls_method+\"/acv_explainer.joblib\")\n",
    "    \n",
    "# elif xai_method == \"Anchor\":\n",
    "#     if classification:\n",
    "#         class_names=['Negative','Positive']\n",
    "#         explainer = anchor_tabular.AnchorTabularExplainer(class_names, feat_list, X_train.values)\n",
    "#     else:\n",
    "#         raise Exception(\"Anchor only works for classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cls_method == \"nb\":\n",
    "    \n",
    "#     probas = cls.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_isotonic = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"isotonic\")\n",
    "#     clf_isotonic.fit(X_train, y_train)\n",
    "#     isotonic_probas = clf_isotonic.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     clf_sigmoid = CalibratedClassifierCV(cls, cv=\"prefit\", method=\"sigmoid\")\n",
    "#     clf_sigmoid.fit(X_train, y_train)\n",
    "#     sigmoid_probas = clf_sigmoid.predict_proba(test_x)[:, 1]\n",
    "    \n",
    "#     cls_score = brier_score_loss(results[\"Actual\"], probas)\n",
    "#     iso_score = brier_score_loss(results[\"Actual\"], isotonic_probas)\n",
    "#     sig_score = brier_score_loss(results[\"Actual\"], sigmoid_probas)\n",
    "    \n",
    "#     if iso_score < sig_score and iso_score < cls_score:\n",
    "#         print(\"Winner is iso\")\n",
    "#         cls = clf_isotonic.calibrated_classifiers_[0].base_estimator\n",
    "#     elif sig_score < iso_score and sig_score < cls_score:\n",
    "#         print(\"Winner is sigmoid\")\n",
    "#         cls = clf_sigmoid.calibrated_classifiers_[0].base_estimator\n",
    "#     else:\n",
    "#         cls = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "aiQNPNswLlzM"
   },
   "outputs": [],
   "source": [
    "# min_X = np.min(X_train)\n",
    "# max_X = np.max(X_train)\n",
    "# mean_X = np.mean(X_train, axis=0)\n",
    "# unique_values = pd.Series({col: X_train[col].unique() for col in X_train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if X_train.columns[-3] in col_dict[\"continuous\"]:\n",
    "#     print(True)\n",
    "# else:\n",
    "#     print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(i):\n",
    "    instance = sample_instances[i]\n",
    "\n",
    "    tr = get_true_rankings(cls, instance, cls_method, trainingdata, feat_list)\n",
    "\n",
    "    if classification:\n",
    "        pred = cls.predict(instance.reshape(1, -1))\n",
    "        proba = cls.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]\n",
    "        p1_list = list(proba)*perm_iter\n",
    "\n",
    "    perm_mape = np.zeros(len(instance))\n",
    "    perm_rmse = np.zeros(len(instance))\n",
    "    perm_r2 = np.zeros(len(instance))\n",
    "\n",
    "    for j in range(len(instance)):\n",
    "        if cats != None:\n",
    "            if trainingdata.columns[j] in cat_cols:\n",
    "                permutations = cycle_values(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], [unique_values[trainingdata.columns[j]]], mode)\n",
    "            else:\n",
    "                permutations = permute_instance(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], mode)\n",
    "        else:\n",
    "            permutations = permute_instance(instance, [j], perm_iter, [min_X[j]], [max_X[j]], [mean_X[j]], mode)\n",
    "\n",
    "        if classification:\n",
    "            p2_list = cls.predict_proba(permutations).transpose()[pred].reshape(perm_iter)\n",
    "            perm_mape[j] = mean_absolute_percentage_error(p1_list, p2_list)\n",
    "            perm_rmse[j] = mean_squared_error(p1_list, p2_list, squared=False)\n",
    "            perm_r2[j] = r2_score(p1_list, p2_list)\n",
    "\n",
    "    #print(\"Final MAPE for all features:\", perm_mape)\n",
    "    mape_corr = scipy.stats.kendalltau(tr, perm_mape, variant=\"b\")[0]\n",
    "    rmse_corr = scipy.stats.kendalltau(tr, perm_rmse, variant=\"b\")[0]\n",
    "    r2_corr = scipy.stats.kendalltau(tr, perm_r2, variant=\"b\")[0]\n",
    "\n",
    "    return mape_corr, rmse_corr, r2_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652073023051,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "nq_FxFrVLlzM",
    "outputId": "5aa1fc25-ef58-4357-fc0a-758bcdb2fc62",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49404ad63ca44af08c953453e13cc973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                        | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/112 [00:02<03:47,  2.05s/it]\u001b[A\n",
      "  6%|██                              | 7/112 [00:03<00:48,  2.19it/s]\u001b[A\n",
      " 12%|███▌                           | 13/112 [00:04<00:31,  3.17it/s]\u001b[A\n",
      " 14%|████▍                          | 16/112 [00:05<00:22,  4.23it/s]\u001b[A\n",
      " 16%|████▉                          | 18/112 [00:05<00:19,  4.71it/s]\u001b[A\n",
      " 18%|█████▌                         | 20/112 [00:06<00:29,  3.14it/s]\u001b[A\n",
      " 21%|██████▎                        | 23/112 [00:06<00:20,  4.27it/s]\u001b[A\n",
      " 22%|██████▉                        | 25/112 [00:08<00:28,  3.08it/s]\u001b[A\n",
      " 26%|████████                       | 29/112 [00:08<00:18,  4.52it/s]\u001b[A\n",
      " 27%|████████▎                      | 30/112 [00:08<00:17,  4.67it/s]\u001b[A\n",
      " 28%|████████▌                      | 31/112 [00:09<00:26,  3.02it/s]\u001b[A\n",
      " 29%|████████▊                      | 32/112 [00:09<00:23,  3.40it/s]\u001b[A\n",
      " 29%|█████████▏                     | 33/112 [00:09<00:21,  3.74it/s]\u001b[A\n",
      " 31%|█████████▋                     | 35/112 [00:09<00:16,  4.78it/s]\u001b[A\n",
      " 33%|██████████▏                    | 37/112 [00:10<00:22,  3.27it/s]\u001b[A\n",
      " 34%|██████████▌                    | 38/112 [00:11<00:21,  3.36it/s]\u001b[A\n",
      " 35%|██████████▊                    | 39/112 [00:11<00:22,  3.31it/s]\u001b[A\n",
      " 37%|███████████▎                   | 41/112 [00:11<00:15,  4.62it/s]\u001b[A\n",
      " 38%|███████████▉                   | 43/112 [00:12<00:21,  3.15it/s]\u001b[A\n",
      " 39%|████████████▏                  | 44/112 [00:13<00:22,  3.01it/s]\u001b[A\n",
      " 42%|█████████████                  | 47/112 [00:13<00:14,  4.49it/s]\u001b[A\n",
      " 44%|█████████████▌                 | 49/112 [00:14<00:20,  3.11it/s]\u001b[A\n",
      " 45%|█████████████▊                 | 50/112 [00:14<00:19,  3.24it/s]\u001b[A\n",
      " 47%|██████████████▋                | 53/112 [00:14<00:11,  4.98it/s]\u001b[A\n",
      " 48%|██████████████▉                | 54/112 [00:15<00:12,  4.74it/s]\u001b[A\n",
      " 49%|███████████████▏               | 55/112 [00:16<00:21,  2.69it/s]\u001b[A\n",
      " 50%|███████████████▌               | 56/112 [00:16<00:19,  2.86it/s]\u001b[A\n",
      " 54%|████████████████▌              | 60/112 [00:16<00:10,  4.87it/s]\u001b[A\n",
      " 54%|████████████████▉              | 61/112 [00:17<00:17,  2.97it/s]\u001b[A\n",
      " 55%|█████████████████▏             | 62/112 [00:17<00:15,  3.29it/s]\u001b[A\n",
      " 56%|█████████████████▍             | 63/112 [00:18<00:12,  3.80it/s]\u001b[A\n",
      " 58%|█████████████████▉             | 65/112 [00:18<00:09,  4.74it/s]\u001b[A\n",
      " 59%|██████████████████▎            | 66/112 [00:18<00:10,  4.51it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 67/112 [00:19<00:16,  2.79it/s]\u001b[A\n",
      " 61%|██████████████████▊            | 68/112 [00:19<00:13,  3.38it/s]\u001b[A\n",
      " 62%|███████████████████            | 69/112 [00:19<00:10,  4.01it/s]\u001b[A\n",
      " 62%|███████████████████▍           | 70/112 [00:19<00:09,  4.65it/s]\u001b[A\n",
      " 63%|███████████████████▋           | 71/112 [00:20<00:09,  4.12it/s]\u001b[A\n",
      " 64%|███████████████████▉           | 72/112 [00:20<00:09,  4.27it/s]\u001b[A\n",
      " 65%|████████████████████▏          | 73/112 [00:21<00:15,  2.45it/s]\u001b[A\n",
      " 66%|████████████████████▍          | 74/112 [00:21<00:13,  2.72it/s]\u001b[A\n",
      " 69%|█████████████████████▎         | 77/112 [00:21<00:07,  4.52it/s]\u001b[A\n",
      " 70%|█████████████████████▌         | 78/112 [00:21<00:08,  4.24it/s]\u001b[A\n",
      " 71%|█████████████████████▊         | 79/112 [00:22<00:11,  2.94it/s]\u001b[A\n",
      " 71%|██████████████████████▏        | 80/112 [00:22<00:10,  2.98it/s]\u001b[A\n",
      " 74%|██████████████████████▉        | 83/112 [00:23<00:05,  5.18it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 84/112 [00:23<00:07,  3.92it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 85/112 [00:25<00:13,  1.96it/s]\u001b[A\n",
      " 78%|████████████████████████       | 87/112 [00:25<00:09,  2.77it/s]\u001b[A\n",
      " 79%|████████████████████████▎      | 88/112 [00:25<00:08,  2.88it/s]\u001b[A\n",
      " 79%|████████████████████████▋      | 89/112 [00:25<00:06,  3.33it/s]\u001b[A\n",
      " 80%|████████████████████████▉      | 90/112 [00:26<00:07,  2.99it/s]\u001b[A\n",
      " 81%|█████████████████████████▏     | 91/112 [00:27<00:10,  2.01it/s]\u001b[A\n",
      " 82%|█████████████████████████▍     | 92/112 [00:27<00:08,  2.49it/s]\u001b[A\n",
      " 83%|█████████████████████████▋     | 93/112 [00:27<00:07,  2.62it/s]\u001b[A\n",
      " 84%|██████████████████████████     | 94/112 [00:28<00:07,  2.47it/s]\u001b[A\n",
      " 86%|██████████████████████████▌    | 96/112 [00:28<00:05,  2.95it/s]\u001b[A\n",
      " 87%|██████████████████████████▊    | 97/112 [00:29<00:06,  2.47it/s]\u001b[A\n",
      " 88%|███████████████████████████▏   | 98/112 [00:29<00:05,  2.78it/s]\u001b[A\n",
      " 88%|███████████████████████████▍   | 99/112 [00:30<00:05,  2.35it/s]\u001b[A\n",
      " 89%|██████████████████████████▊   | 100/112 [00:30<00:05,  2.12it/s]\u001b[A\n",
      " 91%|███████████████████████████▎  | 102/112 [00:30<00:03,  3.04it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 103/112 [00:31<00:03,  2.75it/s]\u001b[A\n",
      " 93%|███████████████████████████▊  | 104/112 [00:31<00:02,  2.95it/s]\u001b[A\n",
      " 94%|████████████████████████████▏ | 105/112 [00:32<00:03,  2.18it/s]\u001b[A\n",
      " 95%|████████████████████████████▍ | 106/112 [00:33<00:03,  1.96it/s]\u001b[A\n",
      " 96%|████████████████████████████▋ | 107/112 [00:33<00:02,  2.38it/s]\u001b[A\n",
      " 97%|█████████████████████████████▏| 109/112 [00:33<00:00,  3.43it/s]\u001b[A\n",
      " 98%|█████████████████████████████▍| 110/112 [00:33<00:00,  3.13it/s]\u001b[A\n",
      " 99%|█████████████████████████████▋| 111/112 [00:34<00:00,  3.10it/s]\u001b[A\n",
      "100%|██████████████████████████████| 112/112 [00:35<00:00,  3.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation Mean correlation: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                        | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/112 [00:01<03:21,  1.82s/it]\u001b[A\n",
      "  3%|▊                               | 3/112 [00:01<00:56,  1.94it/s]\u001b[A\n",
      "  5%|█▋                              | 6/112 [00:02<00:23,  4.51it/s]\u001b[A\n",
      "  7%|██▎                             | 8/112 [00:03<00:41,  2.48it/s]\u001b[A\n",
      " 11%|███▎                           | 12/112 [00:03<00:22,  4.41it/s]\u001b[A\n",
      " 12%|███▉                           | 14/112 [00:04<00:32,  3.00it/s]\u001b[A\n",
      " 13%|████▏                          | 15/112 [00:05<00:29,  3.29it/s]\u001b[A\n",
      " 14%|████▍                          | 16/112 [00:05<00:26,  3.59it/s]\u001b[A\n",
      " 17%|█████▎                         | 19/112 [00:06<00:32,  2.89it/s]\u001b[A\n",
      " 19%|█████▊                         | 21/112 [00:06<00:25,  3.61it/s]\u001b[A\n",
      " 20%|██████                         | 22/112 [00:06<00:23,  3.86it/s]\u001b[A\n",
      " 21%|██████▎                        | 23/112 [00:07<00:21,  4.09it/s]\u001b[A\n",
      " 22%|██████▉                        | 25/112 [00:08<00:31,  2.77it/s]\u001b[A\n",
      " 23%|███████▏                       | 26/112 [00:08<00:27,  3.15it/s]\u001b[A\n",
      " 25%|███████▊                       | 28/112 [00:09<00:29,  2.89it/s]\u001b[A\n",
      " 28%|████████▌                      | 31/112 [00:10<00:32,  2.48it/s]\u001b[A\n",
      " 29%|████████▊                      | 32/112 [00:11<00:35,  2.26it/s]\u001b[A\n",
      " 30%|█████████▍                     | 34/112 [00:11<00:32,  2.43it/s]\u001b[A\n",
      " 32%|█████████▉                     | 36/112 [00:12<00:23,  3.23it/s]\u001b[A\n",
      " 33%|██████████▏                    | 37/112 [00:13<00:31,  2.38it/s]\u001b[A\n",
      " 34%|██████████▌                    | 38/112 [00:13<00:32,  2.30it/s]\u001b[A\n",
      " 35%|██████████▊                    | 39/112 [00:13<00:27,  2.61it/s]\u001b[A\n",
      " 36%|███████████                    | 40/112 [00:14<00:29,  2.47it/s]\u001b[A\n",
      " 38%|███████████▋                   | 42/112 [00:14<00:18,  3.83it/s]\u001b[A\n",
      " 38%|███████████▉                   | 43/112 [00:15<00:26,  2.65it/s]\u001b[A\n",
      " 39%|████████████▏                  | 44/112 [00:15<00:21,  3.10it/s]\u001b[A\n",
      " 40%|████████████▍                  | 45/112 [00:15<00:23,  2.88it/s]\u001b[A\n",
      " 41%|████████████▋                  | 46/112 [00:16<00:22,  3.00it/s]\u001b[A\n",
      " 44%|█████████████▌                 | 49/112 [00:17<00:21,  2.99it/s]\u001b[A\n",
      " 45%|█████████████▊                 | 50/112 [00:17<00:18,  3.36it/s]\u001b[A\n",
      " 46%|██████████████▍                | 52/112 [00:17<00:16,  3.58it/s]\u001b[A\n",
      " 47%|██████████████▋                | 53/112 [00:17<00:14,  4.08it/s]\u001b[A\n",
      " 49%|███████████████▏               | 55/112 [00:19<00:22,  2.54it/s]\u001b[A\n",
      " 52%|████████████████               | 58/112 [00:19<00:13,  4.06it/s]\u001b[A\n",
      " 53%|████████████████▎              | 59/112 [00:19<00:12,  4.23it/s]\u001b[A\n",
      " 54%|████████████████▉              | 61/112 [00:20<00:16,  3.04it/s]\u001b[A\n",
      " 55%|█████████████████▏             | 62/112 [00:20<00:15,  3.33it/s]\u001b[A\n",
      " 56%|█████████████████▍             | 63/112 [00:20<00:12,  3.87it/s]\u001b[A\n",
      " 57%|█████████████████▋             | 64/112 [00:21<00:13,  3.48it/s]\u001b[A\n",
      " 59%|██████████████████▎            | 66/112 [00:21<00:12,  3.77it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 67/112 [00:22<00:16,  2.78it/s]\u001b[A\n",
      " 61%|██████████████████▊            | 68/112 [00:22<00:13,  3.17it/s]\u001b[A\n",
      " 62%|███████████████████            | 69/112 [00:22<00:14,  2.88it/s]\u001b[A\n",
      " 64%|███████████████████▉           | 72/112 [00:23<00:11,  3.56it/s]\u001b[A\n",
      " 65%|████████████████████▏          | 73/112 [00:23<00:11,  3.44it/s]\u001b[A\n",
      " 66%|████████████████████▍          | 74/112 [00:24<00:10,  3.70it/s]\u001b[A\n",
      " 67%|████████████████████▊          | 75/112 [00:24<00:13,  2.79it/s]\u001b[A\n",
      " 70%|█████████████████████▌         | 78/112 [00:25<00:09,  3.40it/s]\u001b[A\n",
      " 71%|█████████████████████▊         | 79/112 [00:25<00:10,  3.25it/s]\u001b[A\n",
      " 71%|██████████████████████▏        | 80/112 [00:25<00:08,  3.68it/s]\u001b[A\n",
      " 72%|██████████████████████▍        | 81/112 [00:26<00:09,  3.32it/s]\u001b[A\n",
      " 73%|██████████████████████▋        | 82/112 [00:26<00:09,  3.20it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 84/112 [00:27<00:07,  3.86it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 85/112 [00:27<00:08,  3.28it/s]\u001b[A\n",
      " 77%|███████████████████████▊       | 86/112 [00:27<00:06,  3.85it/s]\u001b[A\n",
      " 78%|████████████████████████       | 87/112 [00:27<00:06,  3.74it/s]\u001b[A\n",
      " 79%|████████████████████████▋      | 89/112 [00:28<00:05,  4.41it/s]\u001b[A\n",
      " 80%|████████████████████████▉      | 90/112 [00:28<00:05,  4.03it/s]\u001b[A\n",
      " 81%|█████████████████████████▏     | 91/112 [00:29<00:06,  3.23it/s]\u001b[A\n",
      " 82%|█████████████████████████▍     | 92/112 [00:29<00:05,  3.66it/s]\u001b[A\n",
      " 83%|█████████████████████████▋     | 93/112 [00:29<00:05,  3.27it/s]\u001b[A\n",
      " 85%|██████████████████████████▎    | 95/112 [00:30<00:04,  3.71it/s]\u001b[A\n",
      " 86%|██████████████████████████▌    | 96/112 [00:30<00:04,  3.61it/s]\u001b[A\n",
      " 87%|██████████████████████████▊    | 97/112 [00:31<00:06,  2.48it/s]\u001b[A\n",
      " 88%|███████████████████████████▍   | 99/112 [00:31<00:03,  3.29it/s]\u001b[A\n",
      " 90%|███████████████████████████   | 101/112 [00:31<00:02,  3.72it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 103/112 [00:32<00:02,  3.22it/s]\u001b[A\n",
      " 93%|███████████████████████████▊  | 104/112 [00:32<00:02,  3.59it/s]\u001b[A\n",
      " 95%|████████████████████████████▍ | 106/112 [00:33<00:01,  4.69it/s]\u001b[A\n",
      " 96%|████████████████████████████▋ | 107/112 [00:33<00:01,  4.13it/s]\u001b[A\n",
      " 96%|████████████████████████████▉ | 108/112 [00:33<00:00,  4.57it/s]\u001b[A\n",
      " 97%|█████████████████████████████▏| 109/112 [00:34<00:00,  3.14it/s]\u001b[A\n",
      " 99%|█████████████████████████████▋| 111/112 [00:34<00:00,  4.51it/s]\u001b[A\n",
      "100%|██████████████████████████████| 112/112 [00:34<00:00,  3.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_min Mean correlation: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                        | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/112 [00:01<02:49,  1.52s/it]\u001b[A\n",
      "  6%|██                              | 7/112 [00:02<00:36,  2.89it/s]\u001b[A\n",
      "  7%|██▎                             | 8/112 [00:03<00:33,  3.06it/s]\u001b[A\n",
      " 11%|███▎                           | 12/112 [00:03<00:17,  5.66it/s]\u001b[A\n",
      " 12%|███▉                           | 14/112 [00:04<00:33,  2.90it/s]\u001b[A\n",
      " 17%|█████▎                         | 19/112 [00:05<00:26,  3.55it/s]\u001b[A\n",
      " 18%|█████▌                         | 20/112 [00:06<00:27,  3.32it/s]\u001b[A\n",
      " 21%|██████▎                        | 23/112 [00:06<00:19,  4.65it/s]\u001b[A\n",
      " 22%|██████▉                        | 25/112 [00:07<00:28,  3.07it/s]\u001b[A\n",
      " 23%|███████▏                       | 26/112 [00:08<00:28,  3.01it/s]\u001b[A\n",
      " 24%|███████▍                       | 27/112 [00:08<00:29,  2.91it/s]\u001b[A\n",
      " 26%|████████                       | 29/112 [00:08<00:21,  3.83it/s]\u001b[A\n",
      " 28%|████████▌                      | 31/112 [00:10<00:31,  2.55it/s]\u001b[A\n",
      " 29%|█████████▏                     | 33/112 [00:10<00:25,  3.13it/s]\u001b[A\n",
      " 31%|█████████▋                     | 35/112 [00:10<00:20,  3.85it/s]\u001b[A\n",
      " 33%|██████████▏                    | 37/112 [00:11<00:28,  2.64it/s]\u001b[A\n",
      " 35%|██████████▊                    | 39/112 [00:12<00:23,  3.12it/s]\u001b[A\n",
      " 38%|███████████▉                   | 43/112 [00:13<00:23,  2.91it/s]\u001b[A\n",
      " 40%|████████████▍                  | 45/112 [00:14<00:19,  3.47it/s]\u001b[A\n",
      " 41%|████████████▋                  | 46/112 [00:14<00:19,  3.33it/s]\u001b[A\n",
      " 44%|█████████████▌                 | 49/112 [00:15<00:20,  3.01it/s]\u001b[A\n",
      " 46%|██████████████▍                | 52/112 [00:15<00:15,  4.00it/s]\u001b[A\n",
      " 47%|██████████████▋                | 53/112 [00:16<00:15,  3.83it/s]\u001b[A\n",
      " 49%|███████████████▏               | 55/112 [00:17<00:22,  2.54it/s]\u001b[A\n",
      " 53%|████████████████▎              | 59/112 [00:18<00:14,  3.63it/s]\u001b[A\n",
      " 54%|████████████████▉              | 61/112 [00:18<00:16,  3.18it/s]\u001b[A\n",
      " 56%|█████████████████▍             | 63/112 [00:19<00:14,  3.47it/s]\u001b[A\n",
      " 57%|█████████████████▋             | 64/112 [00:19<00:13,  3.65it/s]\u001b[A\n",
      " 58%|█████████████████▉             | 65/112 [00:20<00:14,  3.28it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 67/112 [00:20<00:13,  3.32it/s]\u001b[A\n",
      " 62%|███████████████████            | 69/112 [00:20<00:10,  3.96it/s]\u001b[A\n",
      " 62%|███████████████████▍           | 70/112 [00:21<00:09,  4.33it/s]\u001b[A\n",
      " 63%|███████████████████▋           | 71/112 [00:21<00:11,  3.51it/s]\u001b[A\n",
      " 65%|████████████████████▏          | 73/112 [00:22<00:11,  3.39it/s]\u001b[A\n",
      " 67%|████████████████████▊          | 75/112 [00:22<00:08,  4.39it/s]\u001b[A\n",
      " 68%|█████████████████████          | 76/112 [00:22<00:07,  4.69it/s]\u001b[A\n",
      " 69%|█████████████████████▎         | 77/112 [00:23<00:10,  3.31it/s]\u001b[A\n",
      " 70%|█████████████████████▌         | 78/112 [00:23<00:10,  3.40it/s]\u001b[A\n",
      " 71%|█████████████████████▊         | 79/112 [00:23<00:11,  2.88it/s]\u001b[A\n",
      " 71%|██████████████████████▏        | 80/112 [00:24<00:09,  3.21it/s]\u001b[A\n",
      " 72%|██████████████████████▍        | 81/112 [00:24<00:08,  3.86it/s]\u001b[A\n",
      " 73%|██████████████████████▋        | 82/112 [00:24<00:07,  4.24it/s]\u001b[A\n",
      " 74%|██████████████████████▉        | 83/112 [00:24<00:09,  3.09it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 84/112 [00:25<00:08,  3.27it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 85/112 [00:25<00:09,  2.80it/s]\u001b[A\n",
      " 77%|███████████████████████▊       | 86/112 [00:26<00:10,  2.37it/s]\u001b[A\n",
      " 79%|████████████████████████▎      | 88/112 [00:26<00:06,  3.92it/s]\u001b[A\n",
      " 79%|████████████████████████▋      | 89/112 [00:26<00:06,  3.50it/s]\u001b[A\n",
      " 80%|████████████████████████▉      | 90/112 [00:27<00:05,  3.79it/s]\u001b[A\n",
      " 81%|█████████████████████████▏     | 91/112 [00:27<00:06,  3.16it/s]\u001b[A\n",
      " 82%|█████████████████████████▍     | 92/112 [00:27<00:05,  3.38it/s]\u001b[A\n",
      " 84%|██████████████████████████     | 94/112 [00:28<00:05,  3.48it/s]\u001b[A\n",
      " 85%|██████████████████████████▎    | 95/112 [00:28<00:04,  3.96it/s]\u001b[A\n",
      " 86%|██████████████████████████▌    | 96/112 [00:28<00:03,  4.02it/s]\u001b[A\n",
      " 87%|██████████████████████████▊    | 97/112 [00:29<00:05,  2.91it/s]\u001b[A\n",
      " 88%|███████████████████████████▏   | 98/112 [00:29<00:04,  2.80it/s]\u001b[A\n",
      " 89%|██████████████████████████▊   | 100/112 [00:30<00:03,  3.03it/s]\u001b[A\n",
      " 90%|███████████████████████████   | 101/112 [00:30<00:03,  3.53it/s]\u001b[A\n",
      " 91%|███████████████████████████▎  | 102/112 [00:30<00:02,  3.94it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 103/112 [00:31<00:03,  2.97it/s]\u001b[A\n",
      " 93%|███████████████████████████▊  | 104/112 [00:31<00:02,  3.40it/s]\u001b[A\n",
      " 94%|████████████████████████████▏ | 105/112 [00:31<00:01,  4.18it/s]\u001b[A\n",
      " 95%|████████████████████████████▍ | 106/112 [00:31<00:01,  3.07it/s]\u001b[A\n",
      " 96%|████████████████████████████▋ | 107/112 [00:32<00:01,  3.45it/s]\u001b[A\n",
      " 97%|█████████████████████████████▏| 109/112 [00:32<00:00,  4.05it/s]\u001b[A\n",
      " 98%|█████████████████████████████▍| 110/112 [00:32<00:00,  3.72it/s]\u001b[A\n",
      "100%|██████████████████████████████| 112/112 [00:33<00:00,  3.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_mean Mean correlation: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                        | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/112 [00:01<02:47,  1.51s/it]\u001b[A\n",
      "  4%|█▍                              | 5/112 [00:01<00:26,  4.06it/s]\u001b[A\n",
      "  7%|██▎                             | 8/112 [00:02<00:33,  3.12it/s]\u001b[A\n",
      "  9%|██▊                            | 10/112 [00:03<00:27,  3.72it/s]\u001b[A\n",
      " 12%|███▌                           | 13/112 [00:04<00:29,  3.34it/s]\u001b[A\n",
      " 13%|████▏                          | 15/112 [00:04<00:24,  3.98it/s]\u001b[A\n",
      " 14%|████▍                          | 16/112 [00:04<00:22,  4.34it/s]\u001b[A\n",
      " 16%|████▉                          | 18/112 [00:04<00:18,  5.08it/s]\u001b[A\n",
      " 17%|█████▎                         | 19/112 [00:05<00:28,  3.23it/s]\u001b[A\n",
      " 19%|█████▊                         | 21/112 [00:05<00:23,  3.85it/s]\u001b[A\n",
      " 20%|██████                         | 22/112 [00:06<00:21,  4.15it/s]\u001b[A\n",
      " 21%|██████▋                        | 24/112 [00:06<00:17,  5.05it/s]\u001b[A\n",
      " 22%|██████▉                        | 25/112 [00:07<00:26,  3.32it/s]\u001b[A\n",
      " 23%|███████▏                       | 26/112 [00:07<00:22,  3.84it/s]\u001b[A\n",
      " 24%|███████▍                       | 27/112 [00:07<00:21,  4.00it/s]\u001b[A\n",
      " 25%|███████▊                       | 28/112 [00:07<00:18,  4.44it/s]\u001b[A\n",
      " 26%|████████                       | 29/112 [00:07<00:18,  4.42it/s]\u001b[A\n",
      " 27%|████████▎                      | 30/112 [00:07<00:18,  4.33it/s]\u001b[A\n",
      " 28%|████████▌                      | 31/112 [00:08<00:25,  3.22it/s]\u001b[A\n",
      " 29%|████████▊                      | 32/112 [00:08<00:20,  3.83it/s]\u001b[A\n",
      " 29%|█████████▏                     | 33/112 [00:08<00:18,  4.23it/s]\u001b[A\n",
      " 30%|█████████▍                     | 34/112 [00:09<00:17,  4.36it/s]\u001b[A\n",
      " 31%|█████████▋                     | 35/112 [00:09<00:21,  3.66it/s]\u001b[A\n",
      " 32%|█████████▉                     | 36/112 [00:09<00:19,  3.92it/s]\u001b[A\n",
      " 33%|██████████▏                    | 37/112 [00:09<00:21,  3.56it/s]\u001b[A\n",
      " 34%|██████████▌                    | 38/112 [00:10<00:18,  3.94it/s]\u001b[A\n",
      " 35%|██████████▊                    | 39/112 [00:10<00:15,  4.62it/s]\u001b[A\n",
      " 36%|███████████                    | 40/112 [00:10<00:17,  4.01it/s]\u001b[A\n",
      " 37%|███████████▎                   | 41/112 [00:11<00:22,  3.17it/s]\u001b[A\n",
      " 38%|███████████▋                   | 42/112 [00:11<00:20,  3.34it/s]\u001b[A\n",
      " 38%|███████████▉                   | 43/112 [00:11<00:17,  4.01it/s]\u001b[A\n",
      " 39%|████████████▏                  | 44/112 [00:11<00:16,  4.15it/s]\u001b[A\n",
      " 40%|████████████▍                  | 45/112 [00:11<00:13,  4.94it/s]\u001b[A\n",
      " 41%|████████████▋                  | 46/112 [00:12<00:18,  3.61it/s]\u001b[A\n",
      " 42%|█████████████                  | 47/112 [00:12<00:19,  3.28it/s]\u001b[A\n",
      " 43%|█████████████▎                 | 48/112 [00:12<00:20,  3.19it/s]\u001b[A\n",
      " 45%|█████████████▊                 | 50/112 [00:13<00:13,  4.57it/s]\u001b[A\n",
      " 46%|██████████████                 | 51/112 [00:13<00:12,  4.72it/s]\u001b[A\n",
      " 46%|██████████████▍                | 52/112 [00:13<00:17,  3.36it/s]\u001b[A\n",
      " 47%|██████████████▋                | 53/112 [00:14<00:16,  3.63it/s]\u001b[A\n",
      " 48%|██████████████▉                | 54/112 [00:14<00:16,  3.43it/s]\u001b[A\n",
      " 50%|███████████████▌               | 56/112 [00:14<00:12,  4.62it/s]\u001b[A\n",
      " 51%|███████████████▊               | 57/112 [00:14<00:10,  5.03it/s]\u001b[A\n",
      " 52%|████████████████               | 58/112 [00:15<00:17,  3.01it/s]\u001b[A\n",
      " 54%|████████████████▌              | 60/112 [00:15<00:13,  3.88it/s]\u001b[A\n",
      " 55%|█████████████████▏             | 62/112 [00:16<00:10,  4.69it/s]\u001b[A\n",
      " 56%|█████████████████▍             | 63/112 [00:16<00:10,  4.83it/s]\u001b[A\n",
      " 57%|█████████████████▋             | 64/112 [00:17<00:17,  2.80it/s]\u001b[A\n",
      " 59%|██████████████████▎            | 66/112 [00:17<00:11,  3.87it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 67/112 [00:17<00:10,  4.33it/s]\u001b[A\n",
      " 61%|██████████████████▊            | 68/112 [00:17<00:09,  4.45it/s]\u001b[A\n",
      " 62%|███████████████████            | 69/112 [00:17<00:09,  4.62it/s]\u001b[A\n",
      " 62%|███████████████████▍           | 70/112 [00:18<00:15,  2.64it/s]\u001b[A\n",
      " 64%|███████████████████▉           | 72/112 [00:18<00:09,  4.09it/s]\u001b[A\n",
      " 65%|████████████████████▏          | 73/112 [00:19<00:08,  4.41it/s]\u001b[A\n",
      " 66%|████████████████████▍          | 74/112 [00:19<00:08,  4.46it/s]\u001b[A\n",
      " 67%|████████████████████▊          | 75/112 [00:19<00:07,  4.85it/s]\u001b[A\n",
      " 68%|█████████████████████          | 76/112 [00:20<00:13,  2.66it/s]\u001b[A\n",
      " 69%|█████████████████████▎         | 77/112 [00:20<00:10,  3.28it/s]\u001b[A\n",
      " 71%|█████████████████████▊         | 79/112 [00:20<00:07,  4.58it/s]\u001b[A\n",
      " 71%|██████████████████████▏        | 80/112 [00:20<00:06,  4.76it/s]\u001b[A\n",
      " 72%|██████████████████████▍        | 81/112 [00:20<00:06,  5.00it/s]\u001b[A\n",
      " 73%|██████████████████████▋        | 82/112 [00:21<00:11,  2.72it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 84/112 [00:21<00:06,  4.02it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 85/112 [00:22<00:06,  3.99it/s]\u001b[A\n",
      " 77%|███████████████████████▊       | 86/112 [00:22<00:05,  4.60it/s]\u001b[A\n",
      " 78%|████████████████████████       | 87/112 [00:22<00:04,  5.08it/s]\u001b[A\n",
      " 79%|████████████████████████▎      | 88/112 [00:23<00:09,  2.51it/s]\u001b[A\n",
      " 80%|████████████████████████▉      | 90/112 [00:23<00:05,  3.69it/s]\u001b[A\n",
      " 81%|█████████████████████████▏     | 91/112 [00:23<00:05,  3.89it/s]\u001b[A\n",
      " 83%|█████████████████████████▋     | 93/112 [00:24<00:03,  5.46it/s]\u001b[A\n",
      " 84%|██████████████████████████     | 94/112 [00:24<00:05,  3.16it/s]\u001b[A\n",
      " 85%|██████████████████████████▎    | 95/112 [00:24<00:04,  3.69it/s]\u001b[A\n",
      " 86%|██████████████████████████▌    | 96/112 [00:25<00:04,  3.32it/s]\u001b[A\n",
      " 88%|███████████████████████████▏   | 98/112 [00:25<00:02,  4.92it/s]\u001b[A\n",
      " 89%|██████████████████████████▊   | 100/112 [00:26<00:03,  3.70it/s]\u001b[A\n",
      " 90%|███████████████████████████   | 101/112 [00:26<00:02,  3.77it/s]\u001b[A\n",
      " 91%|███████████████████████████▎  | 102/112 [00:26<00:02,  3.54it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 103/112 [00:26<00:02,  3.82it/s]\u001b[A\n",
      " 95%|████████████████████████████▍ | 106/112 [00:27<00:01,  3.93it/s]\u001b[A\n",
      " 96%|████████████████████████████▋ | 107/112 [00:27<00:01,  4.31it/s]\u001b[A\n",
      " 96%|████████████████████████████▉ | 108/112 [00:28<00:01,  3.74it/s]\u001b[A\n",
      " 97%|█████████████████████████████▏| 109/112 [00:28<00:00,  4.06it/s]\u001b[A\n",
      "100%|██████████████████████████████| 112/112 [00:29<00:00,  3.83it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_max Mean correlation: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                        | 0/112 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                               | 1/112 [00:01<02:51,  1.54s/it]\u001b[A\n",
      "  2%|▌                               | 2/112 [00:01<01:21,  1.36it/s]\u001b[A\n",
      "  6%|██                              | 7/112 [00:02<00:35,  2.99it/s]\u001b[A\n",
      "  7%|██▎                             | 8/112 [00:03<00:30,  3.36it/s]\u001b[A\n",
      " 10%|███                            | 11/112 [00:03<00:18,  5.46it/s]\u001b[A\n",
      " 12%|███▌                           | 13/112 [00:04<00:33,  2.93it/s]\u001b[A\n",
      " 14%|████▍                          | 16/112 [00:04<00:21,  4.46it/s]\u001b[A\n",
      " 16%|████▉                          | 18/112 [00:04<00:17,  5.48it/s]\u001b[A\n",
      " 18%|█████▌                         | 20/112 [00:06<00:30,  2.98it/s]\u001b[A\n",
      " 22%|██████▉                        | 25/112 [00:07<00:25,  3.45it/s]\u001b[A\n",
      " 23%|███████▏                       | 26/112 [00:07<00:24,  3.57it/s]\u001b[A\n",
      " 28%|████████▌                      | 31/112 [00:09<00:21,  3.76it/s]\u001b[A\n",
      " 29%|████████▊                      | 32/112 [00:09<00:21,  3.76it/s]\u001b[A\n",
      " 30%|█████████▍                     | 34/112 [00:09<00:17,  4.59it/s]\u001b[A\n",
      " 33%|██████████▏                    | 37/112 [00:10<00:19,  3.87it/s]\u001b[A\n",
      " 34%|██████████▌                    | 38/112 [00:10<00:18,  3.91it/s]\u001b[A\n",
      " 35%|██████████▊                    | 39/112 [00:10<00:17,  4.28it/s]\u001b[A\n",
      " 37%|███████████▎                   | 41/112 [00:10<00:13,  5.25it/s]\u001b[A\n",
      " 38%|███████████▉                   | 43/112 [00:11<00:19,  3.60it/s]\u001b[A\n",
      " 39%|████████████▏                  | 44/112 [00:12<00:18,  3.63it/s]\u001b[A\n",
      " 41%|████████████▋                  | 46/112 [00:12<00:13,  4.78it/s]\u001b[A\n",
      " 42%|█████████████                  | 47/112 [00:12<00:13,  4.86it/s]\u001b[A\n",
      " 43%|█████████████▎                 | 48/112 [00:12<00:12,  5.04it/s]\u001b[A\n",
      " 44%|█████████████▌                 | 49/112 [00:13<00:20,  3.13it/s]\u001b[A\n",
      " 45%|█████████████▊                 | 50/112 [00:13<00:20,  3.04it/s]\u001b[A\n",
      " 46%|██████████████▍                | 52/112 [00:13<00:12,  4.64it/s]\u001b[A\n",
      " 47%|██████████████▋                | 53/112 [00:14<00:12,  4.63it/s]\u001b[A\n",
      " 48%|██████████████▉                | 54/112 [00:14<00:13,  4.45it/s]\u001b[A\n",
      " 49%|███████████████▏               | 55/112 [00:14<00:17,  3.20it/s]\u001b[A\n",
      " 50%|███████████████▌               | 56/112 [00:15<00:18,  3.04it/s]\u001b[A\n",
      " 52%|████████████████               | 58/112 [00:15<00:11,  4.59it/s]\u001b[A\n",
      " 53%|████████████████▎              | 59/112 [00:15<00:12,  4.30it/s]\u001b[A\n",
      " 54%|████████████████▌              | 60/112 [00:16<00:13,  3.92it/s]\u001b[A\n",
      " 54%|████████████████▉              | 61/112 [00:16<00:14,  3.42it/s]\u001b[A\n",
      " 55%|█████████████████▏             | 62/112 [00:16<00:16,  3.03it/s]\u001b[A\n",
      " 57%|█████████████████▋             | 64/112 [00:17<00:10,  4.62it/s]\u001b[A\n",
      " 58%|█████████████████▉             | 65/112 [00:17<00:10,  4.32it/s]\u001b[A\n",
      " 59%|██████████████████▎            | 66/112 [00:17<00:13,  3.42it/s]\u001b[A\n",
      " 60%|██████████████████▌            | 67/112 [00:17<00:12,  3.72it/s]\u001b[A\n",
      " 61%|██████████████████▊            | 68/112 [00:18<00:13,  3.36it/s]\u001b[A\n",
      " 62%|███████████████████            | 69/112 [00:18<00:10,  4.05it/s]\u001b[A\n",
      " 62%|███████████████████▍           | 70/112 [00:18<00:09,  4.56it/s]\u001b[A\n",
      " 63%|███████████████████▋           | 71/112 [00:18<00:09,  4.11it/s]\u001b[A\n",
      " 64%|███████████████████▉           | 72/112 [00:19<00:13,  2.92it/s]\u001b[A\n",
      " 66%|████████████████████▍          | 74/112 [00:19<00:10,  3.61it/s]\u001b[A\n",
      " 67%|████████████████████▊          | 75/112 [00:20<00:09,  4.07it/s]\u001b[A\n",
      " 68%|█████████████████████          | 76/112 [00:20<00:08,  4.48it/s]\u001b[A\n",
      " 69%|█████████████████████▎         | 77/112 [00:20<00:08,  4.06it/s]\u001b[A\n",
      " 70%|█████████████████████▌         | 78/112 [00:21<00:12,  2.64it/s]\u001b[A\n",
      " 71%|██████████████████████▏        | 80/112 [00:21<00:08,  3.77it/s]\u001b[A\n",
      " 72%|██████████████████████▍        | 81/112 [00:21<00:07,  4.19it/s]\u001b[A\n",
      " 73%|██████████████████████▋        | 82/112 [00:21<00:07,  4.06it/s]\u001b[A\n",
      " 74%|██████████████████████▉        | 83/112 [00:22<00:06,  4.53it/s]\u001b[A\n",
      " 75%|███████████████████████▎       | 84/112 [00:22<00:08,  3.29it/s]\u001b[A\n",
      " 76%|███████████████████████▌       | 85/112 [00:22<00:08,  3.26it/s]\u001b[A\n",
      " 77%|███████████████████████▊       | 86/112 [00:23<00:07,  3.46it/s]\u001b[A\n",
      " 78%|████████████████████████       | 87/112 [00:23<00:05,  4.21it/s]\u001b[A\n",
      " 79%|████████████████████████▎      | 88/112 [00:23<00:06,  3.70it/s]\u001b[A\n",
      " 80%|████████████████████████▉      | 90/112 [00:24<00:05,  3.69it/s]\u001b[A\n",
      " 81%|█████████████████████████▏     | 91/112 [00:24<00:06,  3.33it/s]\u001b[A\n",
      " 82%|█████████████████████████▍     | 92/112 [00:24<00:05,  3.65it/s]\u001b[A\n",
      " 84%|██████████████████████████     | 94/112 [00:25<00:04,  4.36it/s]\u001b[A\n",
      " 85%|██████████████████████████▎    | 95/112 [00:25<00:03,  4.53it/s]\u001b[A\n",
      " 86%|██████████████████████████▌    | 96/112 [00:25<00:04,  3.77it/s]\u001b[A\n",
      " 87%|██████████████████████████▊    | 97/112 [00:26<00:04,  3.08it/s]\u001b[A\n",
      " 88%|███████████████████████████▏   | 98/112 [00:26<00:03,  3.73it/s]\u001b[A\n",
      " 89%|██████████████████████████▊   | 100/112 [00:26<00:02,  4.30it/s]\u001b[A\n",
      " 90%|███████████████████████████   | 101/112 [00:26<00:02,  4.01it/s]\u001b[A\n",
      " 91%|███████████████████████████▎  | 102/112 [00:27<00:02,  3.97it/s]\u001b[A\n",
      " 92%|███████████████████████████▌  | 103/112 [00:27<00:03,  2.90it/s]\u001b[A\n",
      " 95%|████████████████████████████▍ | 106/112 [00:28<00:01,  4.22it/s]\u001b[A\n",
      " 96%|████████████████████████████▋ | 107/112 [00:28<00:01,  3.50it/s]\u001b[A\n",
      " 97%|█████████████████████████████▏| 109/112 [00:29<00:00,  3.75it/s]\u001b[A\n",
      " 98%|█████████████████████████████▍| 110/112 [00:29<00:00,  4.07it/s]\u001b[A\n",
      "100%|██████████████████████████████| 112/112 [00:29<00:00,  3.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_0 Mean correlation: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for dataset_name in datasets:\n",
    "        \n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "\n",
    "        for bucket in tqdm_notebook(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            \n",
    "            #Identify feature names\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "            cat_cols = trainingdata.columns[cats]\n",
    "            \n",
    "            #scale data if necessary\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "                trainingdata = pd.DataFrame(trainingdata, columns=feat_list)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)))\n",
    "\n",
    "            #scale data if necessary\n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "            \n",
    "\n",
    "            #trainingdata = pd.DataFrame(trainingdata, columns=feat_list)\n",
    "            min_X = np.min(trainingdata)\n",
    "            max_X = np.max(trainingdata)\n",
    "            mean_X = np.mean(trainingdata, axis=0)\n",
    "            unique_values = pd.Series({col: trainingdata[col].unique() for col in trainingdata.columns})\n",
    "\n",
    "            for mode in modes:\n",
    "                ktb_list = []\n",
    "                true_v_mape = []\n",
    "                true_v_rmse = []\n",
    "                true_v_r2 = []\n",
    "                \n",
    "                pool = mp.Pool(mp.cpu_count(), initargs=(mp.RLock(),), initializer=tqdm.set_lock)\n",
    "\n",
    "                for result in tqdm(pool.imap(evaluate, [i for i in range(len(sample_instances))]), total = len(sample_instances)):\n",
    "                    true_v_mape.append(result[0])\n",
    "                    true_v_rmse.append(result[1])\n",
    "                    true_v_r2.append(result[2])\n",
    "                    \n",
    "                results[\"MAPE Correctness\"] = true_v_mape\n",
    "                results[\"RMSE Correctness\"] = true_v_rmse\n",
    "                results[\"R2 Correctness\"] = true_v_r2\n",
    "                results[\"Mode\"] = [mode]*results.shape[0]\n",
    "                \n",
    "                print(mode, \"Mean correlation:\", np.mean(results[\"R2 Correctness\"]))\n",
    "                \n",
    "                results.to_csv(os.path.join(save_to,\"samples\", mode+\"_results_bucket_%s.csv\" % (bucketID)), \n",
    "                               index = False, sep = \";\")\n",
    "                \n",
    "                all_results.append(results)\n",
    "                \n",
    "#pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/all_perm_results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "#                               sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.columns[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p2_list.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fidelity_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
