{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b1eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, mean_absolute_percentage_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import scipy.stats as ss\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89a523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_characteristics(tree, feat_list, cur_depth = 0, lvl = 0, depths = [], split_nodes = [], leaf_nodes = []):\n",
    "\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "    \n",
    "    if left_child == sklearn.tree._tree.TREE_LEAF:\n",
    "        depths.append(cur_depth)\n",
    "        leaf_nodes.append(lvl)\n",
    "        \n",
    "    else:\n",
    "        split_nodes.append(lvl)\n",
    "        depths, split_nodes, leaf_nodes = get_tree_characteristics(tree, feat_list, cur_depth+1, left_child, depths, split_nodes, leaf_nodes)\n",
    "        depths, split_noes, leaf_nodes = get_tree_characteristics(tree, feat_list, cur_depth+1, right_child, depths, split_nodes, leaf_nodes)\n",
    "        \n",
    "    return depths, split_nodes, leaf_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9480b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        feat_pos[i]+=n\n",
    "        n=n-1\n",
    "    \n",
    "    return feat_pos\n",
    "\n",
    "def get_reg_features(cls):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    #coef = [abs(val) for val in og_coef]\n",
    "    coef = og_coef.copy()\n",
    "        \n",
    "    return coef\n",
    "\n",
    "def get_nb_features(cls, instance):\n",
    "    pred = cls.predict(instance.reshape(1, -1)).astype(int)\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "    \n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "\n",
    "    likelihoods = []\n",
    "    \n",
    "    for i in range(len(means)):\n",
    "        lk = ss.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = ss.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = lk-alt_lk\n",
    "        likelihoods.append(lkhood)\n",
    "        \n",
    "    return np.abs(likelihoods)\n",
    "\n",
    "def get_true_rankings(cls, instance, cls_method, X_train, feat_list):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        feat_pos = get_reg_features(cls)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        feat_pos = get_nb_features(cls, instance)\n",
    "        \n",
    "    return feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b7bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_instance(instance, i, perm_iter = 100, min_i = [0], max_i=[1], mean_i=[0], unique_values=[[0,1]], mode=\"permutation\"):\n",
    "\n",
    "    permutations = np.array([instance]*perm_iter).transpose()\n",
    "\n",
    "    for j in range(len(i)):\n",
    "        if mode==\"baseline_max\":\n",
    "            n_val = [max_i[j]]*perm_iter\n",
    "        elif mode==\"baseline\" or mode==\"baseline_mean\":\n",
    "            n_val = [mean_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_min\":\n",
    "            n_val = [min_i[j]]*perm_iter\n",
    "        elif mode==\"baseline_0\":\n",
    "            n_val = [0]*perm_iter\n",
    "        else:\n",
    "            n_val = np.random.choice(unique_values[j], perm_iter)\n",
    "\n",
    "        permutations[i[j]] = n_val\n",
    "        \n",
    "    permutations = permutations.transpose()\n",
    "\n",
    "    return permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ab2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples: int = 1000, intercept = 1, sigma = 0.2, num_features = 3):\n",
    "    \n",
    "    #weights = np.random.uniform(-1, 1, num_features)#.reshape(-1, 1)  # Arbitrarily chosen coefficients\n",
    "    weights = [0.55, 0.503, 0.551, 0.49, 0.459]\n",
    "    noise = np.random.randn(n_samples)*sigma # Gaussian noise with standard deviation 0.2\n",
    "\n",
    "    ##CONTINUOUS FEATURES ONLY\n",
    "    # sample X2 from a normal distribution with center 1 and std 2\n",
    "    cont_features = [np.random.randn(n_samples) for i in range(num_features)]\n",
    "\n",
    "    # Generate X1 as a linear combination of the features plus some Gaussian noise\n",
    "    full_weights = [[weights[i]]*n_samples for i in range(num_features)]\n",
    "    cls = intercept + np.sum(np.multiply(full_weights,cont_features), axis=0) + noise\n",
    "    \n",
    "    threshold = np.mean(cls)\n",
    "    X1 = np.zeros(len(cls))\n",
    "    X1[np.where(cls>threshold)] = 1    \n",
    "    \n",
    "    # Combine into a dataframe\n",
    "    cont_df = pd.DataFrame(np.transpose(cont_features), columns = [\"X%s\"%(i) for i in range(2, num_features+2)])\n",
    "    cont_df[\"X1\"] = X1  \n",
    "        \n",
    "    ##CATEGORICAL FEATURES ONLY\n",
    "    cat_features = np.zeros(np.array(cont_features).shape)\n",
    "    cat_features[np.where(np.array(cont_features)>np.mean(cont_features, axis=0))]=1\n",
    "    ## Generate X1 as a linear combination of the features plus some Gaussian noise\n",
    "    full_weights = [[weights[i]]*n_samples for i in range(num_features)]\n",
    "    cls = intercept + np.sum(np.multiply(full_weights,cat_features), axis=0) + noise\n",
    "    \n",
    "    threshold = np.mean(cls)\n",
    "    X1 = np.zeros(len(cls))\n",
    "    X1[np.where(cls>threshold)] = 1    \n",
    "    # Combine into a dataframe\n",
    "    cat_df = pd.DataFrame(np.transpose(cat_features), columns = [\"X%s\"%(i) for i in range(2, num_features+2)])\n",
    "    cat_df[\"X1\"] = X1\n",
    "        \n",
    "    ##CONTINUOUS AND CATEGORICAL FEATURES\n",
    "    cats = num_features//2\n",
    "    mixed_features = np.vstack((cat_features[:cats],np.array(cont_features)[cats:]))\n",
    " \n",
    "    # Generate X1 as a linear combination of the features plus some Gaussian noise\n",
    "    full_weights = [[weights[i]]*n_samples for i in range(num_features)]\n",
    "    cls = intercept + np.sum(np.multiply(full_weights,mixed_features), axis=0) + noise\n",
    "    \n",
    "    threshold = np.mean(cls)\n",
    "    X1 = np.zeros(len(cls))\n",
    "    X1[np.where(cls>threshold)] = 1    \n",
    "    \n",
    "   # Combine into a dataframe\n",
    "    mix_df = pd.DataFrame(np.transpose(mixed_features), columns = [\"X%s\"%(i) for i in range(2, num_features+2)])\n",
    "    mix_df[\"X1\"] = X1\n",
    "    \n",
    "    return cont_df, cat_df, mix_df, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028ecbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55, 0.503, 0.551, 0.49, 0.459]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.124900</td>\n",
       "      <td>-1.799708</td>\n",
       "      <td>0.447009</td>\n",
       "      <td>-0.938291</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.260155</td>\n",
       "      <td>0.109919</td>\n",
       "      <td>0.554466</td>\n",
       "      <td>1.433460</td>\n",
       "      <td>0.046705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.174450</td>\n",
       "      <td>-0.236026</td>\n",
       "      <td>-0.800100</td>\n",
       "      <td>1.466358</td>\n",
       "      <td>1.690151</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.731989</td>\n",
       "      <td>0.624614</td>\n",
       "      <td>0.339062</td>\n",
       "      <td>0.586095</td>\n",
       "      <td>1.261578</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.864385</td>\n",
       "      <td>-0.134838</td>\n",
       "      <td>-0.338043</td>\n",
       "      <td>-1.102814</td>\n",
       "      <td>1.535411</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X2        X3        X4        X5        X6   X1\n",
       "0 -1.124900 -1.799708  0.447009 -0.938291  0.019894  0.0\n",
       "1  2.260155  0.109919  0.554466  1.433460  0.046705  1.0\n",
       "2 -0.174450 -0.236026 -0.800100  1.466358  1.690151  1.0\n",
       "3 -0.731989  0.624614  0.339062  0.586095  1.261578  1.0\n",
       "4  0.864385 -0.134838 -0.338043 -1.102814  1.535411  1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a sample dataset\n",
    "cont_data, cat_data, mixed_data,weights = generate_data(n_samples = 2000, intercept=1, sigma=0.05, num_features = 5)\n",
    "print(weights)\n",
    "cont_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3f61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = True\n",
    "lr = True\n",
    "nb = True\n",
    "\n",
    "random_state = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cda3a6",
   "metadata": {},
   "source": [
    "# Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5324b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cont_data[cont_data.columns.drop(\"X1\")]\n",
    "y = cont_data[\"X1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b76ccbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83       227\n",
      "         1.0       0.88      0.82      0.85       273\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.84      0.84      0.84       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n",
      "Max depth: 12 \t Num splits: 182 \t Num leaves: 183\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "if dt:\n",
    "    space = {\"splitter\": [\"best\", \"random\"],\n",
    "             \"min_samples_split\": [random.uniform(0, 1) for i in range (50)],\n",
    "             \"max_features\": [random.uniform(0,1) for i in range (50)],\n",
    "             \"criterion\":[\"gini\"]}#, \"entropy\"]}\n",
    "    estimator = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_df = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls.predict(X_test)))\n",
    "    \n",
    "    split_nodes = []\n",
    "    leaf_nodes = []\n",
    "\n",
    "    depths, split_nodes, leaf_nodes = get_tree_characteristics(cls_df.tree_, X_train.columns, split_nodes = split_nodes, leaf_nodes = leaf_nodes)\n",
    "\n",
    "    max_depth = cls_df.get_depth()\n",
    "    splits = len(split_nodes)\n",
    "    leaves = len(leaf_nodes)\n",
    "\n",
    "    print(\"Max depth: %s \\t Num splits: %s \\t Num leaves: %s\" %(max_depth, splits, leaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dc8d219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.6\n",
      "Global explanation fidelity: 0.7999999999999999\n",
      "Global data-explanation fidelity: 0.7999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.42004572636427373\n",
      "Local explanation fidelity: 0.13259591739845158\n",
      "Local data-explanation fidelity: 0.3156886217434191\n"
     ]
    }
   ],
   "source": [
    "if dt:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    dt_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "#     print(\"Weights\", cls_df.feature_importances_)\n",
    "#     print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), cls_df.feature_importances_.reshape(1, -1), variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_df, instance, \"decision_tree\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        \n",
    "        dt_global.append(tr)\n",
    "        \n",
    "        pred = cls_df.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_df.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_df.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))#/np.mean(cdist(permutations, [instance])))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(dt_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(dt_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee17ba33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d08c788f3164100b3624dba34ce175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m permute_instance(instance, [i], \u001b[38;5;241m1000\u001b[39m, [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmin()], [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmax()],\n\u001b[0;32m     23\u001b[0m                                    [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmean()], [X_train\u001b[38;5;241m.\u001b[39miloc[:,i]\u001b[38;5;241m.\u001b[39munique()], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermutation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     p2_list \u001b[38;5;241m=\u001b[39m cls_df\u001b[38;5;241m.\u001b[39mpredict_proba(permutations)\u001b[38;5;241m.\u001b[39mtranspose()[pred]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     change\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean([(p1_list[i] \u001b[38;5;241m-\u001b[39m p2_list[i])\u001b[38;5;241m/\u001b[39mcdist(instance, permutations[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(p1_list))]))\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#change.append(mean_absolute_percentage_error(p1_list, p2_list)/np.mean([MAPE(instance, permutation) for permutation in permutations]))\u001b[39;00m\n\u001b[0;32m     28\u001b[0m mx_corr\u001b[38;5;241m.\u001b[39mappend(ss\u001b[38;5;241m.\u001b[39mkendalltau(tr, change, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstatistic)\n",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m permute_instance(instance, [i], \u001b[38;5;241m1000\u001b[39m, [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmin()], [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmax()],\n\u001b[0;32m     23\u001b[0m                                    [X_train\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mmean()], [X_train\u001b[38;5;241m.\u001b[39miloc[:,i]\u001b[38;5;241m.\u001b[39munique()], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermutation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     p2_list \u001b[38;5;241m=\u001b[39m cls_df\u001b[38;5;241m.\u001b[39mpredict_proba(permutations)\u001b[38;5;241m.\u001b[39mtranspose()[pred]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     change\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean([(p1_list[i] \u001b[38;5;241m-\u001b[39m p2_list[i])\u001b[38;5;241m/\u001b[39m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermutations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(p1_list))]))\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#change.append(mean_absolute_percentage_error(p1_list, p2_list)/np.mean([MAPE(instance, permutation) for permutation in permutations]))\u001b[39;00m\n\u001b[0;32m     28\u001b[0m mx_corr\u001b[38;5;241m.\u001b[39mappend(ss\u001b[38;5;241m.\u001b[39mkendalltau(tr, change, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstatistic)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:2916\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2913\u001b[0m sB \u001b[38;5;241m=\u001b[39m XB\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sB) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXB must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "if dt:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    dt_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "#     print(\"Weights\", cls_df.feature_importances_)\n",
    "#     print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), cls_df.feature_importances_.reshape(1, -1), variant=\"b\").statistic)\n",
    "\n",
    "    for instance in tqdm_notebook(X_test.values):\n",
    "        tr = get_true_rankings(cls_df, instance, \"decision_tree\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        \n",
    "        dt_global.append(tr)\n",
    "        \n",
    "        pred = cls_df.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_df.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_df.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(np.mean([(p1_list[i] - p2_list[i])/cdist(instance, permutations[i]) for i in range(len(p1_list))]))\n",
    "            #change.append(mean_absolute_percentage_error(p1_list, p2_list)/np.mean([MAPE(instance, permutation) for permutation in permutations]))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(dt_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(dt_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d473f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([(p1_list[i] - p2_list[i])/mean_absolute_percentage_error(instance, permutations[i]) for i in range(len(p1_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "893b0f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.]),\n",
       " array([0.])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(p1_list[i] - p2_list[i]) for i in range(len(p1_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06de590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.32915568000061424,\n",
       " array([ 5.09412461e-01,  1.07207775e+00, -1.75187029e+00, -5.93779275e-01,\n",
       "         1.41957807e+00, -1.66455264e-01,  1.61131101e+00, -7.54839171e-01,\n",
       "        -1.17296891e+00, -2.41727574e+00,  5.78933764e-02, -1.29342911e+00,\n",
       "         2.07322239e-02, -1.03532264e+00, -1.66770510e-02,  6.63002323e-01,\n",
       "         1.10423725e+00, -9.54573230e-01, -3.82129289e-01, -8.88498272e-01,\n",
       "        -1.45760586e+00,  6.16822694e-02, -5.09863546e-01, -1.22949382e+00,\n",
       "        -2.30921735e-01,  1.25999955e+00,  8.21085704e-01, -5.19448052e-01,\n",
       "         2.58797336e-01, -4.02520099e-01,  4.97040921e-01, -8.36120164e-02,\n",
       "        -8.20727015e-01, -1.88295262e-01, -1.03109742e+00,  3.36586858e-01,\n",
       "         2.12697708e+00,  5.07240911e-01,  5.21627624e-01,  2.82504430e-02,\n",
       "         1.18751227e+00, -1.45779521e+00,  6.06153724e-02, -1.86427693e+00,\n",
       "         9.12249770e-02,  3.53900284e-01,  4.77226348e-01, -1.11266236e+00,\n",
       "         4.57485582e-01, -7.74546916e-01,  1.09605141e-01, -1.36907122e+00,\n",
       "        -4.54757956e-01,  1.76513762e-01,  1.21342421e+00,  4.11536452e-01,\n",
       "         1.06491218e+00, -7.61295031e-01,  9.76512076e-01,  1.61131101e+00,\n",
       "         3.77211847e-02,  1.35824955e+00,  3.58368075e-01,  1.23739701e+00,\n",
       "        -8.25725913e-01, -1.14436666e+00, -1.38774195e+00,  1.17667123e+00,\n",
       "         1.05648381e+00, -6.46970588e-01,  4.38563509e-01, -9.15238272e-01,\n",
       "         1.54942789e+00,  5.06384540e-01, -2.52527035e-01,  2.58797336e-01,\n",
       "        -9.05246336e-01, -1.24934037e-01,  1.54361075e+00,  7.41210430e-01,\n",
       "        -8.86665734e-01,  6.08819295e-01, -1.03925298e-01,  6.72806606e-04,\n",
       "         2.40929663e-01, -5.22992829e-01,  1.05436692e+00,  1.93479399e-01,\n",
       "        -6.00814532e-02,  1.53210152e+00, -2.88513572e-01, -7.44980414e-02,\n",
       "         3.55713429e-01,  6.45545998e-01, -1.21054528e-01, -1.28366782e+00,\n",
       "        -1.42340942e-01,  4.00636545e-01,  1.17667123e+00, -1.08145709e+00,\n",
       "         1.21967154e+00, -8.32734234e-01,  3.05789744e-01, -4.01575367e-01,\n",
       "        -9.30978156e-01,  1.59329474e+00, -7.74546916e-01,  9.82877011e-01,\n",
       "         1.25069263e+00, -5.34746582e-01, -4.43027700e-01, -1.16732149e+00,\n",
       "        -3.12442316e+00,  3.64851590e-03, -9.19916933e-02, -1.11568155e-01,\n",
       "        -4.29304759e-01,  4.15381372e-01,  1.76660357e+00,  3.55158978e-01,\n",
       "        -6.06085698e-01, -1.11520916e+00, -1.59226324e-01,  2.02610868e+00,\n",
       "        -6.75255417e-01,  1.91459580e-01, -1.04087640e+00, -6.50269061e-01,\n",
       "         4.19273604e-01, -1.41402025e+00, -8.72514475e-02, -1.57450091e-01,\n",
       "         5.10062038e-01, -6.75255417e-01,  1.63928367e+00, -2.72731565e-01,\n",
       "         8.23486684e-01,  2.63765042e+00,  8.36086373e-01,  1.13298894e+00,\n",
       "         1.22077205e+00,  1.24559129e-01, -4.37242797e-01, -1.41546509e+00,\n",
       "         5.67303336e-01,  8.00137671e-01,  1.32366600e+00,  1.51077832e+00,\n",
       "         3.70591390e-02, -4.05331801e-01,  2.88431280e-01,  3.55158978e-01,\n",
       "         1.05932550e+00, -1.08008761e+00, -5.34746582e-01, -1.43002101e+00,\n",
       "        -1.13763611e+00, -5.95331737e-03,  5.61953676e-02, -1.02075551e+00,\n",
       "         2.14031179e-01, -1.28376083e+00, -4.41591028e-01, -7.15316224e-01,\n",
       "         2.57709924e-01,  8.67554830e-01,  1.50354689e-01, -2.30022634e-01,\n",
       "         1.93479399e-01, -2.54706669e-01,  1.13298894e+00, -1.03826175e-02,\n",
       "         7.86214545e-02,  7.53438608e-01, -1.41546509e+00,  4.39401738e-01,\n",
       "         1.20082157e-01,  8.96574495e-01, -3.12566006e-01, -8.30462102e-01,\n",
       "        -2.38652753e-01, -4.09401539e-01, -1.28759645e+00,  1.76513762e-01,\n",
       "         1.48536548e+00,  1.75941076e+00, -4.93227318e-01, -5.11489719e-01,\n",
       "         1.12029989e+00,  8.36086373e-01, -8.29857337e-01, -1.54173812e+00,\n",
       "         8.17628586e-02,  1.10086310e+00,  4.38563509e-01, -9.87182928e-01,\n",
       "        -5.40124183e-01,  8.78050910e-01, -5.56287302e-01, -3.23809545e-01,\n",
       "         7.71942338e-01,  1.51103453e+00, -6.93104354e-01,  1.54361075e+00,\n",
       "        -1.00462890e+00, -1.04993428e+00,  4.27338189e-01,  8.61727700e-01,\n",
       "        -5.81510670e-01,  8.30797002e-01, -1.11568155e-01, -1.15837560e-01,\n",
       "         3.36586858e-01,  4.87965502e-01,  4.57485582e-01,  2.12672171e+00,\n",
       "        -1.20269874e+00, -5.28166812e-01,  3.79633430e-01, -1.84479819e+00,\n",
       "         7.33763928e-01,  1.87926188e+00,  3.95721667e-01, -1.21616683e+00,\n",
       "         1.84358100e+00, -4.25590745e-01,  8.41861899e-01,  3.96201537e-01,\n",
       "         9.38622349e-01,  8.39747200e-01,  9.82877011e-01,  5.09412461e-01,\n",
       "         8.23486684e-01, -2.22267779e+00, -1.50040239e-01, -2.19208279e+00,\n",
       "        -1.41402025e+00,  1.75941076e+00,  9.22086345e-01,  8.67554830e-01,\n",
       "         1.69873310e+00,  3.82217395e-01, -4.58264091e-01, -2.29761980e+00,\n",
       "         5.70911753e-01, -7.79375155e-01,  6.31860010e-01,  7.30931990e-01,\n",
       "        -9.56366837e-01,  1.59400720e+00, -1.38250727e+00, -6.37785687e-01,\n",
       "        -9.40103560e-01,  1.13038126e+00, -3.19319836e-01,  1.30542604e+00,\n",
       "         1.00289446e-01,  1.42167448e+00,  4.07095748e-01,  1.01495512e+00,\n",
       "        -5.50197078e-01,  1.12377626e+00,  6.83405770e-01, -1.11568155e-01,\n",
       "        -4.22038824e-01,  5.95866687e-01, -1.03767769e+00, -1.11392489e+00,\n",
       "        -7.86502464e-01,  7.28787000e-01, -4.29228101e-01, -5.81510670e-01,\n",
       "         1.45541637e+00,  2.69797170e-01,  1.92652231e+00, -9.95951455e-02,\n",
       "         8.14079259e-01,  2.05271392e+00, -3.11343160e-01,  3.40539111e-01,\n",
       "         9.22086345e-01,  7.34371590e-02,  9.29231168e-01,  1.58518064e-01,\n",
       "         2.49014179e-01,  7.76485306e-01, -1.02477699e+00, -9.24956009e-01,\n",
       "        -3.52009918e-01,  1.84546904e+00,  1.87794927e+00, -1.59525909e+00,\n",
       "         1.01696059e+00,  1.21328867e+00, -4.71297803e-01, -3.79300741e-01,\n",
       "        -1.46917548e-01, -6.78656742e-01, -1.59525909e+00, -5.56287302e-01,\n",
       "         1.82388958e+00,  2.23151909e-01, -3.12069841e-01, -1.54173812e+00,\n",
       "         1.63928367e+00,  1.24239871e+00,  3.09595839e-02,  1.59516695e+00,\n",
       "         7.63501437e-01,  1.15878994e+00,  1.35411404e+00, -6.68853632e-01,\n",
       "         7.59952810e-01,  2.83872488e-01, -1.03727541e-01,  5.73817738e-01,\n",
       "         1.31512576e+00,  1.61131101e+00,  6.53458100e-01,  7.57263681e-01,\n",
       "         5.92572372e-01, -1.06625717e+00,  1.91372633e-01, -4.47728733e-01,\n",
       "        -8.35266216e-01, -6.34973768e-01, -7.87755495e-03, -2.41727574e+00,\n",
       "         1.47733805e+00, -1.20779629e+00, -5.09144377e-01, -1.35706823e-02,\n",
       "         6.50897179e-01,  1.55944896e+00,  9.45147042e-01, -1.03727541e-01,\n",
       "         1.40874269e+00,  6.19782934e-01,  1.76513762e-01,  6.55798361e-01,\n",
       "        -3.11396507e-02,  4.12353865e-01,  7.10258744e-01,  8.94854182e-01,\n",
       "         7.41009928e-01, -4.58425332e-01, -5.49781407e-01, -8.68350409e-01,\n",
       "         3.33883334e-01,  4.80409747e-01,  7.28787000e-01, -3.23809545e-01,\n",
       "        -1.05353240e+00,  7.33763928e-01, -4.47728733e-01, -1.22414422e+00,\n",
       "        -1.08886734e+00,  2.73725772e-01, -6.96212934e-01,  6.33153132e-01,\n",
       "         8.08281852e-01, -4.43965443e-01, -6.46970588e-01,  1.05648381e+00,\n",
       "         1.12149610e+00,  3.27195318e-01, -2.92596227e-01,  7.49007231e-01,\n",
       "         1.81070800e-01, -1.61266987e-02,  4.72832081e-01,  1.10423725e+00,\n",
       "        -4.35532177e-01, -1.10647133e-01, -7.37553665e-01, -1.94892037e-01,\n",
       "         4.44206828e-01, -1.59525909e+00,  4.26137582e-01,  3.63607332e-01,\n",
       "        -2.22267779e+00, -1.46664874e+00, -5.50197078e-01, -5.44518793e-01,\n",
       "         8.55085024e-01, -6.09901772e-02, -1.88295262e-01, -1.61266987e-02,\n",
       "        -1.69424835e+00, -3.69663318e-01, -1.35535845e+00,  1.75941076e+00,\n",
       "        -9.10739641e-01,  1.27330980e+00, -4.59675164e-01,  6.24699990e-02,\n",
       "        -8.41582699e-02,  9.06402595e-01,  5.30305914e-01, -1.04278260e+00,\n",
       "        -2.91713214e-01,  6.16676671e-01,  1.00289446e-01, -1.02075551e+00,\n",
       "         4.97040921e-01,  1.15088173e+00, -2.05032046e+00,  5.09412461e-01,\n",
       "         1.39340293e+00,  2.73811488e-01,  9.41131290e-01, -6.07448802e-01,\n",
       "         1.38095654e+00,  4.92403305e-01,  1.25684262e+00,  1.65835756e+00,\n",
       "        -9.53741434e-01,  2.90886238e-01,  1.54032238e+00, -1.41061991e+00,\n",
       "        -4.93227318e-01,  4.47772600e-01,  6.06153724e-02,  1.15881582e-01,\n",
       "         2.86495044e-01, -1.44362769e+00,  3.05789744e-01, -2.26364001e+00,\n",
       "        -9.56366837e-01,  8.73212451e-01,  2.54433400e-01,  8.41861899e-01,\n",
       "        -7.74546916e-01,  1.05648381e+00, -1.06625717e+00, -1.24003986e+00,\n",
       "         1.57570822e+00,  5.50753416e-01, -9.68856497e-01,  2.02792090e+00,\n",
       "        -8.78154549e-01, -5.27927641e-01, -5.44789647e-01, -8.03290587e-01,\n",
       "         1.08604462e-01, -3.82129289e-01, -4.95560402e-01,  8.96574495e-01,\n",
       "         6.33153132e-01,  6.50897179e-01, -5.46669751e-01, -7.06040496e-01,\n",
       "         2.23727471e+00,  1.03294929e-01, -3.52437625e-01,  5.30305914e-01,\n",
       "        -8.19590685e-01,  1.22413964e+00, -1.11568155e-01,  3.92816095e-01,\n",
       "        -3.87616976e-02, -2.52701223e-01, -5.59947046e-01,  7.10258744e-01,\n",
       "         7.23240290e-01, -2.30022634e-01, -1.94007478e+00, -5.49379428e-01,\n",
       "         1.13038126e+00, -8.21411665e-01,  4.48956026e-01, -1.79063636e+00,\n",
       "        -1.27667194e+00, -1.22414422e+00, -9.13609692e-01, -2.79228766e-01,\n",
       "        -4.79244147e-01, -6.03890952e-01, -9.63040866e-01,  1.14627927e+00,\n",
       "         1.15088173e+00,  1.31512576e+00, -3.52437625e-01, -2.87131090e+00,\n",
       "         6.15117289e-01, -1.00462890e+00,  1.02222200e+00,  1.21322868e+00,\n",
       "         2.11440366e+00,  1.74267618e-01,  2.54433400e-01,  7.75894523e-01,\n",
       "         1.19518722e+00,  1.16388954e+00,  3.41853687e-01,  1.17598904e+00,\n",
       "         2.18146150e-01, -1.30672999e+00, -4.35098851e-02,  1.03294929e-01,\n",
       "         1.69217307e+00,  7.53438608e-01,  1.69091431e+00,  1.56984575e+00,\n",
       "         9.06402595e-01,  1.55754017e-01,  9.58376348e-01, -1.59226324e-01,\n",
       "        -3.46498550e-01,  9.12504354e-01,  3.92523823e-01, -1.13763611e+00,\n",
       "        -5.08744015e-01,  7.41210430e-01, -1.47171117e+00,  1.05436692e+00,\n",
       "         2.70999392e-01, -1.42130410e-01, -8.39936771e-01, -1.20858300e+00,\n",
       "         3.33883334e-01, -8.86665734e-01,  1.51784582e-01, -7.59306351e-01,\n",
       "         3.36333982e-01, -3.78867686e-01,  2.23727471e+00, -7.60949395e-02,\n",
       "         6.40405053e-01, -8.32734234e-01,  4.06848805e-01, -1.50128483e+00,\n",
       "         1.77143307e-01,  7.42805583e-01, -7.87755495e-03, -3.05146432e-01,\n",
       "         2.84838594e+00, -1.76740151e+00, -5.80678479e-02, -3.68048057e-01,\n",
       "        -8.68350409e-01,  1.40874269e+00, -1.59226324e-01,  2.34763465e-01,\n",
       "         5.41895266e-01,  1.84584692e+00, -6.27038799e-01,  1.64381301e-01,\n",
       "        -7.56017936e-01, -7.79375155e-01, -5.89258037e-01, -9.24571288e-01,\n",
       "         1.77143307e-01, -1.02075551e+00, -6.07448802e-01, -1.95304182e+00,\n",
       "        -1.00440691e+00, -1.84479819e+00, -9.14861733e-01,  6.26505549e-02,\n",
       "         2.77624864e+00, -2.13660773e-01,  1.66607122e-01, -2.34413552e-01,\n",
       "         1.12377626e+00,  1.14627927e+00,  4.81857478e-01, -5.59947046e-01,\n",
       "        -5.28166812e-01, -1.46917548e-01,  4.11536452e-01,  7.61589740e-01,\n",
       "         8.30797002e-01,  2.18146150e-01,  8.23486684e-01, -5.18629600e-01,\n",
       "        -3.12566006e-01,  2.65041519e-01,  1.08455414e+00,  6.15715585e-01,\n",
       "        -2.30938045e-01, -3.55406475e-01,  1.09531373e+00, -7.63575717e-01,\n",
       "         1.54035901e+00,  1.49400625e+00,  9.37685158e-01, -8.64434872e-01,\n",
       "        -9.53741434e-01,  7.17552294e-01,  1.24239871e+00, -3.01748794e-01,\n",
       "        -1.50040239e-01,  4.27338189e-01, -4.93416355e-01, -3.76597849e-01,\n",
       "         7.46186338e-01, -6.78656742e-01,  2.10878352e+00, -2.87131090e+00,\n",
       "        -5.22644628e-01,  6.15117289e-01, -7.60949395e-02, -9.19916933e-02,\n",
       "         8.55089749e-01, -1.42130410e-01,  1.01696059e+00, -1.17293136e+00,\n",
       "        -1.88372779e-01, -2.22267779e+00, -1.28366782e+00, -5.69743343e-01,\n",
       "        -3.61012657e-01, -1.69424835e+00,  9.69508141e-01,  7.17552294e-01,\n",
       "        -6.79392881e-01, -1.03697549e+00, -6.96212934e-01, -4.29228101e-01,\n",
       "         1.24559129e-01, -1.50580944e+00,  1.74128482e-01,  1.56984575e+00,\n",
       "         1.58254535e+00, -4.71836707e-01, -4.55929949e-01, -1.08358030e+00,\n",
       "        -6.75255417e-01,  9.25117154e-01, -3.64637390e-01,  6.50897179e-01,\n",
       "         3.36333982e-01,  4.00896697e-01, -1.03109742e+00,  6.32053546e-01,\n",
       "        -4.24094598e-01,  1.54361075e+00,  2.74314889e-01, -5.44135855e-01,\n",
       "        -1.28366782e+00, -5.91904816e-01, -1.65799044e+00, -4.70799729e-01,\n",
       "         3.69204784e-01,  3.33883334e-01, -3.52009918e-01, -1.04278260e+00,\n",
       "         3.92390564e-01, -6.98422277e-01,  8.73212451e-01,  7.67638190e-01,\n",
       "        -4.25590745e-01,  7.93542971e-01,  1.92636827e-01,  6.06977627e-01,\n",
       "        -2.20574312e+00,  1.10526497e-02,  1.92652231e+00, -1.66966528e+00,\n",
       "         1.14374228e+00, -1.13763611e+00,  1.23185062e+00,  4.47772600e-01,\n",
       "        -1.50128483e+00,  1.45930822e+00, -3.27773579e-01, -4.75204412e-01,\n",
       "        -5.62185847e-02,  1.00528962e+00,  3.66391899e-01, -3.60748722e-01,\n",
       "         1.24559129e-01,  1.15026747e-01, -2.23274144e+00, -6.31466233e-01,\n",
       "         1.63874668e+00, -5.20941289e-01, -1.42130410e-01, -1.18232739e+00,\n",
       "        -1.18845483e+00,  1.87698020e-02,  1.37735847e+00, -1.10647133e-01,\n",
       "        -9.54094589e-01,  3.64851590e-03,  1.30542604e+00,  3.11458028e-01,\n",
       "         1.13038126e+00,  1.92620133e+00,  1.07445652e-02,  1.19916184e+00,\n",
       "         3.60320327e-03,  3.95721667e-01, -1.89969993e+00,  1.12436108e+00,\n",
       "         5.18296233e-01, -2.53801608e+00,  1.23447871e+00,  4.09454759e-01,\n",
       "         1.92652231e+00,  4.70057339e-01,  1.54361075e+00,  8.23486684e-01,\n",
       "         1.07445652e-02,  7.28787000e-01, -9.54573230e-01, -8.62627702e-01,\n",
       "        -2.79228766e-01, -9.05246336e-01,  1.47733805e+00,  7.41009928e-01,\n",
       "         3.83535937e-01,  1.37696579e+00,  9.45147042e-01,  2.14609791e+00,\n",
       "         1.58254535e+00, -9.40103560e-01, -6.46970588e-01, -4.74996684e-01,\n",
       "        -2.20247628e-01,  1.76660357e+00,  9.92019311e-01,  5.77184918e-01,\n",
       "         1.18522354e+00, -7.82145185e-01,  5.10062038e-01, -1.22949382e+00,\n",
       "        -1.19377276e-01,  4.99950600e-01,  5.56335508e-01, -1.18845483e+00,\n",
       "        -2.44660050e+00,  4.11536452e-01,  1.12436108e+00, -1.14436666e+00,\n",
       "        -4.70799729e-01,  9.37461336e-01, -1.53865376e+00,  1.62208945e-01,\n",
       "         8.93820827e-01, -5.09144377e-01,  1.33757359e+00,  4.70678094e-01,\n",
       "         1.38876979e+00, -3.61368919e-01, -3.07504003e-01, -1.11148653e+00,\n",
       "         1.65835756e+00,  7.00686170e-01, -1.34243165e+00,  1.64381301e-01,\n",
       "         5.95866687e-01,  2.22176333e+00, -1.10392529e+00, -4.22038824e-01,\n",
       "         7.86991432e-02, -9.24571288e-01,  8.36761862e-01,  1.67376859e+00,\n",
       "        -7.24765093e-01, -1.65799044e+00,  6.55798361e-01,  2.10878352e+00,\n",
       "        -9.29221256e-01, -9.14861733e-01,  3.05534297e-01, -2.00458668e-01,\n",
       "         1.23447871e+00,  3.07635224e-01,  5.17937696e-02,  6.32053546e-01,\n",
       "        -2.13660773e-01, -2.13154624e-01,  4.97040921e-01,  9.08546428e-02,\n",
       "        -9.31997898e-01, -7.02295002e-01, -3.11396507e-02, -1.89938881e-01,\n",
       "        -2.42695053e-01, -3.53352572e-01,  5.95866687e-01,  6.09761740e-02,\n",
       "         1.65835756e+00,  6.65255268e-01, -2.57120076e-01, -1.79923001e+00,\n",
       "         3.41853687e-01, -9.22379333e-01, -2.25537813e+00, -1.44865930e-01,\n",
       "         7.75894523e-01,  1.14627927e+00, -6.66483470e-01, -1.31411872e+00,\n",
       "        -8.29857337e-01, -9.74156956e-01, -6.25641914e-01, -9.68279828e-01,\n",
       "        -6.11313781e-01,  2.03201113e-01, -1.07224766e+00, -7.45264826e-01,\n",
       "         1.83068925e+00, -9.22379333e-01, -1.15837560e-01,  1.55502313e+00,\n",
       "        -1.24942865e+00,  1.45930822e+00, -2.54706669e-01,  4.70152473e-01,\n",
       "        -6.71428336e-01,  8.25490874e-01,  7.33718738e-01,  2.43873512e+00,\n",
       "        -1.40561510e+00, -5.95331737e-03,  9.38622349e-01,  3.79633430e-01,\n",
       "        -4.09401539e-01,  4.07220599e+00,  1.00159460e+00,  9.38622349e-01,\n",
       "         3.55158978e-01,  1.27417210e+00, -4.25590745e-01,  2.00835813e+00,\n",
       "         1.13298894e+00, -1.79446405e+00, -4.73656830e-01, -1.92032297e+00,\n",
       "         1.66789417e+00,  8.14079259e-01, -8.33978134e-01,  1.30542604e+00,\n",
       "         9.37685158e-01, -9.87182928e-01,  2.28836729e+00,  1.18751227e+00,\n",
       "         1.54625116e+00,  3.16897693e-01,  6.11192268e-01, -1.17795249e-01,\n",
       "        -5.69743343e-01, -9.62528868e-01, -4.22038824e-01, -4.98479314e-01,\n",
       "        -6.39636127e-01,  1.51103453e+00, -1.09000197e+00,  9.56043012e-01,\n",
       "         1.19159456e-01,  6.31860010e-01, -1.02215935e+00, -5.07143032e-01,\n",
       "        -2.20247628e-01, -1.50040239e-01,  1.45930822e+00,  1.12149610e+00,\n",
       "         1.49400625e+00,  5.17937696e-02, -8.88886612e-01, -1.02798328e+00,\n",
       "         1.56984575e+00, -1.83833199e-01,  1.00528962e+00, -4.35098851e-02,\n",
       "         1.65835756e+00,  1.05648381e+00,  1.21342421e+00, -3.60748722e-01,\n",
       "        -1.59226324e-01, -1.03109742e+00,  4.12353865e-01,  1.12780919e+00,\n",
       "         3.95721667e-01,  1.11022662e+00, -8.27114626e-02, -1.68604741e-02,\n",
       "        -9.29221256e-01, -1.83833199e-01, -6.90969823e-01, -9.40103560e-01,\n",
       "        -1.12949834e-01, -4.75204412e-01,  8.94873625e-01,  1.47259493e-01,\n",
       "        -2.18851681e+00,  7.42090270e-01,  2.14609791e+00,  1.58254535e+00,\n",
       "        -1.53865376e+00, -1.48694393e+00,  5.52378131e-01,  2.84838594e+00,\n",
       "        -4.81386534e-01,  1.53463115e-01, -6.52688659e-01,  1.09531373e+00,\n",
       "         5.45839883e-01, -8.19590685e-01, -5.30888818e-02, -1.82031815e+00,\n",
       "         6.09761740e-02,  3.53900284e-01,  1.65206766e-02, -2.47990573e+00,\n",
       "        -3.46498550e-01,  9.58376348e-01,  1.30542604e+00,  1.14239082e-01,\n",
       "         9.38622349e-01, -1.94892037e-01,  4.00636545e-01, -3.06226147e-01,\n",
       "         3.99853261e-01, -5.89258037e-01, -7.60618560e-02, -1.00440691e+00,\n",
       "         3.77211847e-02, -2.51731544e-01, -5.03537153e-01,  1.47733805e+00,\n",
       "        -3.12442316e+00,  9.82877011e-01, -3.61368919e-01, -1.04708498e+00,\n",
       "        -7.08943509e-01,  2.77140685e-01, -5.18629600e-01, -1.20858300e+00,\n",
       "         1.61357585e-01, -1.37900489e+00, -1.10106356e+00,  1.16634152e+00,\n",
       "        -5.37700636e-01,  2.12672171e+00, -2.37695499e-01,  2.43873512e+00,\n",
       "        -1.64570358e+00,  5.45839883e-01, -1.30227506e+00, -6.07448802e-01,\n",
       "        -3.05146432e-01,  1.51002355e+00,  9.58376348e-01, -2.54096469e-01,\n",
       "         1.23739701e+00,  2.54433400e-01,  1.81070800e-01,  6.83405770e-01,\n",
       "         1.02050072e+00,  3.16897693e-01,  1.39340293e+00,  2.90731093e-02,\n",
       "        -8.38490673e-01,  3.40872930e-02, -1.04087640e+00,  2.34763465e-01,\n",
       "         8.98330320e-01, -2.30022634e-01, -1.45779521e+00, -2.41727574e+00,\n",
       "        -2.79290378e-01,  2.34763465e-01,  1.38657475e+00,  1.29769736e+00,\n",
       "         3.60320327e-03,  8.41861899e-01, -1.42340942e-01,  1.22444959e+00,\n",
       "         4.18747906e-01,  9.58376348e-01,  5.98029249e-01,  3.40539111e-01,\n",
       "        -2.25011410e-01, -8.92397214e-01,  4.68521937e-01, -1.14036736e+00,\n",
       "         6.24699990e-02, -1.00548613e+00, -5.46669751e-01,  2.69797170e-01,\n",
       "        -4.29304759e-01,  1.30542604e+00,  1.76513762e-01,  5.04466429e-01,\n",
       "        -2.85004656e-02,  1.22413964e+00,  1.12436108e+00,  5.71991861e-01,\n",
       "        -1.90468153e-01,  9.53346250e-01,  1.28476968e+00, -1.28991332e-01,\n",
       "         3.32365588e-02, -1.00676095e+00, -1.45881859e+00, -1.18376306e+00]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance[4], permutations[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc2074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       265\n",
      "         1.0       0.99      0.99      0.99       235\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "Model weights: [4.66025499 4.36957995 4.85593973 4.15542653 3.89963206]\n"
     ]
    }
   ],
   "source": [
    "#logit\n",
    "if lr:\n",
    "    space = {\"fit_intercept\": [True, False],\n",
    "             \"penalty\": ['l2', 'elasticnet', 'none'],\n",
    "             \"max_iter\": [random.uniform(5,200) for i in range (20)],\n",
    "             \"tol\": np.logspace(-4, 4, 50)}\n",
    "    estimator = LogisticRegression(random_state = random_state)\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_lr = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_lr.predict(X_test)))\n",
    "    print(\"Model weights:\", cls_lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b40efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.9999999999999999\n",
      "Global explanation fidelity: 0.9999999999999999\n",
      "Global data-explanation fidelity: 0.9999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.9999999999999998\n",
      "Local explanation fidelity: 0.15\n",
      "Local data-explanation fidelity: 0.3670666666666666\n"
     ]
    }
   ],
   "source": [
    "if lr:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    exp_global = []\n",
    "\n",
    "    \n",
    "    #print(\"Weights\", cls_lr.coef_[0])\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(weights, cls_lr.coef_[0], variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_lr, instance, \"logit\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(weights, tr, variant=\"b\").statistic)\n",
    "        \n",
    "        pred = cls_lr.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_lr.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_lr.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "        \n",
    "        exp_global.append(change)\n",
    "        mx_corr.append(ss.kendalltau(np.abs(tr), change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.abs(cls_lr.coef_[0]), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.abs(cls_lr.coef_[0]), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.mean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "217e4e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97       265\n",
      "         1.0       0.95      1.00      0.97       235\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.97      0.98      0.97       500\n",
      "weighted avg       0.97      0.97      0.97       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "    space = {'var_smoothing': np.logspace(0, -9, 100)}\n",
    "    estimator = GaussianNB()\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_nb = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "452e6fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.7999999999999999\n",
      "Global explanation fidelity: 0.9999999999999999\n",
      "Global data-explanation fidelity: 0.7999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.08519999999999998\n",
      "Local explanation fidelity: 0.6876\n",
      "Local data-explanation fidelity: 0.4145333333333333\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "#     nb_global = [ss.kstest(np.random.normal(cls_nb.theta_[0][ind], np.sqrt(cls_nb.var_)[0][ind],1000), \n",
    "#           np.random.normal(cls_nb.theta_[1][ind], np.sqrt(cls_nb.var_)[1][ind],1000)).statistic for ind in range(len(weights))]\n",
    "\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    nb_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "    #print(\"Weights\", nb_global)\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), nb_global, variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_nb, instance, \"nb\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        nb_global.append(tr)\n",
    "        \n",
    "        pred = cls_nb.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_nb.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_nb.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "        \n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(nb_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(nb_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.mean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db52f2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 94.29819103,  86.56189099, 111.20045041,  90.18544013,\n",
       "        83.49338862])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(exp_global, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6eec88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55, 0.503, 0.551, 0.49, 0.459]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee836feb",
   "metadata": {},
   "source": [
    "# Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc636f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cat_data[cat_data.columns.drop(\"X1\")]\n",
    "y = cat_data[\"X1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44b77359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       266\n",
      "         1.0       1.00      1.00      1.00       234\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n",
      "Max depth: 5 \t Num splits: 19 \t Num leaves: 20\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "if dt:\n",
    "    space = {\"splitter\": [\"best\", \"random\"],\n",
    "             \"min_samples_split\": [random.uniform(0, 1) for i in range (50)],\n",
    "             \"max_features\": [random.uniform(0,1) for i in range (50)],\n",
    "             \"criterion\":[\"gini\"]}#, \"entropy\"]}\n",
    "    estimator = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_df = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls.predict(X_test)))\n",
    "    \n",
    "    split_nodes = []\n",
    "    leaf_nodes = []\n",
    "\n",
    "    depths, split_nodes, leaf_nodes = get_tree_characteristics(cls_df.tree_, X_train.columns, split_nodes = split_nodes, leaf_nodes = leaf_nodes)\n",
    "\n",
    "    max_depth = cls_df.get_depth()\n",
    "    splits = len(split_nodes)\n",
    "    leaves = len(leaf_nodes)\n",
    "\n",
    "    print(\"Max depth: %s \\t Num splits: %s \\t Num leaves: %s\" %(max_depth, splits, leaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c6ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.0\n",
      "Global explanation fidelity: -0.19999999999999998\n",
      "Global data-explanation fidelity: 0.0\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: -0.06888156617270719\n",
      "Local explanation fidelity: -0.003911099180066295\n",
      "Local data-explanation fidelity: 0.2358947510377595\n"
     ]
    }
   ],
   "source": [
    "if dt:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    dt_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "#     print(\"Weights\", cls_df.feature_importances_)\n",
    "#     print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), cPerfls_df.feature_importances_.reshape(1, -1), variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_df, instance, \"decision_tree\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        \n",
    "        dt_global.append(tr)\n",
    "        \n",
    "        pred = cls_df.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_df.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_df.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(dt_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(dt_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00e11692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       266\n",
      "         1.0       1.00      1.00      1.00       234\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n",
      "Model weights: [6.05633197 6.21009034 6.06533825 6.10242977 6.0974812 ]\n"
     ]
    }
   ],
   "source": [
    "#logit\n",
    "if lr:\n",
    "    space = {\"fit_intercept\": [True, False],\n",
    "             \"penalty\": ['l2', 'elasticnet', 'none'],\n",
    "             \"max_iter\": [random.uniform(5,200) for i in range (20)],\n",
    "             \"tol\": np.logspace(-4, 4, 50)}\n",
    "    estimator = LogisticRegression(random_state = random_state)\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_lr = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_lr.predict(X_test)))\n",
    "    print(\"Model weights:\", cls_lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7985df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: -0.19999999999999998\n",
      "Global explanation fidelity: 0.0\n",
      "Global data-explanation fidelity: 0.0\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: -0.2\n",
      "Local explanation fidelity: 0.055999999999999994\n",
      "Local data-explanation fidelity: 0.3118666666666667\n"
     ]
    }
   ],
   "source": [
    "if lr:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    exp_global = []\n",
    "\n",
    "    \n",
    "    #print(\"Weights\", cls_lr.coef_[0])\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(weights, cls_lr.coef_[0], variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_lr, instance, \"logit\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(weights, tr, variant=\"b\").statistic)\n",
    "        \n",
    "        pred = cls_lr.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_lr.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_lr.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "        \n",
    "        exp_global.append(change)\n",
    "        mx_corr.append(ss.kendalltau(np.abs(tr), change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.abs(cls_lr.coef_[0]), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.abs(cls_lr.coef_[0]), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928ec5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       266\n",
      "         1.0       1.00      1.00      1.00       234\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "    space = {'var_smoothing': np.logspace(0, -9, 100)}\n",
    "    estimator = GaussianNB()\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_nb = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0530080c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: -0.39999999999999997\n",
      "Global explanation fidelity: 0.9999999999999999\n",
      "Global data-explanation fidelity: -0.39999999999999997\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: -0.4612\n",
      "Local explanation fidelity: 0.808\n",
      "Local data-explanation fidelity: 0.05234999999999997\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "#     nb_global = [ss.kstest(np.random.normal(cls_nb.theta_[0][ind], np.sqrt(cls_nb.var_)[0][ind],1000), \n",
    "#           np.random.normal(cls_nb.theta_[1][ind], np.sqrt(cls_nb.var_)[1][ind],1000)).statistic for ind in range(len(weights))]\n",
    "\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    nb_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "    #print(\"Weights\", nb_global)\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), nb_global, variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_nb, instance, \"nb\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        nb_global.append(tr)\n",
    "        \n",
    "        pred = cls_nb.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_nb.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_nb.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "        \n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(nb_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(nb_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388f9b5",
   "metadata": {},
   "source": [
    "# Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ef88e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cont_data[cont_data.columns.drop(\"X1\")]\n",
    "y = cont_data[\"X1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edfc547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.84      0.80       227\n",
      "         1.0       0.86      0.79      0.82       273\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.81      0.82      0.81       500\n",
      "weighted avg       0.82      0.81      0.81       500\n",
      "\n",
      "Max depth: 12 \t Num splits: 126 \t Num leaves: 127\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "if dt:\n",
    "    space = {\"splitter\": [\"best\", \"random\"],\n",
    "             \"min_samples_split\": [random.uniform(0, 1) for i in range (50)],\n",
    "             \"max_features\": [random.uniform(0,1) for i in range (50)],\n",
    "             \"criterion\":[\"gini\"]}#, \"entropy\"]}\n",
    "    estimator = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_df = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls.predict(X_test)))\n",
    "    \n",
    "    split_nodes = []\n",
    "    leaf_nodes = []\n",
    "\n",
    "    depths, split_nodes, leaf_nodes = get_tree_characteristics(cls_df.tree_, X_train.columns, split_nodes = split_nodes, leaf_nodes = leaf_nodes)\n",
    "\n",
    "    max_depth = cls_df.get_depth()\n",
    "    splits = len(split_nodes)\n",
    "    leaves = len(leaf_nodes)\n",
    "\n",
    "    print(\"Max depth: %s \\t Num splits: %s \\t Num leaves: %s\" %(max_depth, splits, leaves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af42075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.9999999999999999\n",
      "Global explanation fidelity: 0.7999999999999999\n",
      "Global data-explanation fidelity: 0.7999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.5696294231491453\n",
      "Local explanation fidelity: 0.13718746148496735\n",
      "Local data-explanation fidelity: 0.3490408861908746\n"
     ]
    }
   ],
   "source": [
    "if dt:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    dt_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "#     print(\"Weights\", cls_df.feature_importances_)\n",
    "#     print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), cls_df.feature_importances_.reshape(1, -1), variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_df, instance, \"decision_tree\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        \n",
    "        dt_global.append(tr)\n",
    "        \n",
    "        pred = cls_df.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_df.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_df.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(dt_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(dt_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3c156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       227\n",
      "         1.0       1.00      0.99      0.99       273\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.99      0.99      0.99       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n",
      "Model weights: [4.76514275 4.54992483 4.89709404 4.14464018 3.95259274]\n"
     ]
    }
   ],
   "source": [
    "#logit\n",
    "if lr:\n",
    "    space = {\"fit_intercept\": [True, False],\n",
    "             \"penalty\": ['l2', 'elasticnet', 'none'],\n",
    "             \"max_iter\": [random.uniform(5,200) for i in range (20)],\n",
    "             \"tol\": np.logspace(-4, 4, 50)}\n",
    "    estimator = LogisticRegression(random_state = random_state)\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_lr = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_lr.predict(X_test)))\n",
    "    print(\"Model weights:\", cls_lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d0b28e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.9999999999999999\n",
      "Global explanation fidelity: 0.9999999999999999\n",
      "Global data-explanation fidelity: 0.9999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.9999999999999998\n",
      "Local explanation fidelity: 0.1184\n",
      "Local data-explanation fidelity: 0.3568666666666666\n"
     ]
    }
   ],
   "source": [
    "if lr:\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    exp_global = []\n",
    "\n",
    "    \n",
    "    #print(\"Weights\", cls_lr.coef_[0])\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(weights, cls_lr.coef_[0], variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_lr, instance, \"logit\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(weights, tr, variant=\"b\").statistic)\n",
    "        \n",
    "        pred = cls_lr.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_lr.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_lr.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "        \n",
    "        exp_global.append(change)\n",
    "        mx_corr.append(ss.kendalltau(np.abs(tr), change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "\n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.abs(cls_lr.coef_[0]), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.abs(cls_lr.coef_[0]), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ecf5daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97       227\n",
      "         1.0       1.00      0.95      0.97       273\n",
      "\n",
      "    accuracy                           0.97       500\n",
      "   macro avg       0.97      0.97      0.97       500\n",
      "weighted avg       0.97      0.97      0.97       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "    space = {'var_smoothing': np.logspace(0, -9, 100)}\n",
    "    estimator = GaussianNB()\n",
    "    cls = GridSearchCV(estimator, space, verbose = 0)\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    cls_nb = cls.best_estimator_\n",
    "    print(classification_report(y_test, cls_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b98ef069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model correctness: 0.7999999999999999\n",
      "Global explanation fidelity: 0.9999999999999999\n",
      "Global data-explanation fidelity: 0.7999999999999999\n",
      "---------------------------------------------------------------\n",
      "Local model correctness: 0.12279999999999999\n",
      "Local explanation fidelity: 0.6915999999999999\n",
      "Local data-explanation fidelity: 0.4256\n"
     ]
    }
   ],
   "source": [
    "if nb:\n",
    "#     nb_global = [ss.kstest(np.random.normal(cls_nb.theta_[0][ind], np.sqrt(cls_nb.var_)[0][ind],1000), \n",
    "#           np.random.normal(cls_nb.theta_[1][ind], np.sqrt(cls_nb.var_)[1][ind],1000)).statistic for ind in range(len(weights))]\n",
    "\n",
    "    dm_corr = []\n",
    "    mx_corr = []\n",
    "    dx_corr = []\n",
    "    \n",
    "    nb_global = []\n",
    "    exp_global = []\n",
    "    \n",
    "    #print(\"Weights\", nb_global)\n",
    "    #print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), nb_global, variant=\"b\").statistic)\n",
    "\n",
    "    for instance in X_test.values:\n",
    "        tr = get_true_rankings(cls_nb, instance, \"nb\", X_train, X_train.columns)\n",
    "        dm_corr.append(ss.kendalltau(np.abs(weights), tr, variant=\"b\").statistic)\n",
    "        nb_global.append(tr)\n",
    "        \n",
    "        pred = cls_nb.predict(instance.reshape(1, -1)).astype(int)\n",
    "        p1_list = [cls_nb.predict_proba(instance.reshape(1, -1)).reshape(2)[pred]]*1000\n",
    "        change = []\n",
    "        for i in range(len(instance)):\n",
    "            permutations = permute_instance(instance, [i], 1000, [X_train.iloc[:, i].min()], [X_train.iloc[:, i].max()],\n",
    "                                           [X_train.iloc[:, i].mean()], [X_train.iloc[:,i].unique()], mode=\"permutation\")\n",
    "            p2_list = cls_nb.predict_proba(permutations).transpose()[pred].reshape(1000)\n",
    "            change.append(mean_absolute_percentage_error(p1_list, p2_list))\n",
    "            \n",
    "        mx_corr.append(ss.kendalltau(tr, change, variant=\"b\").statistic)\n",
    "        dx_corr.append(ss.kendalltau(np.abs(weights), change, variant=\"b\"))\n",
    "        exp_global.append(change)\n",
    "        \n",
    "    print(\"Global model correctness:\", ss.kendalltau(np.abs(weights), np.nanmean(nb_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global explanation fidelity:\", ss.kendalltau(np.nanmean(nb_global, axis=0), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    print(\"Global data-explanation fidelity:\", ss.kendalltau(np.abs(weights), np.nanmean(exp_global, axis=0), variant=\"b\").statistic)\n",
    "    \n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"Local model correctness:\", np.nanmean(dm_corr))\n",
    "    print(\"Local explanation fidelity:\", np.nanmean(mx_corr))\n",
    "    print(\"Local data-explanation fidelity:\", np.nanmean(dx_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ac56e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " -0.6,\n",
       " -0.39999999999999997,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.9999999999999999,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.9999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.7999999999999999,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " -0.7999999999999999,\n",
       " -0.7999999999999999,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " -0.7999999999999999,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " -0.6,\n",
       " -0.39999999999999997,\n",
       " 0.9999999999999999,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.9999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.6,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.9999999999999999,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " -0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7999999999999999,\n",
       " 0.9999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " -0.7999999999999999,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " -0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " -0.6,\n",
       " 0.6,\n",
       " 0.7999999999999999,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.6,\n",
       " 0.6,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.7999999999999999,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.7999999999999999,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " 0.7999999999999999,\n",
       " -0.19999999999999998,\n",
       " -0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.6,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.6,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " -0.7999999999999999,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " -0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " -0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " -0.6,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.7999999999999999,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.6,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.6,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " -0.6,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.9999999999999999,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " 0.9999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.6,\n",
       " 0.6,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " -0.7999999999999999,\n",
       " -0.6,\n",
       " 0.6,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " 0.6,\n",
       " 0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " -0.39999999999999997,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.19999999999999998,\n",
       " 0.7999999999999999,\n",
       " 0.6,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " -0.19999999999999998,\n",
       " 0.0,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " -0.6,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " -0.6,\n",
       " -0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.0,\n",
       " 0.9999999999999999,\n",
       " 0.7999999999999999,\n",
       " 0.19999999999999998,\n",
       " 0.39999999999999997,\n",
       " 0.39999999999999997,\n",
       " -0.39999999999999997,\n",
       " -0.19999999999999998,\n",
       " 0.19999999999999998,\n",
       " 0.0,\n",
       " -0.7999999999999999,\n",
       " -0.7999999999999999,\n",
       " 0.39999999999999997,\n",
       " 0.19999999999999998,\n",
       " 0.6,\n",
       " -0.6,\n",
       " 0.19999999999999998]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
