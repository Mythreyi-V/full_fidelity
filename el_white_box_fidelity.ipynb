{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 7972,
     "status": "error",
     "timestamp": 1651541188913,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "JYtoSYnbd5nH",
    "outputId": "94a1ff35-bde1-44bb-849f-4b33780d8411"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# import os\n",
    "\n",
    "# os.chdir('drive/MyDrive/three-phase-fidelity-wip')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1651541188912,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "t0_5SLqLeHZZ"
   },
   "outputs": [],
   "source": [
    "# # # # !pip install -r requirements.txt\n",
    "# !pip install ipython==7.31.1\n",
    "# !pip install importlib-metadata==4.10.0\n",
    "# !pip install acv-exp\n",
    "# !pip install hyperopt==0.27\n",
    "# # # ## !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1651541188913,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NyAaFrBnYtpf"
   },
   "outputs": [],
   "source": [
    "# !pip install lime\n",
    "# !pip install shap\n",
    "# !pip install anchor-exp\n",
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-wJR-jq3gE6Z"
   },
   "outputs": [],
   "source": [
    "# !pip install --requirement requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1647493915201,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "PkN3Je0napuy",
    "outputId": "d7b21da4-386d-4a28-9c11-3f6fc1e17a06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statistics\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "from learning import *\n",
    "import pyAgrum\n",
    "# from acv_explainers import ACXplainer\n",
    "\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K24AGuxDNzfw"
   },
   "outputs": [],
   "source": [
    "# !pip install acv-exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JUIxPEXyapu7"
   },
   "outputs": [],
   "source": [
    "def get_reg_features(cls, percentile):\n",
    "\n",
    "    og_coef = cls.coef_\n",
    "    if len(og_coef.shape) > 1:\n",
    "        og_coef = og_coef[0]\n",
    "    \n",
    "    coef = [abs(val) for val in og_coef]\n",
    "    \n",
    "    min_coef = min(coef)\n",
    "    max_coef = max(coef)\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "    \n",
    "    feat_pos = [i for i in range(len(coef)) if coef[i] >= q1_min]\n",
    "    \n",
    "    return coef, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1THTivAQapu-"
   },
   "outputs": [],
   "source": [
    "def get_nb_features(cls, instance, percentile):\n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    means = cls.theta_[pred][0]\n",
    "    std = np.sqrt(cls.var_[pred])[0]\n",
    "\n",
    "    alt = 1-pred\n",
    "    alt_means = cls.theta_[alt][0]\n",
    "    alt_std = np.sqrt(cls.var_[alt])[0]\n",
    "    \n",
    "    likelihoods = []\n",
    "    \n",
    "#     for i in range(len(means)):\n",
    "#         lkhood = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "#         #likelihoods.append(abs(lkhood))\n",
    "#         likelihoods.append(lkhood)\n",
    "\n",
    "    for i in range(len(means)):\n",
    "        lk = scipy.stats.norm(means[i], std[i]).logpdf(instance[i])\n",
    "        alt_lk = scipy.stats.norm(alt_means[i], alt_std[i]).logpdf(instance[i])\n",
    "        lkhood = abs(lk-alt_lk)\n",
    "        likelihoods.append(lkhood)\n",
    "    \n",
    "    min_coef = min(likelihoods)\n",
    "    max_coef = max(likelihoods)\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "    \n",
    "#     bins = pd.cut(likelihoods, 10, retbins = True, duplicates = \"drop\")[1]\n",
    "#     lim_1 = bins[-2]\n",
    "#     lim_2 = bins[1]\n",
    "    \n",
    "#     sortedls = sorted(likelihoods, reverse=True)\n",
    "#     pos = math.ceil(len(likelihoods)/4)\n",
    "#     lim = likelihoods[pos]\n",
    "    \n",
    "    feat_pos = [i for i in range(len(likelihoods)) if likelihoods[i] >= q1_min]# or likelihoods[i] <= lim_2]\n",
    "    \n",
    "    return likelihoods, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OpYOJjtVapu_"
   },
   "outputs": [],
   "source": [
    "def get_tree_features(cls, instance):\n",
    "    tree = cls.tree_\n",
    "    lvl = 0\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "\n",
    "    feats = []\n",
    "    \n",
    "    while left_child != sklearn.tree._tree.TREE_LEAF and right_child != sklearn.tree._tree.TREE_LEAF:\n",
    "        feature = tree.feature[lvl]\n",
    "        feats.append(feature)\n",
    "        \n",
    "        if instance[feature] < tree.threshold[lvl]:\n",
    "            lvl = left_child\n",
    "        else:\n",
    "            lvl = right_child\n",
    "            \n",
    "        left_child = tree.children_left[lvl]\n",
    "        right_child = tree.children_right[lvl]\n",
    "            \n",
    "            \n",
    "    feat_pos = set(feats)\n",
    "    \n",
    "    score = np.zeros(len(instance))\n",
    "    n = len(feats)\n",
    "    for i in feats:\n",
    "        score[i]+=n\n",
    "        n=n-1\n",
    "    \n",
    "    return score, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DPtMAbfwapvA"
   },
   "outputs": [],
   "source": [
    "def get_path_depths(tree, feat_list, cur_depth = 0, lvl = 0, depths = []):\n",
    "\n",
    "    left_child = tree.children_left[lvl]\n",
    "    right_child = tree.children_right[lvl]\n",
    "    \n",
    "    if left_child == sklearn.tree._tree.TREE_LEAF:\n",
    "        depths.append(cur_depth)\n",
    "        \n",
    "    else:\n",
    "        depths = get_path_depths(tree, feat_list, cur_depth+1, left_child, depths)\n",
    "        depths = get_path_depths(tree, feat_list, cur_depth+1, right_child, depths)\n",
    "    return depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WB7Wr9gTapvB"
   },
   "outputs": [],
   "source": [
    "def get_shap_features(explainer, instance, cls, classification, exp_iter, feat_list, percentile):\n",
    "    \n",
    "    shap_exp = []\n",
    "    \n",
    "    pred = cls.predict(instance.reshape(1, -1))\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        if type(explainer) == shap.explainers._tree.Tree:\n",
    "            exp = explainer(instance, check_additivity = False).values\n",
    "        else:\n",
    "            exp = explainer(instance.reshape(1, -1)).values\n",
    "        \n",
    "        #print(exp.shape)\n",
    "        #print(exp)\n",
    "        \n",
    "        if exp.shape == (1, len(feat_list), 2):\n",
    "            exp = exp[0]\n",
    "            \n",
    "        #print(exp.shape)\n",
    "        \n",
    "        if exp.shape == (len(feat_list), 2):\n",
    "            exp = np.array([feat[pred] for feat in exp]).reshape(len(feat_list))\n",
    "        elif exp.shape == (1, len(feat_list)) or exp.shape == (len(feat_list), 1):\n",
    "            exp = exp.reshape(len(feat_list))\n",
    "            \n",
    "        #print(np.array(exp).shape)\n",
    "            \n",
    "        shap_exp.append(exp)\n",
    "        \n",
    "    #print(np.array(shap_exp).shape)\n",
    "        \n",
    "    if np.array(shap_exp).shape != (exp_iter, len(feat_list)):\n",
    "        raise Exception(\"Explanation shape is not correct. It is\", np.array(shap_exp).shape, \"instead of the expected\", (exp_iter, len(feat_list)))\n",
    "    \n",
    "#     if classification==True, type(explainer) == shap.explainers._tree.Tree:\n",
    "#         shap_exp = []\n",
    "#         for each in full_exp:\n",
    "#             single_exp = [feat[0] for feat in each]\n",
    "#             shap_exp.append(single_exp)\n",
    "#     else:\n",
    "#         shap_exp = full_exp\n",
    "        \n",
    "    avg_val = np.average(shap_exp, axis = 0)\n",
    "    abs_val = [abs(val) for val in avg_val]\n",
    "    \n",
    "    #Get recall and precision for the average of shap values\n",
    "    min_coef = min(abs_val)\n",
    "    max_coef = max(abs_val)\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "\n",
    "    sorted_val = np.copy(abs_val)\n",
    "    sorted_val.sort()\n",
    "    \n",
    "    shap_features = set([i for i in range(len(feat_list)) if abs_val[i] > q1_min])\n",
    "    \n",
    "    return abs_val, shap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sXu0zuNrapvD"
   },
   "outputs": [],
   "source": [
    "def get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list, percentile):\n",
    "    lime_exp = []\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        if classification==True:\n",
    "            lime_exp.extend(explainer.explain_instance(instance, cls.predict_proba, \n",
    "                                                num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "        else:\n",
    "            lime_exp.extend(explainer.explain_instance(instance, cls.predict, \n",
    "                                                num_features=len(feat_list), labels=[0,1]).as_list())\n",
    "            \n",
    "    weights = [[] for each in feat_list]\n",
    "    for exp in lime_exp:\n",
    "        feat = exp[0]\n",
    "        if '<' in feat:\n",
    "            feat = exp[0].replace(\"= \",'')\n",
    "            parts = feat.split('<')\n",
    "        elif '>' in feat:\n",
    "            feat = exp[0].replace(\"= \",'')\n",
    "            parts = feat.split('>')\n",
    "        else:\n",
    "            parts = feat.split(\"=\")\n",
    "        \n",
    "        for part in parts:\n",
    "            if part.replace('.','').replace(' ','').isdigit()==False:\n",
    "                feat_name = part.replace(' ','')\n",
    "        n = feat_list.index(feat_name)\n",
    "        weights[n].append(exp[1])\n",
    "    \n",
    "    weights = np.transpose(weights)\n",
    "    avg_weight = np.average(np.array(weights), axis = 0)\n",
    "    abs_weight = [abs(weight) for weight in avg_weight]\n",
    "    \n",
    "    min_coef = min(abs_weight)\n",
    "    max_coef = max(abs_weight)\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "    \n",
    "    sorted_weight = np.copy(abs_weight)\n",
    "    sorted_weight.sort()\n",
    "    \n",
    "    lime_features = set([i for i in range(len(feat_list)) if abs_weight[i] >= q1_min])\n",
    "    \n",
    "    return abs_weight, lime_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CsLOYeO-apvE"
   },
   "outputs": [],
   "source": [
    "def get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list, percentile):\n",
    "    label_lst = [\"Negative\", \"Positive\"]\n",
    "    \n",
    "    feat_pos = []\n",
    "    lkhoods = []\n",
    "    \n",
    "    for i in range(exp_iter):\n",
    "        [bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                                       None, scaler, cls, save_to+\"/\"+cls_method+\"/\", dataset, show_in_notebook = False)\n",
    "        \n",
    "        ie = pyAgrum.LazyPropagation(bn)\n",
    "        result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "        result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "        row = instance['original_vector']\n",
    "        #print(row)\n",
    "\n",
    "        likelihood = [0]*len(feat_list)\n",
    "\n",
    "        for j in range(len(feat_list)):\n",
    "            var_labels = bn.variable(feat_list[j]).labels()\n",
    "            str_bins = list(var_labels)\n",
    "            bins = []\n",
    "\n",
    "            for disc_bin in str_bins:\n",
    "                disc_bin = disc_bin.strip('\"(]')\n",
    "                cat = [float(val) for val in disc_bin.split(',')]\n",
    "                bins.append(cat)\n",
    "\n",
    "            for k in range(len(bins)):\n",
    "                if k == 0 and row[j] <= bins[k][0]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif k == len(bins)-1 and row[j] >= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "                elif row[j] > bins[k][0] and row[j] <= bins[k][1]:\n",
    "                    feat_bin = str_bins[k]\n",
    "\n",
    "            ie = pyAgrum.LazyPropagation(bn)\n",
    "            ie.setEvidence({feat_list[j]: feat_bin})\n",
    "            ie.makeInference()\n",
    "            \n",
    "            result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "            new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "            #print(result_proba, new_proba)\n",
    "            proba_change = result_proba-new_proba\n",
    "            likelihood[j] = abs(proba_change)\n",
    "\n",
    "        lkhoods.append(likelihood)\n",
    "        \n",
    "    min_coef = min( np.mean(lkhoods, axis=0))\n",
    "    max_coef = max( np.mean(lkhoods, axis=0))\n",
    "    \n",
    "    k = (max_coef-min_coef)*percentile\n",
    "    q1_min = max_coef - k\n",
    "\n",
    "    #If fixing all features produces the same result for the class,\n",
    "    #return all features\n",
    "    if len(set(np.mean(lkhoods, axis=0)))==1:\n",
    "        feat_pos.extend(range(len(feat_list)))\n",
    "    else:\n",
    "        feat_pos.extend(list(np.where(np.mean(lkhoods, axis=0) >= q1_min)[0]))\n",
    "\n",
    "    feat_pos = set(feat_pos)\n",
    "    \n",
    "    return np.mean(lkhoods, axis=0), feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nxqKbpTLcjNS"
   },
   "outputs": [],
   "source": [
    "def get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter):\n",
    "    instance = instance.reshape(1, -1)\n",
    "    y = cls.predict(instance)\n",
    "    \n",
    "    t=np.var(y_train)\n",
    "\n",
    "    feats = []\n",
    "    feat_imp = []\n",
    "\n",
    "    for i in range(exp_iter):\n",
    "        sufficient_expl, sdp_expl, sdp_global = explainer.sufficient_expl_rf(instance, y, X_train, y_train,\n",
    "                                                                                 t=t, pi_level=0.8)\n",
    "        clean_expl = sufficient_expl.copy()\n",
    "        clean_expl = clean_expl[0]\n",
    "        clean_expl = [sublist for sublist in clean_expl if sum(n<0 for n in sublist)==0 ]\n",
    "\n",
    "        clean_sdp = sdp_expl[0].copy()\n",
    "        clean_sdp = [sdp for sdp in clean_sdp if sdp > 0]\n",
    "        \n",
    "        lximp = explainer.compute_local_sdp(X_train.shape[1], clean_expl)\n",
    "        feat_imp.append(lximp)\n",
    "        \n",
    "        if len(clean_expl)==0 or len(clean_expl[0])==0:\n",
    "            print(\"No explamation meets pi level\")\n",
    "        else:\n",
    "            lens = [len(i) for i in clean_expl]\n",
    "            me_loc = [i for i in range(len(lens)) if lens[i]==min(lens)]\n",
    "            mse_loc = np.argmax(np.array(clean_sdp)[me_loc])\n",
    "            mse = np.array(clean_expl)[me_loc][mse_loc]\n",
    "            feats.extend(mse)\n",
    "\n",
    "    if len(feats)==0:\n",
    "        feat_pos = []\n",
    "    else:\n",
    "        feat_pos = set(feats)\n",
    "    \n",
    "      \n",
    "    feat_imp = np.mean(feat_imp, axis=0)\n",
    "    \n",
    "    return feat_imp, feat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBi4uTnFapvG"
   },
   "outputs": [],
   "source": [
    "def get_explanation_features(explainer, instance, cls, scaler, dataset, \n",
    "                             classification, exp_iter, xai_method, feat_list, X_train, y_train, percentile):\n",
    "    if xai_method == \"SHAP\":\n",
    "        exp_score, feat_pos = get_shap_features(explainer, instance, cls, classification, exp_iter, feat_list, percentile)\n",
    "        \n",
    "    elif xai_method == \"LIME\":\n",
    "        exp_score, feat_pos = get_lime_features(explainer, instance, cls, classification, exp_iter, feat_list, percentile)\n",
    "        \n",
    "    elif xai_method == \"LINDA\":\n",
    "        exp_score, feat_pos = get_linda_features(instance, cls, scaler, dataset, exp_iter, feat_list, percentile)\n",
    "\n",
    "    elif xai_method == \"ACV\":\n",
    "        exp_score, feat_pos = get_acv_features(explainer, instance, cls, X_train, y_train, exp_iter)\n",
    "        \n",
    "    explanation_features = [feat_list[i] for i in feat_pos]\n",
    "    #explanation_features = set(explanation_features)\n",
    "        \n",
    "    return exp_score, explanation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6zVKTJDapvI"
   },
   "outputs": [],
   "source": [
    "def get_true_features(cls, instance, cls_method, X_train, feat_list, percentile):\n",
    "    if cls_method == \"decision_tree\":\n",
    "        true_score, feat_pos = get_tree_features(cls, instance)\n",
    "        \n",
    "    elif cls_method == \"logit\" or cls_method == \"lin_reg\":\n",
    "        true_score, feat_pos = get_reg_features(cls, percentile)\n",
    "        \n",
    "    elif cls_method == \"nb\":\n",
    "        true_score, feat_pos = get_nb_features(cls, instance, percentile)\n",
    "        \n",
    "    true_features = [feat_list[i] for i in feat_pos]\n",
    "    true_features = set(true_features)\n",
    "    \n",
    "    #print(feat_pos)\n",
    "    \n",
    "    return true_score, true_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yN3tWnbapvJ"
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)\n",
    "\n",
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"prefix\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"decision_tree\"\n",
    "\n",
    "classification = True\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "save_to = os.path.join(PATH, dataset_ref, cls_method, method_name)\n",
    "\n",
    "xai_method = \"SHAP\"\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 5\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "random_state = 22\n",
    "percentile = 0.05\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],\n",
    "    \"production\" : [\"production\"],\n",
    "    \"bpic2011\": [\"bpic2011_f1\"],\n",
    "    \"hospital\": [\"hospital_billing_2\"],\n",
    "    \"traffic\": [\"traffic_fines_1\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fidelity(i):\n",
    "    #print(\"i:\", i)\n",
    "    instance = sample_instances[i]\n",
    "#     print(instance)\n",
    "    true_score, true_features = get_true_features(cls, instance, cls_method, trainingdata, feat_list, percentile)\n",
    "   # print(\"True Score\", true_score)\n",
    "    \n",
    "    if xai_method == \"LINDA\":\n",
    "        instance = test_dict[i]\n",
    "    \n",
    "    exp_score, explanation_features = get_explanation_features(explainer, instance, cls, scaler, dataset_ref, classification, exp_iter, xai_method, \n",
    "                                                        feat_list, trainingdata, targets, percentile)\n",
    "#     print(\"Exp Score\", exp_score)\n",
    "#     print(\"True Features: \", true_features)\n",
    "#     print(\"Explanation Features: \", explanation_features)\n",
    "    \n",
    "    if len(explanation_features) == 0:\n",
    "        recall = 0\n",
    "        precision = 0\n",
    "    else:\n",
    "        recall = len(true_features.intersection(explanation_features))/len(true_features)\n",
    "        precision = len(true_features.intersection(explanation_features))/len(explanation_features)\n",
    "        \n",
    "#     print(\"Recall:\", recall)\n",
    "#     print(\"Precision:\", precision)\n",
    "        \n",
    "    corr = scipy.stats.kendalltau(true_score, exp_score)[0]\n",
    "    \n",
    "#     print(\"Corr:\", corr)\n",
    "    #progress_bar.clear()\n",
    "    #progress_bar.update(i)\n",
    "    \n",
    "    return precision, recall, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for dataset_name in datasets:\n",
    "        \n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for bucket in tqdm(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "\n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "                \n",
    "            #Identify feature names\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "            cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "            \n",
    "            #create explanation mechanism\n",
    "            if xai_method == \"SHAP\":\n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    explainer = shap.Explainer(cls, trainingdata)\n",
    "            elif xai_method == \"LIME\":\n",
    "                explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            elif xai_method == \"ACV\":\n",
    "                explainer = joblib.load(os.path.join(PATH,'%s/%s/%s/acv_surrogate/acv_explainer_bucket_%s.joblib'% \n",
    "                                                                    (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            elif xai_method == \"LINDA\":\n",
    "                test_dict = generate_local_predictions( sample_instances, results[\"Actual\"], cls, scaler, None )\n",
    "                explainer = None\n",
    "\n",
    "            compiled_precision = []\n",
    "            compiled_recall = []\n",
    "            compiled_corr = []\n",
    "\n",
    "            #for i in tqdm_notebook(range(len(test_x))):\n",
    "            #explain the chosen instances and find the fidelity\n",
    "            pool = mp.Pool(mp.cpu_count(), initargs=(mp.RLock(),), initializer=tqdm.set_lock)\n",
    "\n",
    "            #with tqdm(total = len(test_x)) as progress_bar:\n",
    "            start = time.time()\n",
    "            #print(start)\n",
    "            #recall, precision, corr = zip(*pool.map(test_fidelity, [i for i in range(len(test_x))]))\n",
    "            for i in range(len(sample_instances)):\n",
    "                result = test_fidelity(i)\n",
    "                compiled_precision.append(result[0])\n",
    "                compiled_recall.append(result[1])\n",
    "                compiled_corr.append(result[2])\n",
    "\n",
    "            print(time.time()-start, \"seconds\")\n",
    "\n",
    "            # compiled_precision = list(precision)\n",
    "            # compiled_recall = list(recall)\n",
    "            # compiled_corr = list(corr)\n",
    "\n",
    "            compiled_corr = np.nan_to_num(compiled_corr)\n",
    "\n",
    "            results[xai_method+\" Precision\"] = compiled_precision\n",
    "            results[xai_method+\" Recall\"] = compiled_recall\n",
    "            results[xai_method+\" Correlation\"] = compiled_corr\n",
    "            \n",
    "            print(\"Average precision:\", np.mean(compiled_precision))\n",
    "            print(\"Average recall:\", np.mean(compiled_recall))\n",
    "            print(\"Average correlation:\", np.mean(compiled_corr))\n",
    "            print(\"\\n---------------------\\n\")\n",
    "            \n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False) \n",
    "            all_results.append(results)\n",
    "            \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for dataset_name in datasets:\n",
    "        \n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s/%s/pipelines'% \n",
    "                                                                    (dataset_ref, cls_method, method_name)))])\n",
    "        for bucket in tqdm(range(num_buckets)):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            pipeline_path = os.path.join(PATH, save_to, \"pipelines/pipeline_bucket_%s.joblib\" % \n",
    "                                         (bucketID))\n",
    "            pipeline = joblib.load(pipeline_path)\n",
    "            feature_combiner = pipeline['encoder']\n",
    "            if 'scaler' in pipeline.named_steps:\n",
    "                scaler = pipeline['scaler']\n",
    "            else:\n",
    "                scaler = None\n",
    "            cls = pipeline['cls']\n",
    "\n",
    "            #import training data for bucket\n",
    "            trainingdata = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/train_data_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            targets = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/train_data/y_train_bucket_%s.csv\" % \n",
    "                                                          (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            if scaler != None:\n",
    "                trainingdata = scaler.transform(trainingdata)\n",
    "\n",
    "            #find relevant samples for bucket\n",
    "            sample_instances = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/test_sample_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID))).values\n",
    "            results = pd.read_csv(os.path.join(PATH, \"%s/%s/%s/samples/results_bucket_%s.csv\" % \n",
    "                                      (dataset_ref, cls_method, method_name, bucketID)), sep=\";\")\n",
    "\n",
    "            if scaler != None:\n",
    "                sample_instances = scaler.transform(sample_instances)\n",
    "                \n",
    "            #Identify feature names\n",
    "            feat_list = [feat.replace(\" \", \"_\") for feat in feature_combiner.get_feature_names()]\n",
    "            class_names = [\"Negative\", \"Positive\"]\n",
    "                        cats = [feat for col in dataset_manager.dynamic_cat_cols+dataset_manager.static_cat_cols \n",
    "                    for feat in range(len(feat_list)) if col in feat_list[feat]]\n",
    "            \n",
    "            #create explanation mechanism\n",
    "            if xai_method == \"SHAP\":\n",
    "                if cls_method == \"xgboost\" or cls_method == \"decision_tree\":\n",
    "                    explainer = shap.Explainer(cls)\n",
    "                elif cls_method == \"nb\":\n",
    "                    explainer = shap.Explainer(cls.predict_proba, trainingdata)\n",
    "                else:\n",
    "                    explainer = shap.Explainer(cls, trainingdata)\n",
    "            elif xai_method == \"LIME\":\n",
    "                explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                  feature_names = feat_list, class_names=class_names, categorical_features = cats)\n",
    "            elif xai_method == \"ACV\":\n",
    "                explainer = joblib.load(os.path.join(PATH,'%s/%s/%s/acv_surrogate/acv_explainer_bucket_%s.joblib'% \n",
    "                                                                    (dataset_ref, cls_method, method_name, bucketID)))\n",
    "            elif xai_method == \"LINDA\":\n",
    "                test_dict = generate_local_predictions( sample_instances, results[\"Actual\"], cls, scaler, None )\n",
    "                explainer = None\n",
    "\n",
    "            compiled_precision = []\n",
    "            compiled_recall = []\n",
    "            compiled_corr = []\n",
    "\n",
    "            #for i in tqdm_notebook(range(len(test_x))):\n",
    "            #explain the chosen instances and find the fidelity\n",
    "            pool = mp.Pool(mp.cpu_count(), initargs=(mp.RLock(),), initializer=tqdm.set_lock)\n",
    "\n",
    "            #with tqdm(total = len(test_x)) as progress_bar:\n",
    "            start = time.time()\n",
    "            print(start)\n",
    "            #recall, precision, corr = zip(*pool.map(test_fidelity, [i for i in range(len(test_x))]))\n",
    "            for result in tqdm(pool.imap(test_fidelity, [i for i in range(len(sample_instances))]), total = len(sample_instances)):\n",
    "                compiled_precision.append(result[0])\n",
    "                compiled_recall.append(result[1])\n",
    "                compiled_corr.append(result[2])\n",
    "\n",
    "            print(time.time()-start, \"seconds\")\n",
    "\n",
    "            # compiled_precision = list(precision)\n",
    "            # compiled_recall = list(recall)\n",
    "            # compiled_corr = list(corr)\n",
    "\n",
    "            compiled_corr = np.nan_to_num(compiled_corr)\n",
    "\n",
    "            results[xai_method+\" Precision\"] = compiled_precision\n",
    "            results[xai_method+\" Recall\"] = compiled_recall\n",
    "            results[xai_method+\" Correlation\"] = compiled_corr\n",
    "            \n",
    "            results.to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results_bucket_%s.csv\") % \n",
    "                                (dataset_ref, cls_method, method_name, bucketID), sep=\";\", index=False) \n",
    "            all_results.append(results)\n",
    "            \n",
    "pd.concat(all_results).to_csv(os.path.join(PATH,\"%s/%s/%s/samples/results.csv\") % (dataset_ref, cls_method, method_name), \n",
    "                              sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647494001184,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "21ykq7QaapvM",
    "outputId": "163f2d0a-9903-4b96-986e-b1c03844004a"
   },
   "outputs": [],
   "source": [
    "type(explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647494001184,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SyywooFsapvM",
    "outputId": "ccbf5413-3d2e-4536-8247-a3a01e26944e"
   },
   "outputs": [],
   "source": [
    "true_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647494001184,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "LSEd_z3SapvN",
    "outputId": "abb49720-888e-4849-e36c-adb566f37d66"
   },
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1647494001528,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "Xc00SNhLapvN",
    "outputId": "a902d6cc-7483-4a23-adc1-466e46e73e5c"
   },
   "outputs": [],
   "source": [
    "explanation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1647494001529,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "omuDCa-17awb",
    "outputId": "577a18e7-e9bd-4377-9e52-edb6dba14ca1"
   },
   "outputs": [],
   "source": [
    "true_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1647494001529,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "WI5Nz7XjapvO",
    "outputId": "1c0088a2-8aa8-4526-cc75-47ab0aa56cca",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1647494001529,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "sEeIBMllapvO",
    "outputId": "34fca938-db75-41ab-f14b-e39a6c5dc14d"
   },
   "outputs": [],
   "source": [
    "print(np.mean(compiled_precision))\n",
    "print(np.mean(compiled_recall))\n",
    "print(np.mean(compiled_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1647494001529,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "LTQQwJ_yNzf4",
    "outputId": "dca5d347-83ff-4b74-dc14-a77aa7676b96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(compiled_corr)\n",
    "plt.xlim(-1.05,1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1647494001530,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "3k8MaMg1Nzf4",
    "outputId": "d51f6eac-0ee5-4829-8783-e5916967bdc0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compiled_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1647494001530,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "EEl48JQGNzf4",
    "outputId": "d0a6c140-4999-42f7-8b7e-5a75316e6b31"
   },
   "outputs": [],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1647494001530,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggmo3ZwNw7WahbpkVzVLN5i6jFbk-vO0eV7keYsmQ=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "B5CLn_gENzf5",
    "outputId": "5761092c-f8a3-48bf-f476-9b8fcc707de4"
   },
   "outputs": [],
   "source": [
    "instance = test_dict[5+14]\n",
    "label_lst = [\"Negative\", \"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMMntlVrNzf5"
   },
   "outputs": [],
   "source": [
    "[bn, inference, infoBN] = generate_BN_explanations(instance, label_lst, feat_list, \"Result\", \n",
    "                                                                       None, scaler, cls, save_to+\"/\"+cls_method+\"/\", dataset, show_in_notebook = False)\n",
    "        \n",
    "ie = pyAgrum.LazyPropagation(bn)\n",
    "result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "result_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "row = instance['original_vector']\n",
    "#print(row)\n",
    "\n",
    "likelihood = [0]*len(feat_list)\n",
    "\n",
    "for j in range(len(feat_list)):\n",
    "    var_labels = bn.variable(feat_list[j]).labels()\n",
    "    str_bins = list(var_labels)\n",
    "    bins = []\n",
    "\n",
    "    for disc_bin in str_bins:\n",
    "        disc_bin = disc_bin.strip('\"(]')\n",
    "        cat = [float(val) for val in disc_bin.split(',')]\n",
    "        bins.append(cat)\n",
    "\n",
    "    for k in range(len(bins)):\n",
    "        if k == 0 and row[j] <= bins[k][0]:\n",
    "            feat_bin = str_bins[k]\n",
    "        elif k == len(bins)-1 and row[j] >= bins[k][1]:\n",
    "            feat_bin = str_bins[k]\n",
    "        elif row[j] > bins[k][0] and row[j] <= bins[k][1]:\n",
    "            feat_bin = str_bins[k]\n",
    "\n",
    "    ie = pyAgrum.LazyPropagation(bn)\n",
    "    ie.setEvidence({feat_list[j]: feat_bin})\n",
    "    ie.makeInference()\n",
    "\n",
    "    result_posterior = ie.posterior(bn.idFromName(\"Result\")).topandas()\n",
    "    new_proba = result_posterior.loc[\"Result\", label_lst[instance['predictions']]]\n",
    "    #print(result_proba, new_proba)\n",
    "    proba_change = result_proba-new_proba\n",
    "    likelihood[j] = abs(proba_change)\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmrhfHi2Nzf5"
   },
   "outputs": [],
   "source": [
    "bins = pd.cut(likelihood, 4, retbins = True, duplicates = \"drop\")\n",
    "q1_min = bins[1][-2]\n",
    "q1_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JANfJxaVNzf5"
   },
   "outputs": [],
   "source": [
    "feat_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7eGp2cbNzf5"
   },
   "outputs": [],
   "source": [
    "gnb.sideBySide(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuWMVR32Nzf5"
   },
   "outputs": [],
   "source": [
    "compiled_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5jnVzLJapvP"
   },
   "outputs": [],
   "source": [
    "print(true_features)\n",
    "print(explanation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQRAWgH0Nzf6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfgTUpSRNzf6"
   },
   "outputs": [],
   "source": [
    "cls.predict(instance.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNBZHz_pNzf6"
   },
   "outputs": [],
   "source": [
    "cls.predict_proba(instance.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An_KLpWqNzf6"
   },
   "outputs": [],
   "source": [
    "cls.predict_log_proba(instance.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omiKDiYCKHyy"
   },
   "outputs": [],
   "source": [
    "# #! git add .\n",
    "# #! git commit -m \"update notebook with white box acv code\"\n",
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6ySEhlJN1dD"
   },
   "outputs": [],
   "source": [
    "# !git add .\n",
    "# !git commit -m \"update notebook with acv code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FqRoTkvNmNV"
   },
   "outputs": [],
   "source": [
    "# !git config --global user.email \"n9455647@qut.edu.au\"\n",
    "# !git config --global user.name \"Mythreyi-V\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_white_box_fidelity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "100bb41e9af843d4a42def950d2ce068": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "254f12685809432c8b3990134fdc0681": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e98fc36b054a42e19570c41882a4e0c6",
      "placeholder": "",
      "style": "IPY_MODEL_100bb41e9af843d4a42def950d2ce068",
      "value": " 26/26 [00:08&lt;00:00,  3.15it/s]"
     }
    },
    "29828f9cda034630b0ee0f8881c38f9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7404773f76b400494957139d068592f",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7725236fce944f384e72836720a2524",
      "value": 26
     }
    },
    "3a45728e4b8646779dba75d89096f1a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63a0e5d43ed1455e9d52c2948985ec63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a45728e4b8646779dba75d89096f1a8",
      "placeholder": "",
      "style": "IPY_MODEL_96b50d66299d4c22b73405bc9a3f9579",
      "value": "100%"
     }
    },
    "96b50d66299d4c22b73405bc9a3f9579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a81712ac2ac14e5985b2fc7941b3bd30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_63a0e5d43ed1455e9d52c2948985ec63",
       "IPY_MODEL_29828f9cda034630b0ee0f8881c38f9a",
       "IPY_MODEL_254f12685809432c8b3990134fdc0681"
      ],
      "layout": "IPY_MODEL_db7a683a288d419f852d87935a29997e"
     }
    },
    "b7404773f76b400494957139d068592f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db7a683a288d419f852d87935a29997e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7725236fce944f384e72836720a2524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e98fc36b054a42e19570c41882a4e0c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
